{
    "docs": [
        {
            "location": "/", 
            "text": "hIPPYlib - Inverse Problem PYthon library\n\n\nhIPPYlib implements state-of-the-art \nscalable\n \nadjoint-based\n algorithms for PDE-based \ndeterministic and Bayesian inverse problems\n. It builds on \nFEniCS\n for the discretization of the PDE and on \nPETSc\n for scalable and efficient linear algebra operations and solvers.\n\n\nFeatures\n\n\n\n\nFriendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form\n\n\nAutomatic generation of efficient code for the discretization of weak forms using FEniCS\n\n\nSymbolic differentiation of weak forms to generate derivatives and adjoint information\n\n\nGlobalized Inexact Newton-CG method to solve the inverse problem\n\n\nLow rank representation of the posterior covariace using randomized algorithms.\n\n\n\n\nSee also our \ntutorial\n and list of related \npublications\n.\n\n\nLatest Release\n\n\n\n\n Initial release coming soon \n\n\nDevelopment version\n\n\nDownload hippylib-1.0.0.tgz\n\n\n\n\nContact\n\n\nDeveloped by the \nhIPPYlib team\n at \nUT Austin\n and \nUC Merced\n.\n\n\nTo ask question and find answers see \nhere\n.\n\n\nPlease cite with \n\n\n@article{VillaPetraGhattas2016,\ntitle = {\nhIPPYlib: AN EXTENSIBLE SOFTWARE FRAMEWORK FOR LARGE-SCALE DETERMINISTIC AND LINEARIZED BAYESIAN INVERSION\n},\nauthor = {Villa U., and Petra, N., and Ghattas, O.},\nyear = {2016},\nurl = {http://hippylib.github.io}\n}", 
            "title": "Home"
        }, 
        {
            "location": "/#hippylib-inverse-problem-python-library", 
            "text": "hIPPYlib implements state-of-the-art  scalable   adjoint-based  algorithms for PDE-based  deterministic and Bayesian inverse problems . It builds on  FEniCS  for the discretization of the PDE and on  PETSc  for scalable and efficient linear algebra operations and solvers.", 
            "title": "hIPPYlib - Inverse Problem PYthon library"
        }, 
        {
            "location": "/#features", 
            "text": "Friendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form  Automatic generation of efficient code for the discretization of weak forms using FEniCS  Symbolic differentiation of weak forms to generate derivatives and adjoint information  Globalized Inexact Newton-CG method to solve the inverse problem  Low rank representation of the posterior covariace using randomized algorithms.   See also our  tutorial  and list of related  publications .", 
            "title": "Features"
        }, 
        {
            "location": "/#latest-release", 
            "text": "Initial release coming soon   Development version  Download hippylib-1.0.0.tgz", 
            "title": "Latest Release"
        }, 
        {
            "location": "/#contact", 
            "text": "Developed by the  hIPPYlib team  at  UT Austin  and  UC Merced .  To ask question and find answers see  here .  Please cite with   @article{VillaPetraGhattas2016,\ntitle = { hIPPYlib: AN EXTENSIBLE SOFTWARE FRAMEWORK FOR LARGE-SCALE DETERMINISTIC AND LINEARIZED BAYESIAN INVERSION },\nauthor = {Villa U., and Petra, N., and Ghattas, O.},\nyear = {2016},\nurl = {http://hippylib.github.io}\n}", 
            "title": "Contact"
        }, 
        {
            "location": "/tutorial/", 
            "text": "Tutorial\n\n\nThese tutorials are the best place to learn about the basic features and the algorithms in \nhIPPYlib\n.\n\n\n\n\nFEniCS101\n notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.\n\n\nSubsurface Bayesian\n notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.\n\n\nAdvection-Diffusion Bayesian\n notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.\n\n\nHessian Spectrum\n notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.\n\n\n\n\nThe interactive ipython notebooks are located in the \nTutorial\n folder of the \nhIPPYlib\n release.\n\n\nTo run the notebooks follow these instructions.\n\n\n\n\nOpen a FEniCS terminal and type\n\n\n\n\n$ cd Tutorial\n$ ipython notebook\n\n\n\n\n\n\nA new tab will open in your web-brower showing the notebooks.\n\n\nClick on the notebook you would like to use.\n\n\nTo run all the code in the notebook simply click on Cell --\n Run All.\n\n\n\n\nFor more information on installing ipython and using notebooks see \nhere\n.", 
            "title": "README"
        }, 
        {
            "location": "/tutorial/#tutorial", 
            "text": "These tutorials are the best place to learn about the basic features and the algorithms in  hIPPYlib .   FEniCS101  notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.  Subsurface Bayesian  notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.  Advection-Diffusion Bayesian  notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.  Hessian Spectrum  notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.   The interactive ipython notebooks are located in the  Tutorial  folder of the  hIPPYlib  release.  To run the notebooks follow these instructions.   Open a FEniCS terminal and type   $ cd Tutorial\n$ ipython notebook   A new tab will open in your web-brower showing the notebooks.  Click on the notebook you would like to use.  To run all the code in the notebook simply click on Cell --  Run All.   For more information on installing ipython and using notebooks see  here .", 
            "title": "Tutorial"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/", 
            "text": "FEniCS101 Tutorial\n\n\nIn this tutorial we consider the boundary value problem (BVP)\n\n\n\n\n\\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}\n\n\n\n\nwhere \n \\Omega = (0,1) \\times (0,1) \n, \n \\Gamma_D \n and and \n \\Gamma_N \n are the union of\nthe left and right, and top and bottom boundaries of \n \\Omega \n,\nrespectively.\n\n\nHere\n\n\\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}\n\n\n\n\nThe exact solution is\n\n u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). \n\n\n\n\nWeak formulation\n\n\nLet us define the Hilbert spaces \n V_{u_0}, V_0 \\in \\Omega \n as\n\n V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},\n\n\n V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.\n\n\n\n\nTo obtain the weak formulation, we multiply the PDE by an arbitrary function \n v \\in V_0 \n and integrate over the domain \n\\Omega \n leading to\n\n\n\n\n -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. \n\n\n\n\nThen, integration by parts the non-conforming term gives\n\n\n\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. \n\n\n\n\nFinally by recalling that \n v = 0 \n on \n \\Gamma_D \n and that \n k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma \n on \n\\Gamma_N \n, we find the weak formulation:\n\n\nFind\n \n u \\in V_{u_0}\n\n\nsuch that\n\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. \n\n\n\n\n1. Load modules\n\n\nTo start we load the following modules:\n\n\n\n\n\n\ndolfin: the python/C++ interface to FEniCS\n\n\n\n\n\n\nmath\n: the python module for mathematical functions\n\n\n\n\n\n\nnumpy\n: a python package for linear algebra\n\n\n\n\n\n\nmatplotlib\n: a python package used for plotting the results\n\n\n\n\n\n\nfrom dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\n\n\n\n2. Define the mesh and the finite element space\n\n\nWe construct a triangulation (mesh) \n \\mathcal{T}_h \n of the computational domain \n \\Omega := [0, 1]^2 \n with \nn\n elements in each direction.\n\n\nOn the mesh \n \\mathcal{T}_h \n, we then define the finite element space \n V_h \\subset H^1(\\Omega) \n consisting of globally continuous piecewise polinomials functions. The \ndegree\n variable defines the polinomial degree.\n\n\nn = 16\ndegree = 1\nmesh = RectangleMesh(0, 0, 1, 1, n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint \ndim(Vh) = \n, Vh.dim()\n\n\n\n\ndim(Vh) =  289\n\n\n\n\n\n3. Define boundary labels\n\n\nTo partition the boundary of \n\\Omega\n in the subdomains \n\\Gamma_{\\rm top}\n, \n\\Gamma_{\\rm bottom}\n, \n\\Gamma_{\\rm left}\n, \n\\Gamma_{\\rm right}\n we assign a unique label \nboundary_parts\n to each of part of \n \\partial \\Omega\n.\n\n\nclass TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1) \n DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1]) \n DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0]) \n DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1) \n DOLFIN_EPS\n\nboundary_parts = FacetFunction(\nsize_t\n, mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)\n\n\n\n\n4. Define the coefficients of the PDE and the boundary conditions\n\n\nWe first define the coefficients of the PDE using the \nConstant\n and \nExpression\n classes. \nConstant\n is used to define coefficients that do not depend on the space coordinates, \nExpression\n is used to define coefficients that are a known function of the space coordinates \nx[0]\n (x-axis direction) and \nx[1]\n (y-axis direction).\n\n\nIn the finite element method community, Dirichlet boundary conditions are also known as \nessential\n boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class \nDirichletBC\n to indicate this type of condition.\n\n\nOn the other hand, Newman boundary conditions are also known as \nnatural\n boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure \nds[i]\n to integrate over the portion of the boundary marked with label \ni\n.\n\n\nu_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])')\nsigma_top    = Expression('0')\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))')\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure(\nds\n, subdomain_data=boundary_parts)\n\n\n\n\n5. Define and solve the variational problem\n\n\nWe also define two special types of functions: the \nTrialFunction\n \nu\n and the \nTestFunction\n \nv\n. These special types of function are used by \nFEniCS\n to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.\n\n\nMore specifically, by denoting by \n\\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}\n the finite element basis for the space \nV_h\n, a function \n u_h \\in V_h\n can be written as\n\n u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), \n\nwhere \n{\\rm u}_i\n represents the coefficients in the finite element expansion of \nu_h\n.\n\n\nWe then define\n\n\n\n\n\n\nthe bilinear form \n a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx \n;\n\n\n\n\n\n\nthe linear form \n L(v_h) = \\int_\\Omega f v_h dx + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds + \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds\n.\n\n\n\n\n\n\nWe can then solve the variational problem\n\n\nFind\n \n u_h \\in V_h\n\n\nsuch that\n\n\n a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h \n\n\n\n\nusing directly the built-in \nsolve\n method in FEniCS.\n\n\nNOTE:\n As an alternative one can also assemble the finite element matrix \n A \n and the right hand side \n b \n that stems from the discretization of \n a \n and \n L \n, and then solve the linear system\n\n A {\\rm u} = {\\rm b}, \n\nwhere\n\n\n\n\n\n\n\n\n {\\rm u} \n is the vector collecting the coefficient of the finite element expasion of \n u_h \n,\n\n\n\n\n\n\nthe entries of the matrix A are such that \n A_{ij} = a(\\phi_j, \\phi_i) \n,\n\n\n\n\n\n\nthe entries of the right hand side b are such that \n b_i = L(\\phi_i) \n.\n\n\n\n\n\n\nu = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b, \ncg\n)\n\nnb.plot(uh)\n\n\n\n\n\n\n6. Compute the discretization error\n\n\nFor this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution \n u_h \n and the exact solution \n u_{\\rm ex} \n)\n\n \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, \n \nand\n\n \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. \n\n\n\n\nu_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])')\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'))\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint \n|| u_h - u_e ||_L2 = \n, err_L2\nprint \n|| u_h - u_e ||_H1 = \n, err_H1\n\n\n\n\n|| u_h - u_e ||_L2 =  0.00842388144799\n|| u_h - u_e ||_H1 =  0.394127114471\n\n\n\n7. Convergence of the finite element method\n\n\nWe now verify numerically a well-known convergence result for the finite element method.\n\n\nLet denote with \n s \n the polynomial degree of the finite element space, and assume that the solution \n u_{\\rm ex}\n is at least in \n H^{s+1}(\\Omega) \n. Then we have\n\n \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. \n\n\n\n\nIn the code below, the function \ncompute(n, degree)\n solves the PDE using a mesh with \nn\n elements in each direction and finite element spaces of polinomial order \ndegree\n.\n\n\nThe figure below shows the discretization errors in the \n H^1 \n and \n L^2 \n as a function of the mesh size \n h \n ( \n h = \\frac{1}{n}\n ) for piecewise linear (P1, \n s=1 \n ) and piecewise quadratic (P2, \ns=2\n ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:\n\n\n\n\n\n\nfor piecewise linear finite element P1 we observe first order convergence in the \n H^1\n-norm and second order convergence in the \n L^2\n-norm;\n\n\n\n\n\n\nfor piecewise quadratic finite element P2 we observe second order convergence in the \n H^1\n-norm and third order convergence in the \n L^2\n-norm.\n\n\n\n\n\n\ndef compute(n, degree):\n    mesh = RectangleMesh(0, 0, 1, 1, n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction(\nsize_t\n, mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure(\nds\n, subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or')\nplt.loglog(h, err_L2_P1, '-*b')\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g')\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k')\nplt.xlabel(\nMesh size h\n)\nplt.ylabel(\nError\n)\nplt.title(\nP1 Finite Element\n)\nplt.legend([\nH1 error\n, \nL2 error\n, \nFirst Order\n, \nSecond Order\n], 'lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or')\nplt.loglog(h, err_L2_P2, '-*b')\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g')\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k')\nplt.xlabel(\nMesh size h\n)\nplt.ylabel(\nError\n)\nplt.title(\nP2 Finite Element\n)\nplt.legend([\nH1 error\n, \nL2 error\n, \nSecond Order\n, \nThird Order\n], 'lower right')\n\nplt.show()\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "FEniCS101"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#fenics101-tutorial", 
            "text": "In this tutorial we consider the boundary value problem (BVP)   \\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}   where   \\Omega = (0,1) \\times (0,1)  ,   \\Gamma_D   and and   \\Gamma_N   are the union of\nthe left and right, and top and bottom boundaries of   \\Omega  ,\nrespectively.  Here \\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}   The exact solution is  u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).", 
            "title": "FEniCS101 Tutorial"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#weak-formulation", 
            "text": "Let us define the Hilbert spaces   V_{u_0}, V_0 \\in \\Omega   as  V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},   V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.   To obtain the weak formulation, we multiply the PDE by an arbitrary function   v \\in V_0   and integrate over the domain  \\Omega   leading to    -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0.    Then, integration by parts the non-conforming term gives    \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0.    Finally by recalling that   v = 0   on   \\Gamma_D   and that   k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma   on  \\Gamma_N  , we find the weak formulation:  Find    u \\in V_{u_0}  such that   \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.", 
            "title": "Weak formulation"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#1-load-modules", 
            "text": "To start we load the following modules:    dolfin: the python/C++ interface to FEniCS    math : the python module for mathematical functions    numpy : a python package for linear algebra    matplotlib : a python package used for plotting the results    from dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space", 
            "text": "We construct a triangulation (mesh)   \\mathcal{T}_h   of the computational domain   \\Omega := [0, 1]^2   with  n  elements in each direction.  On the mesh   \\mathcal{T}_h  , we then define the finite element space   V_h \\subset H^1(\\Omega)   consisting of globally continuous piecewise polinomials functions. The  degree  variable defines the polinomial degree.  n = 16\ndegree = 1\nmesh = RectangleMesh(0, 0, 1, 1, n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint  dim(Vh) =  , Vh.dim()  dim(Vh) =  289", 
            "title": "2. Define the mesh and the finite element space"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#3-define-boundary-labels", 
            "text": "To partition the boundary of  \\Omega  in the subdomains  \\Gamma_{\\rm top} ,  \\Gamma_{\\rm bottom} ,  \\Gamma_{\\rm left} ,  \\Gamma_{\\rm right}  we assign a unique label  boundary_parts  to each of part of   \\partial \\Omega .  class TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1)   DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1])   DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0])   DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1)   DOLFIN_EPS\n\nboundary_parts = FacetFunction( size_t , mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)", 
            "title": "3. Define boundary labels"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions", 
            "text": "We first define the coefficients of the PDE using the  Constant  and  Expression  classes.  Constant  is used to define coefficients that do not depend on the space coordinates,  Expression  is used to define coefficients that are a known function of the space coordinates  x[0]  (x-axis direction) and  x[1]  (y-axis direction).  In the finite element method community, Dirichlet boundary conditions are also known as  essential  boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class  DirichletBC  to indicate this type of condition.  On the other hand, Newman boundary conditions are also known as  natural  boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure  ds[i]  to integrate over the portion of the boundary marked with label  i .  u_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])')\nsigma_top    = Expression('0')\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))')\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure( ds , subdomain_data=boundary_parts)", 
            "title": "4. Define the coefficients of the PDE and the boundary conditions"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#5-define-and-solve-the-variational-problem", 
            "text": "We also define two special types of functions: the  TrialFunction   u  and the  TestFunction   v . These special types of function are used by  FEniCS  to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.  More specifically, by denoting by  \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}  the finite element basis for the space  V_h , a function   u_h \\in V_h  can be written as  u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x),  \nwhere  {\\rm u}_i  represents the coefficients in the finite element expansion of  u_h .  We then define    the bilinear form   a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx  ;    the linear form   L(v_h) = \\int_\\Omega f v_h dx + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds + \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds .    We can then solve the variational problem  Find    u_h \\in V_h  such that   a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h    using directly the built-in  solve  method in FEniCS.  NOTE:  As an alternative one can also assemble the finite element matrix   A   and the right hand side   b   that stems from the discretization of   a   and   L  , and then solve the linear system  A {\\rm u} = {\\rm b},  \nwhere      {\\rm u}   is the vector collecting the coefficient of the finite element expasion of   u_h  ,    the entries of the matrix A are such that   A_{ij} = a(\\phi_j, \\phi_i)  ,    the entries of the right hand side b are such that   b_i = L(\\phi_i)  .    u = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b,  cg )\n\nnb.plot(uh)", 
            "title": "5. Define and solve the variational problem"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#6-compute-the-discretization-error", 
            "text": "For this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution   u_h   and the exact solution   u_{\\rm ex}  )  \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx },   \nand  \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}.    u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])')\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'))\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint  || u_h - u_e ||_L2 =  , err_L2\nprint  || u_h - u_e ||_H1 =  , err_H1  || u_h - u_e ||_L2 =  0.00842388144799\n|| u_h - u_e ||_H1 =  0.394127114471", 
            "title": "6. Compute the discretization error"
        }, 
        {
            "location": "/Tutorial/1_FEniCS101/#7-convergence-of-the-finite-element-method", 
            "text": "We now verify numerically a well-known convergence result for the finite element method.  Let denote with   s   the polynomial degree of the finite element space, and assume that the solution   u_{\\rm ex}  is at least in   H^{s+1}(\\Omega)  . Then we have  \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}.    In the code below, the function  compute(n, degree)  solves the PDE using a mesh with  n  elements in each direction and finite element spaces of polinomial order  degree .  The figure below shows the discretization errors in the   H^1   and   L^2   as a function of the mesh size   h   (   h = \\frac{1}{n}  ) for piecewise linear (P1,   s=1   ) and piecewise quadratic (P2,  s=2  ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:    for piecewise linear finite element P1 we observe first order convergence in the   H^1 -norm and second order convergence in the   L^2 -norm;    for piecewise quadratic finite element P2 we observe second order convergence in the   H^1 -norm and third order convergence in the   L^2 -norm.    def compute(n, degree):\n    mesh = RectangleMesh(0, 0, 1, 1, n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction( size_t , mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure( ds , subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or')\nplt.loglog(h, err_L2_P1, '-*b')\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g')\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k')\nplt.xlabel( Mesh size h )\nplt.ylabel( Error )\nplt.title( P1 Finite Element )\nplt.legend([ H1 error ,  L2 error ,  First Order ,  Second Order ], 'lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or')\nplt.loglog(h, err_L2_P2, '-*b')\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g')\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k')\nplt.xlabel( Mesh size h )\nplt.ylabel( Error )\nplt.title( P2 Finite Element )\nplt.legend([ H1 error ,  L2 error ,  Second Order ,  Third Order ], 'lower right')\n\nplt.show()   Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "7. Convergence of the finite element method"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/", 
            "text": "\\def\\data{\\bf d_\\rm{obs}}\n\\def\\vec{\\bf}\n\\def\\m{\\bf m}\n\\def\\map{\\bf m_{\\text{MAP}}}\n\\def\\postcov{\\bf \\Gamma_{\\text{post}}}\n\\def\\prcov{\\bf \\Gamma_{\\text{prior}}}\n\\def\\matrix{\\bf}\n\\def\\Hmisfit{\\bf H_{\\text{misfit}}}\n\\def\\HT{\\tilde{\\bf H}_{\\text{misfit}}}\n\\def\\diag{diag}\n\\def\\Vr{\\matrix V_r}\n\\def\\Wr{\\matrix W_r}\n\\def\\Ir{\\matrix I_r}\n\\def\\Dr{\\matrix D_r}\n\\def\\H{\\matrix H}\n\n\n\n\n\nExample: Bayesian quantification of parameter uncertainty:\n\n\nEstimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE\n\n\nIn this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.\n\n\nFor simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.\n\n\nBayes' Theorem:\n\n\nThe posterior probability distribution combines the prior pdf\n\n\\pi_{\\text{prior}}(\\m)\n over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf\n\n\\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m)\n, which explicitly\nrepresents the probability that a given set of parameters \n\\m\n\nmight give rise to the observed data \n\\vec{d}_{\\text{obs}} \\in\n\\mathbb{R}^m\n, namely:\n\n\n\n\n\n\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}\n\n\n\n\n\nNote that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.\n\n\nGaussian prior and noise:\n\n\nThe prior:\n\n\nWe consider a Gaussian prior with mean \n\\vec m_{\\text prior}\n and covariance \n\\bf \\Gamma_{\\text{prior}}\n. The covariance is given by the discretization of the inverse of differential operator \n\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}\n, where \n\\gamma\n, \n\\delta > 0\n control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem\n\n\nThe likelihood:\n\n\n\n\n\n\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )\n\n\n\n\n\n\n\n\n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)\n\n\n\n\n\nHere \n\\bf f\n is the parameter-to-observable map that takes a parameter vector \n\\m\n and maps\nit to the space observation vector \n\\data\n.\n\n\nThe posterior:\n\n\n\n\n\n\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)\n\n\n\n\n\nThe Gaussian approximation of the posterior: \n\\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})\n\n\n\n\nThe mean of this posterior distribution, \n\\vec{\\map}\n, is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,\n\n\n\n\n\n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n-\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n-\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n\\Big).\n\n\n\n\n\nThe posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of \n\\mathcal{J}\n at \n\\map\n, namely\n\n\n\n\n\n\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}\n\n\n\n\n\nThe prior-preconditioned Hessian of the data misfit:\n\n\n\n\n\n  \\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}\n\n\n\n\n\nThe generalized eigenvalue problem:\n\n\n\n\n\n \\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda},\n\n\n\n\n\nwhere \n\\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}\n\ncontains the generalized eigenvalues and the columns of \n\\matrix V\\in\n\\mathbb R^{n\\times n}\n the generalized eigenvectors such that \n\n\\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I}\n.\n\n\nRandomized eigensolvers to construct the approximate spectral decomposition:\n\n\nWhen the generalized eigenvalues \n\\{\\lambda_i\\}\n decay rapidly, we can\nextract a low-rank approximation of \n\\Hmisfit\n by retaining only the \nr\n\nlargest eigenvalues and corresponding eigenvectors,\n\n\n\n\n\n \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2},\n\n\n\n\n\nHere, \n\\matrix{V}_r \\in \\mathbb{R}^{n\\times r}\n contains only the \nr\n\ngeneralized eigenvectors of \n\\Hmisfit\n that correspond to the \nr\n largest eigenvalues,\nwhich are assembled into the diagonal matrix \n\\matrix{\\Lambda}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r}\n.\n\n\nThe approximate posterior covariance::\n\n\nUsing the Sherman\u2013Morrison\u2013Woodbury formula, we write\n\n\n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}\n\n\n\n\n\nwhere \n\\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r}\n. The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that \nr\n is\nchosen such that \n\\lambda_r\n is small relative to 1. \n\n\nTherefore we can approximate the posterior covariance as\n\n\n\n\n\n\\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r\n\\matrix{V}_r^T\n\n\n\n\n\nDrawing samples from a Gaussian distribution with covariance \n\\H^{-1}\n\n\n\n\nLet \n\\bf x\n be a sample for the prior distribution, i.e. \n\\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov)\n, then, using the low rank approximation of the posterior covariance, we compute a sample \n{\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})\n as \n\n\n  {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + \\bf I \\big\\} {\\bf x} \n\n\n\n\n\nThis tutorial shows:\n\n\n\n\ndescription of the inverse problem (the forward problem, the prior, and the misfit functional)\n\n\nconvergence of the inexact Newton-CG algorithm\n\n\nlow-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit) \n\n\nhow to construct the low-rank approximation of the Hessian of the data misfit\n\n\nhow to apply the inverse and square-root inverse Hessian to a vector efficiently\n\n\nsamples from the Gaussian approximation of the posterior\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nunderstand the Bayesian inverse framework\n\n\nvisualise and understand the results\n\n\nmodify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\nBayes' formula\n\n\nrandomized eigensolvers\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, A great python package that I used for plotting many of the results\n\n\nNumpy\n, A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.\n\n\n\n\n1. Load modules\n\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append( \n../\n )\nfrom hippylib import *\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\n2. Generate the true parameter\n\n\nThis function generates a random field with a prescribed anysotropic covariance function.\n\n\ndef true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise,\nnoise\n)\n    noise_size = noise.array().shape[0]\n    noise.set_local( np.random.randn( noise_size ) )\n    atrue = dl.Vector()\n    prior.init_vector(atrue, 0)\n    prior.sample(noise,atrue)\n    return atrue\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nWe compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the \nstate\n and \nadjoint\n variable and P1 for the \nparameter\n.\n\n\nndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint \nNumber of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\n.format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())\n\n\n\n\nNumber of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641\n\n\n\n4. Set up the forward problem\n\n\nTo set up the forward problem we use the \nPDEVariationalProblem\n class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables \nVh\n\n- the pde in weak form \npde_varf\n\n- the boundary conditions \nbc\n for the forward problem and \nbc0\n for the adjoint and incremental problems.\n\n\nThe \nPDEVariationalProblem\n class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.\n\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] \n dl.DOLFIN_EPS or x[1] \n 1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression(\nx[1]\n)\nu_bdr0 = dl.Expression(\n0.0\n)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Expression(\n0.0\n)\n\ndef pde_varf(u,a,p):\n    return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0)\n\n\n\n\n4. Set up the prior\n\n\nTo obtain the synthetic true paramter \na_{\\rm true}\n we generate a realization of a Gaussian random field with zero average and covariance matrix \n\\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2}\n, where \n\\widetilde{\\mathcal{A}}\n is a differential operator of the form\n\n \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. \n\nHere \n\\Theta\n is an s.p.d. anisotropic tensor of the form\n\n \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix} \n\n\n\n\nFor the prior model, we assume that we can measure the log-permeability coefficient at \nN\n locations, and we denote with \na^1_{\\rm true}\n, \n\\ldots\n, \na^N_{\\rm true}\n such measures.\nWe also introduce the mollifier functions\n\n \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N,\n\nand we let\n\n \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M},\n\nwhere \np\n is a penalization costant (10 for this problem) and \n \\mathcal{M} = \\sum_{i=1}^N \\delta_i I\n.\n\n\nWe then compute \na_{\\rm pr}\n, the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving\n\n\na_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle.\n\n\n\n\n\nFinally the prior distribution is \n\\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior})\n, with \n\\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2}\n.\n\n\ngamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\natrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen)\n\nprint \nPrior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\n.format(delta, gamma,2)    \n\nobjs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\nTrue Parameter\n, \nPrior mean\n]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)\n\n\n\n\nPrior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2\n\n\n\n\n\n5. Set up the misfit functional and generate synthetic observations\n\n\nTo setup the observation operator, we generate \nntargets\n random locations where to evaluate the value of the state.\n\n\nTo generate the synthetic observation, we first solve the forward problem using the true parameter \na_{\\rm true}\n. Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise.\n\nrel_noise\n is the signal to noise ratio.\n\n\nntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint \nNumber of observation points: {0}\n.format(ntargets)\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, atrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\nlinf\n)\nnoise_std_dev = rel_noise * MAX\nrandn_perturb(misfit.d, noise_std_dev)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\nTrue State\n, subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle=\nObservations\n, subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()\n\n\n\n\nNumber of observation points: 300\n\n\n\n\n\n6. Set up the model and test gradient and Hessian\n\n\nThe model is defined by three component:\n- the \nPDEVariationalProblem\n \npde\n which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the \nPrior\n \nprior\n which provides methods to apply the regularization (\nprecision\n) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the \nMisfit\n \nmisfit\n which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.\n\n\nTo test gradient and the Hessian of the model we use forward finite differences.\n\n\nmodel = Model(pde, prior, misfit)\n\na0 = dl.interpolate(dl.Expression(\nsin(x[0])\n), Vh[PARAMETER])\nmodelVerify(model, a0.vector(), 1e-12)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  9.09491932242e-15\n\n\n\n\n\n7. Compute the MAP point\n\n\nWe used the globalized Newtown-CG method to compute the MAP point.\n\n\na0 = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\nrel_tolerance\n] = 1e-9\nsolver.parameters[\nabs_tolerance\n] = 1e-12\nsolver.parameters[\nmax_iter\n]      = 25\nsolver.parameters[\ninner_rel_tolerance\n] = 1e-15\nsolver.parameters[\nc_armijo\n] = 1e-4\nsolver.parameters[\nGN_iter\n] = 5\n\nx = solver.solve(a0)\n\nif solver.converged:\n    print \n\\nConverged in \n, solver.it, \n iterations.\n\nelse:\n    print \n\\nNot Converged\n\n\nprint \nTermination reason: \n, solver.termination_reasons[solver.reason]\nprint \nFinal gradient norm: \n, solver.final_grad_norm\nprint \nFinal cost: \n, solver.final_cost\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\nState\n)\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\nParameter\n)\nplt.show()\n\n\n\n\nIt  cg_it cost            misfit          reg             (g,da)          ||g||L2        alpha          tolcg         \n  1   1    1.205749e+03    1.205435e+03    3.147595e-01   -1.569088e+04   1.041993e+05   1.000000e+00   5.000000e-01\n  2   3    3.456819e+02    3.444282e+02    1.253761e+00   -1.845351e+03   1.430874e+04   1.000000e+00   3.705684e-01\n  3   1    2.745939e+02    2.732846e+02    1.309355e+00   -1.421202e+02   1.002730e+04   1.000000e+00   3.102127e-01\n  4   7    1.691715e+02    1.647563e+02    4.415191e+00   -2.126866e+02   3.868977e+03   1.000000e+00   1.926929e-01\n  5   6    1.573196e+02    1.522914e+02    5.028129e+00   -2.345175e+01   1.820008e+03   1.000000e+00   1.321613e-01\n  6  14    1.424898e+02    1.297463e+02    1.274356e+01   -2.988290e+01   1.157435e+03   1.000000e+00   1.053940e-01\n  7   2    1.421591e+02    1.294122e+02    1.274692e+01   -6.608825e-01   7.299626e+02   1.000000e+00   8.369856e-02\n  8  22    1.407910e+02    1.253199e+02    1.547109e+01   -2.732846e+00   4.407936e+02   1.000000e+00   6.504072e-02\n  9  16    1.407819e+02    1.253760e+02    1.540587e+01   -1.827524e-02   5.039967e+01   1.000000e+00   2.199285e-02\n 10  29    1.407814e+02    1.253304e+02    1.545101e+01   -9.452146e-04   1.061811e+01   1.000000e+00   1.009465e-02\n 11  36    1.407814e+02    1.253304e+02    1.545106e+01   -8.147549e-08   9.003262e-02   1.000000e+00   9.295390e-04\n 12  62    1.407814e+02    1.253304e+02    1.545106e+01   -2.063794e-12   3.281322e-04   1.000000e+00   5.611670e-05\n\nConverged in  12  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  1.27740754831e-08\nFinal cost:  140.781419038\n\n\n\n\n\n8. Compute the low rank Gaussian approximation of the posterior\n\n\nWe used the \ndouble pass\n algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve\n\n\n\n\n \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}. \n\n\n\n\nThe Figure shows the largest \nk\n generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\nThe effective rank is independent of the mesh size.\n\n\nmodel.setPointForHessianEvaluations(x)\nHmisfit = ReducedHessian(model, solver.parameters[\ninner_rel_tolerance\n], gauss_newton_approx=False, misfit_only=True)\nk = 50\np = 20\nprint \nSingle/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\n.format(k,p)\nOmega = np.random.randn(x[PARAMETER].array().shape[0], k+p)\nd, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, d, U)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\nEigenvector\n, which=[0,1,2,5,10,15])\n\n\n\n\nSingle/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.\n\n\n\n\n\n\n\n9. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\nEstimator\n, tol=5e-2, min_iter=20, max_iter=2000)\n    print \nPosterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\n.format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(\nExact\n)\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [\nPrior variance\n, \nPosterior variance\n]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 1.144005e-01; Prior trace 4.031887e-01; Correction trace 2.887882e-01\n\n\n\n\n\n10. Generate samples from Prior and Posterior\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\nnoise\n)\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh[PARAMETER], name=\nsample_prior\n)\ns_post = dl.Function(Vh[PARAMETER], name=\nsample_post\n)\n\nrange_pr = 2*math.sqrt( pr_pw_variance.max() )\nps_max   = 2*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\nPrior sample\n, vmin=-range_pr, vmax=range_pr)\n    nb.plot(s_post, subplot_loc=122,mytitle=\nPosterior sample\n, vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "Subsurface Bayesian"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#example-bayesian-quantification-of-parameter-uncertainty", 
            "text": "", 
            "title": "Example: Bayesian quantification of parameter uncertainty:"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#estimating-the-gaussian-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde", 
            "text": "In this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.  For simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.  Bayes' Theorem:  The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m)  over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m) , which explicitly\nrepresents the probability that a given set of parameters  \\m \nmight give rise to the observed data  \\vec{d}_{\\text{obs}} \\in\n\\mathbb{R}^m , namely:   \n\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}   Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.  Gaussian prior and noise:  The prior:  We consider a Gaussian prior with mean  \\vec m_{\\text prior}  and covariance  \\bf \\Gamma_{\\text{prior}} . The covariance is given by the discretization of the inverse of differential operator  \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where  \\gamma ,  \\delta > 0  control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem  The likelihood:   \n\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )    \n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)   Here  \\bf f  is the parameter-to-observable map that takes a parameter vector  \\m  and maps\nit to the space observation vector  \\data .  The posterior:   \n\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)   The Gaussian approximation of the posterior:  \\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})   The mean of this posterior distribution,  \\vec{\\map} , is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,   \n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n-\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n-\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n\\Big).   The posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of  \\mathcal{J}  at  \\map , namely   \n\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}   The prior-preconditioned Hessian of the data misfit:   \n  \\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}   The generalized eigenvalue problem:   \n \\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda},   where  \\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} \ncontains the generalized eigenvalues and the columns of  \\matrix V\\in\n\\mathbb R^{n\\times n}  the generalized eigenvectors such that  \\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I} .  Randomized eigensolvers to construct the approximate spectral decomposition:  When the generalized eigenvalues  \\{\\lambda_i\\}  decay rapidly, we can\nextract a low-rank approximation of  \\Hmisfit  by retaining only the  r \nlargest eigenvalues and corresponding eigenvectors,   \n \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2},   Here,  \\matrix{V}_r \\in \\mathbb{R}^{n\\times r}  contains only the  r \ngeneralized eigenvectors of  \\Hmisfit  that correspond to the  r  largest eigenvalues,\nwhich are assembled into the diagonal matrix  \\matrix{\\Lambda}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r} .  The approximate posterior covariance::  Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}   where  \\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r} . The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that  r  is\nchosen such that  \\lambda_r  is small relative to 1.   Therefore we can approximate the posterior covariance as   \n\\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r\n\\matrix{V}_r^T   Drawing samples from a Gaussian distribution with covariance  \\H^{-1}   Let  \\bf x  be a sample for the prior distribution, i.e.  \\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample  {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})  as  \n  {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + \\bf I \\big\\} {\\bf x}", 
            "title": "Estimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#this-tutorial-shows", 
            "text": "description of the inverse problem (the forward problem, the prior, and the misfit functional)  convergence of the inexact Newton-CG algorithm  low-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit)   how to construct the low-rank approximation of the Hessian of the data misfit  how to apply the inverse and square-root inverse Hessian to a vector efficiently  samples from the Gaussian approximation of the posterior", 
            "title": "This tutorial shows:"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#goals", 
            "text": "By the end of this notebook, you should be able to:   understand the Bayesian inverse framework  visualise and understand the results  modify the problem and code", 
            "title": "Goals:"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#mathematical-tools-used", 
            "text": "Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search  Bayes' formula  randomized eigensolvers", 
            "title": "Mathematical tools used:"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#list-of-software-used", 
            "text": "FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , A great python package that I used for plotting many of the results  Numpy , A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.", 
            "title": "List of software used:"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#1-load-modules", 
            "text": "import dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append(  ../  )\nfrom hippylib import *\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#2-generate-the-true-parameter", 
            "text": "This function generates a random field with a prescribed anysotropic covariance function.  def true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise, noise )\n    noise_size = noise.array().shape[0]\n    noise.set_local( np.random.randn( noise_size ) )\n    atrue = dl.Vector()\n    prior.init_vector(atrue, 0)\n    prior.sample(noise,atrue)\n    return atrue", 
            "title": "2. Generate the true parameter"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces", 
            "text": "We compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the  state  and  adjoint  variable and P1 for the  parameter .  ndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint  Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2} .format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())  Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641", 
            "title": "3. Set up the mesh and finite element spaces"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#4-set-up-the-forward-problem", 
            "text": "To set up the forward problem we use the  PDEVariationalProblem  class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables  Vh \n- the pde in weak form  pde_varf \n- the boundary conditions  bc  for the forward problem and  bc0  for the adjoint and incremental problems.  The  PDEVariationalProblem  class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.  def u_boundary(x, on_boundary):\n    return on_boundary and ( x[1]   dl.DOLFIN_EPS or x[1]   1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression( x[1] )\nu_bdr0 = dl.Expression( 0.0 )\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Expression( 0.0 )\n\ndef pde_varf(u,a,p):\n    return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0)", 
            "title": "4. Set up the forward problem"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#4-set-up-the-prior", 
            "text": "To obtain the synthetic true paramter  a_{\\rm true}  we generate a realization of a Gaussian random field with zero average and covariance matrix  \\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2} , where  \\widetilde{\\mathcal{A}}  is a differential operator of the form  \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I.  \nHere  \\Theta  is an s.p.d. anisotropic tensor of the form  \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix}    For the prior model, we assume that we can measure the log-permeability coefficient at  N  locations, and we denote with  a^1_{\\rm true} ,  \\ldots ,  a^N_{\\rm true}  such measures.\nWe also introduce the mollifier functions  \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N, \nand we let  \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M}, \nwhere  p  is a penalization costant (10 for this problem) and   \\mathcal{M} = \\sum_{i=1}^N \\delta_i I .  We then compute  a_{\\rm pr} , the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving \na_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle.   Finally the prior distribution is  \\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior}) , with  \\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2} .  gamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\natrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen)\n\nprint  Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2} .format(delta, gamma,2)    \n\nobjs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [ True Parameter ,  Prior mean ]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)  Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2", 
            "title": "4. Set up the prior"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations", 
            "text": "To setup the observation operator, we generate  ntargets  random locations where to evaluate the value of the state.  To generate the synthetic observation, we first solve the forward problem using the true parameter  a_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise. rel_noise  is the signal to noise ratio.  ntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint  Number of observation points: {0} .format(ntargets)\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, atrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm( linf )\nnoise_std_dev = rel_noise * MAX\nrandn_perturb(misfit.d, noise_std_dev)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle= True State , subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle= Observations , subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()  Number of observation points: 300", 
            "title": "5. Set up the misfit functional and generate synthetic observations"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian", 
            "text": "The model is defined by three component:\n- the  PDEVariationalProblem   pde  which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the  Prior   prior  which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the  Misfit   misfit  which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.  To test gradient and the Hessian of the model we use forward finite differences.  model = Model(pde, prior, misfit)\n\na0 = dl.interpolate(dl.Expression( sin(x[0]) ), Vh[PARAMETER])\nmodelVerify(model, a0.vector(), 1e-12)  (yy, H xx) - (xx, H yy) =  9.09491932242e-15", 
            "title": "6. Set up the model and test gradient and Hessian"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#7-compute-the-map-point", 
            "text": "We used the globalized Newtown-CG method to compute the MAP point.  a0 = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[ rel_tolerance ] = 1e-9\nsolver.parameters[ abs_tolerance ] = 1e-12\nsolver.parameters[ max_iter ]      = 25\nsolver.parameters[ inner_rel_tolerance ] = 1e-15\nsolver.parameters[ c_armijo ] = 1e-4\nsolver.parameters[ GN_iter ] = 5\n\nx = solver.solve(a0)\n\nif solver.converged:\n    print  \\nConverged in  , solver.it,   iterations. \nelse:\n    print  \\nNot Converged \n\nprint  Termination reason:  , solver.termination_reasons[solver.reason]\nprint  Final gradient norm:  , solver.final_grad_norm\nprint  Final cost:  , solver.final_cost\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle= State )\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle= Parameter )\nplt.show()  It  cg_it cost            misfit          reg             (g,da)          ||g||L2        alpha          tolcg         \n  1   1    1.205749e+03    1.205435e+03    3.147595e-01   -1.569088e+04   1.041993e+05   1.000000e+00   5.000000e-01\n  2   3    3.456819e+02    3.444282e+02    1.253761e+00   -1.845351e+03   1.430874e+04   1.000000e+00   3.705684e-01\n  3   1    2.745939e+02    2.732846e+02    1.309355e+00   -1.421202e+02   1.002730e+04   1.000000e+00   3.102127e-01\n  4   7    1.691715e+02    1.647563e+02    4.415191e+00   -2.126866e+02   3.868977e+03   1.000000e+00   1.926929e-01\n  5   6    1.573196e+02    1.522914e+02    5.028129e+00   -2.345175e+01   1.820008e+03   1.000000e+00   1.321613e-01\n  6  14    1.424898e+02    1.297463e+02    1.274356e+01   -2.988290e+01   1.157435e+03   1.000000e+00   1.053940e-01\n  7   2    1.421591e+02    1.294122e+02    1.274692e+01   -6.608825e-01   7.299626e+02   1.000000e+00   8.369856e-02\n  8  22    1.407910e+02    1.253199e+02    1.547109e+01   -2.732846e+00   4.407936e+02   1.000000e+00   6.504072e-02\n  9  16    1.407819e+02    1.253760e+02    1.540587e+01   -1.827524e-02   5.039967e+01   1.000000e+00   2.199285e-02\n 10  29    1.407814e+02    1.253304e+02    1.545101e+01   -9.452146e-04   1.061811e+01   1.000000e+00   1.009465e-02\n 11  36    1.407814e+02    1.253304e+02    1.545106e+01   -8.147549e-08   9.003262e-02   1.000000e+00   9.295390e-04\n 12  62    1.407814e+02    1.253304e+02    1.545106e+01   -2.063794e-12   3.281322e-04   1.000000e+00   5.611670e-05\n\nConverged in  12  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  1.27740754831e-08\nFinal cost:  140.781419038", 
            "title": "7. Compute the MAP point"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#8-compute-the-low-rank-gaussian-approximation-of-the-posterior", 
            "text": "We used the  double pass  algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve    \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}.    The Figure shows the largest  k  generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\nThe effective rank is independent of the mesh size.  model.setPointForHessianEvaluations(x)\nHmisfit = ReducedHessian(model, solver.parameters[ inner_rel_tolerance ], gauss_newton_approx=False, misfit_only=True)\nk = 50\np = 20\nprint  Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}. .format(k,p)\nOmega = np.random.randn(x[PARAMETER].array().shape[0], k+p)\nd, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, d, U)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], U, mytitle= Eigenvector , which=[0,1,2,5,10,15])  Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.", 
            "title": "8. Compute the low rank Gaussian approximation of the posterior"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#9-prior-and-posterior-pointwise-variance-fields", 
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method= Estimator , tol=5e-2, min_iter=20, max_iter=2000)\n    print  Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e} .format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance( Exact )\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [ Prior variance ,  Posterior variance ]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 1.144005e-01; Prior trace 4.031887e-01; Correction trace 2.887882e-01", 
            "title": "9. Prior and posterior pointwise variance fields"
        }, 
        {
            "location": "/Tutorial/2_SubsurfaceBayesian/#10-generate-samples-from-prior-and-posterior", 
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise, noise )\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh[PARAMETER], name= sample_prior )\ns_post = dl.Function(Vh[PARAMETER], name= sample_post )\n\nrange_pr = 2*math.sqrt( pr_pw_variance.max() )\nps_max   = 2*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle= Prior sample , vmin=-range_pr, vmax=range_pr)\n    nb.plot(s_post, subplot_loc=122,mytitle= Posterior sample , vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "10. Generate samples from Prior and Posterior"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/", 
            "text": "\\def\\D{\\mathcal{D}}\n\\def\\ipar{m}\n\\def\\R{\\mathbb{R}}\n\\def\\del{\\partial}\n\\def\\vec{\\bf}\n\\def\\priorm{\\mu_0}\n\\def\\C{\\mathcal{C}}\n\\def\\Acal{\\mathcal{A}}\n\\def\\postm{\\mu_{\\rm{post}}}\n\\def\\iparpost{\\ipar_\\text{post}}\n\\def\\obs{\\vec{d}} \n\\def\\yobs{\\obs^{\\text{obs}}}\n\\def\\obsop{\\mathcal{B}}\n\\def\\dd{\\vec{\\bar{d}}}\n\\def\\iFF{\\mathcal{F}}\n\\def\\iFFadj{\\mathcal{F}^*}\n\\def\\ncov{\\Gamma_{\\mathrm{noise}}}\n\n\n\n\n\nExample: Bayesian initial condition inversion in an advection-diffusion problem\n\n\nIn this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.\n\n\nThe Bayesian inverse problem:\n\n\nFollowing the Bayesian framework, we utilize \na Gaussian prior measure \n\\priorm = \\mathcal{N}(\\ipar_0,\\C_0)\n,\nwith \n\\C_0=\\Acal^{-2}\n where \n\\Acal\n is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure, \n\\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})\n with\n\n\\iparpost\n and \n\\C_\\text{post}\n.\n\n\n\n\nThe posterior mean \n\\iparpost\n is characterized as the minimizer of\n\n\n\n\n\n\n\n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}\n\n\n\n\n\nwhich can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator \n\\mathcal{B}\n extracts the values of the forward solution \nu\n on a set of\nlocations \n\\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D\n at\ntimes \n\\{t_1, \\ldots, t_N\\} \\subset [0, T]\n.\n\n\n\n\nThe posterior covariance \n\\C_{\\text{post}}\n is the inverse of the Hessian of \n\\mathcal{J}(\\ipar)\n, i.e.,\n\n\n\n\n\n\n\n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.\n\n\n\n\n\nThe forward problem:\n\n\nThe PDE in the parameter-to-observable map \n\\iFF\n models diffusive transport\nin a domain \n\\D \\subset \\R^d\n (\nd \\in \\{2, 3\\}\n):\n\n\n\n\n\n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}\n\n\n\n\n\nHere, \n\\kappa > 0\n is the diffusion coefficient and \nT > 0\n is the final\ntime. The velocity field\n\n\\vec{v}\n is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:\n\n\n\n\n\n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}\n\n\n\n\n\nHere, \nq\n is pressure, \n\\text{Re}\n is the Reynolds number. The Dirichlet boundary data\n\n\\vec{g} \\in \\R^d\n is given by \n\n\\vec{g} = \\vec{e}_2\n on the left wall of the domain, \n\n\\vec{g}=-\\vec{e}_2\n on the right wall,  and \n\\vec{g} = \\vec{0}\n everywhere else.\n\n\nThe adjoint problem:\n\n\n\n\n\n\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}\n\n\n\n\n\n1. Load modules\n\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append( \n../\n )\nfrom hippylib import *\nsys.path.append( \n../applications/ad_diff/\n )\nfrom model_ad_diff import TimeDependentAD\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(1)\n\n\n\n\n2. Construct the velocity field\n\n\ndef v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0] \n dl.DOLFIN_EPS and x[1] \n dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    XW = dl.MixedFunctionSpace([Xh, Wh])\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0] \n 1e-14) - (x[0] \n 1 - 1e-14)'))\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={\nnewton_solver\n:\n                                         {\nrelative_tolerance\n:1e-4, \nmaximum_iterations\n:100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\nVelocity\n)\n    nb.plot(qh, subplot_loc=122,mytitle=\nPressure\n)\n    plt.show()\n\n    return v\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nmesh = dl.refine( dl.Mesh(\nad_20.xml\n) )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh, \nLagrange\n, 1)\nprint \nNumber of dofs: {0}\n.format( Vh.dim() )\n\n\n\n\n\n\nNumber of dofs: 2023\n\n\n\n4. Set up model (prior, true/proposed initial condition)\n\n\n#gamma = 1\n#delta = 10\n#prior = LaplacianPrior(Vh, gamma, delta)\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Expression('0.5'), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))'), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [\nTrue Initial Condition\n, \nPrior mean\n]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\n\n\n\n\n\n5. Generate the synthetic observations\n\n\nrel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm(\nlinf\n, \nlinf\n)\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nproblem.ud.randn_perturb(noise_std_dev)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue, \nSolution\n)\n\n\n\n\n\n\n6. Test the gradient and the Hessian of the cost (negative log posterior)\n\n\na0 = true_initial_condition.copy()\nmodelVerify(problem, a0, 1e-12, is_quadratic=True)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  -2.66447204172e-14\n\n\n\n\n\n7. Evaluate the gradient\n\n\n[u,a,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,a,p], 1e-12)\nproblem.solveAdj(p, [u,a,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,a,p], mg)\n\nprint \n(g,g) = \n, grad_norm\n\n\n\n\n(g,g) =  1.67407425719e+12\n\n\n\n8. The Gaussian approximation of the posterior\n\n\nH = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) \n\nk = 80\np = 20\nprint \nSingle Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\n.format(k,p)\nOmega = np.random.randn(a.array().shape[0], k+p)\nd, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, d, U )\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, U, mytitle=\nEigenvector\n, which=[0,1,2,5,10,20,30,45,60])\n\n\n\n\nSingle Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.\n\n\n\n\n\n\n\n9. Compute the MAP point\n\n\nH.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[\nprint_level\n] = 1\nsolver.parameters[\nrel_tolerance\n] = 1e-6\nsolver.solve(a, -mg)\nproblem.solveFwd(u, [u,a,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,a,p])\nprint \nTotal cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\n.format(total_cost, reg_cost, misfit_cost)\n\nposterior.mean = a\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, a), mytitle=\nInitial Condition\n)\nplt.show()\n\nnb.show_solution(Vh, a, u, \nSolution\n)\n\n\n\n\n Iterartion :  0  (B r, r) =  30439.3327254\n Iteration :  1  (B r, r) =  0.0608908617948\n Iteration :  2  (B r, r) =  1.04124879143e-05\n Iteration :  3  (B r, r) =  9.49494299284e-09\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  9.74419980955e-05\nTotal cost 84.6353; Reg Cost 69.0841; Misfit 15.5513\n\n\n\n\n\n\n\n10. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\nEstimator\n, tol=5e-2, min_iter=20, max_iter=2000)\n    print \nPosterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\n.format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance()\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [\nPrior Variance\n, \nPosterior Variance\n]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 0.000563201; Prior trace 0.0285287; Correction trace 0.0279655\n\n\n\n\n\n11. Draw samples from the prior and posterior distributions\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\nnoise\n)\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh, name=\nsample_prior\n)\ns_post = dl.Function(Vh, name=\nsample_post\n)\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\nPrior sample\n, vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\nPosterior sample\n, vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "Advection-Diffusion Bayesian"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#example-bayesian-initial-condition-inversion-in-an-advection-diffusion-problem", 
            "text": "In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.  The Bayesian inverse problem:  Following the Bayesian framework, we utilize \na Gaussian prior measure  \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) ,\nwith  \\C_0=\\Acal^{-2}  where  \\Acal  is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure,  \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})  with \\iparpost  and  \\C_\\text{post} .   The posterior mean  \\iparpost  is characterized as the minimizer of    \n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}   which can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator  \\mathcal{B}  extracts the values of the forward solution  u  on a set of\nlocations  \\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D  at\ntimes  \\{t_1, \\ldots, t_N\\} \\subset [0, T] .   The posterior covariance  \\C_{\\text{post}}  is the inverse of the Hessian of  \\mathcal{J}(\\ipar) , i.e.,    \n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.   The forward problem:  The PDE in the parameter-to-observable map  \\iFF  models diffusive transport\nin a domain  \\D \\subset \\R^d  ( d \\in \\{2, 3\\} ):   \n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}   Here,  \\kappa > 0  is the diffusion coefficient and  T > 0  is the final\ntime. The velocity field \\vec{v}  is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:   \n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}   Here,  q  is pressure,  \\text{Re}  is the Reynolds number. The Dirichlet boundary data \\vec{g} \\in \\R^d  is given by  \\vec{g} = \\vec{e}_2  on the left wall of the domain,  \\vec{g}=-\\vec{e}_2  on the right wall,  and  \\vec{g} = \\vec{0}  everywhere else.  The adjoint problem:   \n\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}", 
            "title": "Example: Bayesian initial condition inversion in an advection-diffusion problem"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#1-load-modules", 
            "text": "import dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append(  ../  )\nfrom hippylib import *\nsys.path.append(  ../applications/ad_diff/  )\nfrom model_ad_diff import TimeDependentAD\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(1)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#2-construct-the-velocity-field", 
            "text": "def v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0]   dl.DOLFIN_EPS and x[1]   dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    XW = dl.MixedFunctionSpace([Xh, Wh])\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0]   1e-14) - (x[0]   1 - 1e-14)'))\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={ newton_solver :\n                                         { relative_tolerance :1e-4,  maximum_iterations :100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle= Velocity )\n    nb.plot(qh, subplot_loc=122,mytitle= Pressure )\n    plt.show()\n\n    return v", 
            "title": "2. Construct the velocity field"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces", 
            "text": "mesh = dl.refine( dl.Mesh( ad_20.xml ) )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh,  Lagrange , 1)\nprint  Number of dofs: {0} .format( Vh.dim() )   Number of dofs: 2023", 
            "title": "3. Set up the mesh and finite element spaces"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition", 
            "text": "#gamma = 1\n#delta = 10\n#prior = LaplacianPrior(Vh, gamma, delta)\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Expression('0.5'), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))'), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [ True Initial Condition ,  Prior mean ]\nnb.multi1_plot(objs, mytitles)\nplt.show()", 
            "title": "4. Set up model (prior, true/proposed initial condition)"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations", 
            "text": "rel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm( linf ,  linf )\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nproblem.ud.randn_perturb(noise_std_dev)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue,  Solution )", 
            "title": "5. Generate the synthetic observations"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior", 
            "text": "a0 = true_initial_condition.copy()\nmodelVerify(problem, a0, 1e-12, is_quadratic=True)  (yy, H xx) - (xx, H yy) =  -2.66447204172e-14", 
            "title": "6. Test the gradient and the Hessian of the cost (negative log posterior)"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#7-evaluate-the-gradient", 
            "text": "[u,a,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,a,p], 1e-12)\nproblem.solveAdj(p, [u,a,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,a,p], mg)\n\nprint  (g,g) =  , grad_norm  (g,g) =  1.67407425719e+12", 
            "title": "7. Evaluate the gradient"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#8-the-gaussian-approximation-of-the-posterior", 
            "text": "H = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) \n\nk = 80\np = 20\nprint  Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}. .format(k,p)\nOmega = np.random.randn(a.array().shape[0], k+p)\nd, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, d, U )\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, U, mytitle= Eigenvector , which=[0,1,2,5,10,20,30,45,60])  Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.", 
            "title": "8. The Gaussian approximation of the posterior"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#9-compute-the-map-point", 
            "text": "H.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[ print_level ] = 1\nsolver.parameters[ rel_tolerance ] = 1e-6\nsolver.solve(a, -mg)\nproblem.solveFwd(u, [u,a,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,a,p])\nprint  Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g} .format(total_cost, reg_cost, misfit_cost)\n\nposterior.mean = a\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, a), mytitle= Initial Condition )\nplt.show()\n\nnb.show_solution(Vh, a, u,  Solution )   Iterartion :  0  (B r, r) =  30439.3327254\n Iteration :  1  (B r, r) =  0.0608908617948\n Iteration :  2  (B r, r) =  1.04124879143e-05\n Iteration :  3  (B r, r) =  9.49494299284e-09\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  9.74419980955e-05\nTotal cost 84.6353; Reg Cost 69.0841; Misfit 15.5513", 
            "title": "9. Compute the MAP point"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields", 
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method= Estimator , tol=5e-2, min_iter=20, max_iter=2000)\n    print  Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g} .format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance()\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [ Prior Variance ,  Posterior Variance ]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 0.000563201; Prior trace 0.0285287; Correction trace 0.0279655", 
            "title": "10. Prior and posterior pointwise variance fields"
        }, 
        {
            "location": "/Tutorial/3_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions", 
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise, noise )\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh, name= sample_prior )\ns_post = dl.Function(Vh, name= sample_post )\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle= Prior sample , vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle= Posterior sample , vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "11. Draw samples from the prior and posterior distributions"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/", 
            "text": "Spectrum of the Reduced Hessian\n\n\nThe linear source inversion problem\n\n\nWe consider the following linear source inversion problem.\nFind the state \nu \\in H^1_{\\Gamma_D}(\\Omega)\n and the source (\nparameter\n) \na \\in H^1(\\Omega)\n that solves\n\n\\begin{align*}\n{} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{align*}\n\n\n\n\nHere:\n\n\n\n\n\n\n\n\nu_d\n is a \nn_{\\rm obs}\n finite dimensional vector that denotes noisy observations of the state \nu\n in \nn_{\\rm obs}\n locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n. More specifically, \nu_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i\n, where \n\\eta_i\n are i.i.d. \n\\mathcal{N}(0, \\sigma^2)\n.\n\n\n\n\n\n\n\n\nB: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}\n is the linear operator that evaluates the state \nu\n at the observation locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n.\n\n\n\n\n\n\n\n\n\\delta\n and \n\\gamma\n are the parameters of the regularization penalizing the \nL^2(\\Omega)\n and \nH^1(\\Omega)\n norm of \na-a_0\n, respectively.\n\n\n\n\n\n\n\n\nk\n, \n{\\bf v}\n, \nc\n are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.\n\n\n\n\n\n\n\n\n\\Gamma_D \\subset \\partial \\Omega\n, \n\\Gamma_N \\subset \\partial \\Omega\n represents the subdomain of \n\\partial\\Omega\n where we impose Dirichlet or Neumann boundary conditions, respectively.\n\n\n\n\n\n\n1. Load modules\n\n\nimport dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nimport sys\nsys.path.append(\n../\n)\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\n\n\n\n2. The linear source inversion problem\n\n\ndef pde_varf(u,a,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - a*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1] \n dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    np.random.seed(seed=2)\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print \nNumber of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\n.format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())\n\n\n    u_bdr = dl.Expression(\n0.0\n)\n    u_bdr0 = dl.Expression(\n0.0\n)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    atrue = dl.interpolate( dl.Expression(\nexp( -50*(x[0] - .5)*(x[0] - .5) - 50*(x[1] - .5)*(x[1] - .5))\n), Vh[PARAMETER]).vector()\n    a0 = dl.interpolate(dl.Expression(\n0.0\n), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0)\n\n    if verbose:\n        print \nNumber of observation points: {0}\n.format(targets.shape[0])\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, atrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm(\nlinf\n)\n    noise_std_dev = rel_noise * MAX\n    randn_perturb(misfit.d, noise_std_dev)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle = \nTrue source\n, subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\nTrue state\n, subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle=\nObservations\n, subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    a = a0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,a,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[\nprint_level\n] = -1\n    solver.parameters[\nrel_tolerance\n] = 1e-9\n    solver.solve(a, -mg)\n\n    if solver.converged:\n        if verbose:\n            print \nCG converged in \n, solver.iter, \n iterations.\n\n    else:\n        print \nCG did not converged.\n\n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], a), mytitle = \nReconstructed source\n, subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle=\nReconstructed state\n, subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\nMisfit\n, subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print \nDouble Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\n.format(k_evec,p_evec)\n    Omega = np.random.randn(a.array().shape[0], k_evec+p_evec)\n    d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(d, mytitle=\nGeneralized Eigenvalues\n)\n        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\nEigenvectors\n, which=[0,1,2,5,10,15])\n        plt.show()\n\n    return d, U, Vh[PARAMETER], solver.iter\n\n\n\n\n\n3. Solution of the source inversion problem\n\n\nndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Expression(\n1.0\n)\nv = dl.Expression((\n0.0\n, \n0.0\n))\nc = dl.Expression(\n0.\n)\n\nd, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta)\n\n\n\n\nNumber of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300\n\n\n\n\n\nCG converged in  49  iterations.\n\n\n\n\n\nDouble Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.\n\n\n\n\n\n\n\n4. Mesh independence of the spectrum of the preconditioned Hessian\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Expression(\n1.0\n)\nv = dl.Expression((\n0.0\n, \n0.0\n))\nc = dl.Expression(\n0.\n)\n\nn = [16,32,64]\nd1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint \nNumber of Iterations: \n, niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\nEigenvalues Mesh {0} by {1}\n.format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\nEigenvalues Mesh {0} by {1}\n.format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\nEigenvalues Mesh {0} by {1}\n.format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\nMesh {0} by {1} Eigen\n.format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\nMesh {0} by {1} Eigen\n.format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\nMesh {0} by {1} Eigen\n.format(n[2],n[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  45 48 49\n\n\n\n\n\n\n\n\n\n\n\n5. Dependence on the noise level\n\n\nWe solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Expression(\n1.0\n)\nv = dl.Expression((\n0.0\n, \n0.0\n))\nc = dl.Expression(\n0.\n)\n\nrel_noise = [1e-3,1e-2,1e-1]\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint \nNumber of Iterations: \n, niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\nEigenvalues rel_noise {0:g}\n.format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\nEigenvalues rel_noise {0:g}\n.format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\nEigenvalues rel_noise {0:g}\n.format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\nrel_noise {0:g} Eigen\n.format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\nrel_noise {0:g} Eigen\n.format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\nrel_noise {0:g} Eigen\n.format(rel_noise[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  112 48 18\n\n\n\n\n\n\n\n\n\n\n\n6. Dependence on the PDE coefficients\n\n\nAssume a constant reaction term \nc = 1\n, and we consider different values for the diffusivity coefficient \nk\n.\n\n\nThe smaller the value of \nk\n the slower the decay in the spectrum.\n\n\nrel_noise = 0.01\n\nk = dl.Expression(\n1.0\n)\nv = dl.Expression((\n0.0\n, \n0.0\n))\nc = dl.Expression(\n1.0\n)\n\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Expression(\n0.1\n)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Expression(\n0.01\n)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint \nNumber of Iterations: \n, niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\nEigenvalues k=1.0\n, subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\nEigenvalues k=0.1\n, subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\nEigenvalues k=0.01\n, subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\nk=1. Eigen\n, which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\nk=0.1 Eigen\n, which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\nk=0.01 Eigen\n, which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  61 99 158\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "Hessian Spectrum"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#spectrum-of-the-reduced-hessian", 
            "text": "", 
            "title": "Spectrum of the Reduced Hessian"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#the-linear-source-inversion-problem", 
            "text": "We consider the following linear source inversion problem.\nFind the state  u \\in H^1_{\\Gamma_D}(\\Omega)  and the source ( parameter )  a \\in H^1(\\Omega)  that solves \\begin{align*}\n{} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{align*}   Here:     u_d  is a  n_{\\rm obs}  finite dimensional vector that denotes noisy observations of the state  u  in  n_{\\rm obs}  locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . More specifically,  u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where  \\eta_i  are i.i.d.  \\mathcal{N}(0, \\sigma^2) .     B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}  is the linear operator that evaluates the state  u  at the observation locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} .     \\delta  and  \\gamma  are the parameters of the regularization penalizing the  L^2(\\Omega)  and  H^1(\\Omega)  norm of  a-a_0 , respectively.     k ,  {\\bf v} ,  c  are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.     \\Gamma_D \\subset \\partial \\Omega ,  \\Gamma_N \\subset \\partial \\Omega  represents the subdomain of  \\partial\\Omega  where we impose Dirichlet or Neumann boundary conditions, respectively.", 
            "title": "The linear source inversion problem"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#1-load-modules", 
            "text": "import dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nimport sys\nsys.path.append( ../ )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#2-the-linear-source-inversion-problem", 
            "text": "def pde_varf(u,a,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - a*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1]   dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    np.random.seed(seed=2)\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print  Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2} .format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())\n\n\n    u_bdr = dl.Expression( 0.0 )\n    u_bdr0 = dl.Expression( 0.0 )\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    atrue = dl.interpolate( dl.Expression( exp( -50*(x[0] - .5)*(x[0] - .5) - 50*(x[1] - .5)*(x[1] - .5)) ), Vh[PARAMETER]).vector()\n    a0 = dl.interpolate(dl.Expression( 0.0 ), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0)\n\n    if verbose:\n        print  Number of observation points: {0} .format(targets.shape[0])\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, atrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm( linf )\n    noise_std_dev = rel_noise * MAX\n    randn_perturb(misfit.d, noise_std_dev)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle =  True source , subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle= True state , subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle= Observations , subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    a = a0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,a,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[ print_level ] = -1\n    solver.parameters[ rel_tolerance ] = 1e-9\n    solver.solve(a, -mg)\n\n    if solver.converged:\n        if verbose:\n            print  CG converged in  , solver.iter,   iterations. \n    else:\n        print  CG did not converged. \n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], a), mytitle =  Reconstructed source , subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle= Reconstructed state , subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle= Misfit , subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print  Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}. .format(k_evec,p_evec)\n    Omega = np.random.randn(a.array().shape[0], k_evec+p_evec)\n    d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(d, mytitle= Generalized Eigenvalues )\n        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle= Eigenvectors , which=[0,1,2,5,10,15])\n        plt.show()\n\n    return d, U, Vh[PARAMETER], solver.iter", 
            "title": "2. The linear source inversion problem"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#3-solution-of-the-source-inversion-problem", 
            "text": "ndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Expression( 1.0 )\nv = dl.Expression(( 0.0 ,  0.0 ))\nc = dl.Expression( 0. )\n\nd, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta)  Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300   CG converged in  49  iterations.   Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.", 
            "title": "3. Solution of the source inversion problem"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian", 
            "text": "gamma = 70.\ndelta = 1e-1\n\nk = dl.Expression( 1.0 )\nv = dl.Expression(( 0.0 ,  0.0 ))\nc = dl.Expression( 0. )\n\nn = [16,32,64]\nd1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint  Number of Iterations:  , niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle= Eigenvalues Mesh {0} by {1} .format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle= Eigenvalues Mesh {0} by {1} .format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle= Eigenvalues Mesh {0} by {1} .format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle= Mesh {0} by {1} Eigen .format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle= Mesh {0} by {1} Eigen .format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle= Mesh {0} by {1} Eigen .format(n[2],n[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  45 48 49", 
            "title": "4. Mesh independence of the spectrum of the preconditioned Hessian"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#5-dependence-on-the-noise-level", 
            "text": "We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.  gamma = 70.\ndelta = 1e-1\n\nk = dl.Expression( 1.0 )\nv = dl.Expression(( 0.0 ,  0.0 ))\nc = dl.Expression( 0. )\n\nrel_noise = [1e-3,1e-2,1e-1]\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint  Number of Iterations:  , niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle= Eigenvalues rel_noise {0:g} .format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle= Eigenvalues rel_noise {0:g} .format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle= Eigenvalues rel_noise {0:g} .format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle= rel_noise {0:g} Eigen .format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle= rel_noise {0:g} Eigen .format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle= rel_noise {0:g} Eigen .format(rel_noise[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  112 48 18", 
            "title": "5. Dependence on the noise level"
        }, 
        {
            "location": "/Tutorial/4_HessianSpectrum/#6-dependence-on-the-pde-coefficients", 
            "text": "Assume a constant reaction term  c = 1 , and we consider different values for the diffusivity coefficient  k .  The smaller the value of  k  the slower the decay in the spectrum.  rel_noise = 0.01\n\nk = dl.Expression( 1.0 )\nv = dl.Expression(( 0.0 ,  0.0 ))\nc = dl.Expression( 1.0 )\n\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Expression( 0.1 )\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Expression( 0.01 )\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint  Number of Iterations:  , niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle= Eigenvalues k=1.0 , subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle= Eigenvalues k=0.1 , subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle= Eigenvalues k=0.01 , subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle= k=1. Eigen , which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle= k=0.1 Eigen , which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle= k=0.01 Eigen , which=[0,1,5])\n\nplt.show()  Number of Iterations:  61 99 158      Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 3.0 dated June 2007.", 
            "title": "6. Dependence on the PDE coefficients"
        }, 
        {
            "location": "/building/", 
            "text": "Installation\n\n\nDownload hippylib\n\n\nRelease 1.0 of hIPPYlib is available for download \nhere\n.\nSimply decompress the file hIPPYlib-1.0.0.tgz in your working directory. There is no installation necessary.\nYou can directly run the examples from the \napplication\n folder or view the notebooks from the \ntutorial\n folder. \n\n\nTo checkout the development version of hippylib use the command\n\n\ngit clone bblbblbl\n\n\n\n\nThe current version of hIPPYlib depends on \nFEniCS\n version 1.6.\nBelow you can find how to install the correct version of FEniCS on your system.\n\n\nInstall FEniCS\n\n\n\n\nMacOS 10.10 and 10.11 systems:\n\n\n\n\nDownload FEniCS 1.6.0 from \nhere\n.\nFind  your  MacOS  version  (either  10.11  or  10.10)  and  download  the appropriate  binaries  of  FEniCS  1.6.0.\nIf  you  are  running bash as default shell, you can add the following line to your profile in your home folder:\n\n\nsource /Applications/FEniCS.app/Contents/Resources/share/fenics/fenics.conf\n\n\n\n\nAlternatively you can just double-click on the FEniCS icon in your Applications directory and that will generate a new shell preconfigured with the paths that FEniCS needs. Just run FEniCS from within this shell.\n\n\n\n\nMacOS 10.9:\n\n\n\n\nDownload FEniCS 1.5.0 from \nhere\n.\nFind  your  MacOS  version and  download  the appropriate  binaries  of  FEniCS  1.5.0.\nNote FEniCS 1.5.0 will not be supported by future releases of hIPPYlib\n\n\n\n\nUbuntu LTS 14.04:\n\n\n\n\nOpen a shell and run the following commands:\n\n\nsudo add-apt-repository ppa:fenics-packages/fenics-1.6.x\nsudo apt-get update\nsudo apt-get install -y fenics\nsudo apt-get dist-upgrade\nsudo apt-get install -y ipython-notebook\nsudo apt-get install -y paraview\nsudo apt-get install -y git\n\n\n\n\nIf in the future you decide to uninstall FEniCS and remove all its dependencies, you can run the following commands:\n\n\nsudo apt-get purge --auto-remove fenics\nsudo ppa-purge ppa:fenics-packages/fenics-1.6.x", 
            "title": "Installation"
        }, 
        {
            "location": "/building/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/building/#download-hippylib", 
            "text": "Release 1.0 of hIPPYlib is available for download  here .\nSimply decompress the file hIPPYlib-1.0.0.tgz in your working directory. There is no installation necessary.\nYou can directly run the examples from the  application  folder or view the notebooks from the  tutorial  folder.   To checkout the development version of hippylib use the command  git clone bblbblbl  The current version of hIPPYlib depends on  FEniCS  version 1.6.\nBelow you can find how to install the correct version of FEniCS on your system.", 
            "title": "Download hippylib"
        }, 
        {
            "location": "/building/#install-fenics", 
            "text": "MacOS 10.10 and 10.11 systems:   Download FEniCS 1.6.0 from  here .\nFind  your  MacOS  version  (either  10.11  or  10.10)  and  download  the appropriate  binaries  of  FEniCS  1.6.0.\nIf  you  are  running bash as default shell, you can add the following line to your profile in your home folder:  source /Applications/FEniCS.app/Contents/Resources/share/fenics/fenics.conf  Alternatively you can just double-click on the FEniCS icon in your Applications directory and that will generate a new shell preconfigured with the paths that FEniCS needs. Just run FEniCS from within this shell.   MacOS 10.9:   Download FEniCS 1.5.0 from  here .\nFind  your  MacOS  version and  download  the appropriate  binaries  of  FEniCS  1.5.0.\nNote FEniCS 1.5.0 will not be supported by future releases of hIPPYlib   Ubuntu LTS 14.04:   Open a shell and run the following commands:  sudo add-apt-repository ppa:fenics-packages/fenics-1.6.x\nsudo apt-get update\nsudo apt-get install -y fenics\nsudo apt-get dist-upgrade\nsudo apt-get install -y ipython-notebook\nsudo apt-get install -y paraview\nsudo apt-get install -y git  If in the future you decide to uninstall FEniCS and remove all its dependencies, you can run the following commands:  sudo apt-get purge --auto-remove fenics\nsudo ppa-purge ppa:fenics-packages/fenics-1.6.x", 
            "title": "Install FEniCS"
        }, 
        {
            "location": "/outreach/", 
            "text": "Outreach\n\n\nPublications\n\n\n\n\nU. Villa, N. Petra, O. Ghattas, \nhIPPYlib: An extensible software framework for large-scale deterministic and linearized Bayesian inverse problems\n, 2016\n\n\n\n\nPresentations\n\n\n\n\nU. Villa, \nAn Analytical Technique for Forward and Inverse Propagation of Uncertainty\n, SIAM Conference on Uncertainty Quantification, April 5-8, 2016, Lausanne, Switzerland\n\n\n\n\nWorkshops\n\n\n\n\nN. Petra, \nSAMSI Optimization Program Summer School\n, Research Triangle Park, NC,  August 8-12, 2016\n\n\nN. Petra and O. Ghattas, \nIDEALab: Inverse Problems and Uncertainty Quantification\n, Brown University, Providence, RD, July 6-10, 2015\n\n\n\n\nGraduate level courses\n\n\n\n\nA. Alexanderian @NC State, \nInverse problems\n, Fall 2016\n\n\nT. Bui @UT Austin, \nComputational PDE-Constrained Bayesian inverse problems\n, Fall 2016\n\n\nG. Stadler @NYU, \nAdvanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems\n, Spring 2016 \nlink\n\n\nN. Petra @UC Merced, \nSpecial Topics: Computational and Variational Inverse Problems\n, Fall 2015 \nlink\n\n\nO. Ghattas @UT Austin, \nComputational and Variational Inverse Problems\n, Fall 2015 \nlink\n\n\n\n\nProjects\n\n\n\n\n\n\n\nPIs: O. Ghattas, M. Parno, N. Petra, Y. Marzouk, U. Villa, \nIntegrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF", 
            "title": "Outreach"
        }, 
        {
            "location": "/outreach/#outreach", 
            "text": "", 
            "title": "Outreach"
        }, 
        {
            "location": "/outreach/#publications", 
            "text": "U. Villa, N. Petra, O. Ghattas,  hIPPYlib: An extensible software framework for large-scale deterministic and linearized Bayesian inverse problems , 2016", 
            "title": "Publications"
        }, 
        {
            "location": "/outreach/#presentations", 
            "text": "U. Villa,  An Analytical Technique for Forward and Inverse Propagation of Uncertainty , SIAM Conference on Uncertainty Quantification, April 5-8, 2016, Lausanne, Switzerland", 
            "title": "Presentations"
        }, 
        {
            "location": "/outreach/#workshops", 
            "text": "N. Petra,  SAMSI Optimization Program Summer School , Research Triangle Park, NC,  August 8-12, 2016  N. Petra and O. Ghattas,  IDEALab: Inverse Problems and Uncertainty Quantification , Brown University, Providence, RD, July 6-10, 2015", 
            "title": "Workshops"
        }, 
        {
            "location": "/outreach/#graduate-level-courses", 
            "text": "A. Alexanderian @NC State,  Inverse problems , Fall 2016  T. Bui @UT Austin,  Computational PDE-Constrained Bayesian inverse problems , Fall 2016  G. Stadler @NYU,  Advanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems , Spring 2016  link  N. Petra @UC Merced,  Special Topics: Computational and Variational Inverse Problems , Fall 2015  link  O. Ghattas @UT Austin,  Computational and Variational Inverse Problems , Fall 2015  link", 
            "title": "Graduate level courses"
        }, 
        {
            "location": "/outreach/#projects", 
            "text": "PIs: O. Ghattas, M. Parno, N. Petra, Y. Marzouk, U. Villa,  Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF", 
            "title": "Projects"
        }, 
        {
            "location": "/about/", 
            "text": "About hIPPYlib\n\n\nAuthors\n\n\n\n\nOmar Ghattas\n\n\nNoemi Petra\n\n\nUmberto Villa\n\n\n\n\n\n\n\nCopyright\n\n\n 2016 The University of Texas at Austin, University of California Merced.\n\n\nLicense\n\n\n\n\nGNU General Public License version 3 (GPL)\n\n\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nbootstrap\n, \nbootswatch\n, and \nMathJax\n.\nHosted on \nGitHub\n.", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-hippylib", 
            "text": "", 
            "title": "About hIPPYlib"
        }, 
        {
            "location": "/about/#authors", 
            "text": "Omar Ghattas  Noemi Petra  Umberto Villa", 
            "title": "Authors"
        }, 
        {
            "location": "/about/#copyright", 
            "text": "2016 The University of Texas at Austin, University of California Merced.", 
            "title": "Copyright"
        }, 
        {
            "location": "/about/#license", 
            "text": "GNU General Public License version 3 (GPL)     Website built with  MkDocs ,  bootstrap ,  bootswatch , and  MathJax .\nHosted on  GitHub .", 
            "title": "License"
        }
    ]
}