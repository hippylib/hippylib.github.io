{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"hIPPYlib - Inverse Problem PYthon library hIPPYlib implements state-of-the-art scalable adjoint-based algorithms for PDE-based deterministic and Bayesian inverse problems . It builds on FEniCS for the discretization of the PDE and on PETSc for scalable and efficient linear algebra operations and solvers. Features Friendly, compact, near-mathematical FEniCS notation to express, differentiate, and discretize the PDE forward model and likelihood function Large scale optimization algorithms, such as globalized inexact Newton-CG method, to solve the inverse problem Randomized algorithms for trace estimation, eigenvalues and singular values decomposition. Scalable sampling of Gaussian random fields Linearized Bayesian inversion with low-rank based representation of the posterior covariance Hessian informed MCMC algorithms to explore the posterior distribution Forward propagation of uncertainty capabilities using Monte Carlo and Taylor expansion control variates See also our tutorial and list of related publications . For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School on Inverse Problems: Systematic Integration of Data with Models under Uncertainty available here . The complete API reference is available here . Latest Release Development version Download hippylib-3.1.0.zip Previous releases Join us! Dr. Villa is searching for a postdoctoral researcher interested in computational imaging and predictive sciences to join his team at the Oden Institute, The University of Texas at Austin. See here for instructions on how to apply. Contact Developed by the hIPPYlib team at UT Austin and UC Merced . Slack channel The hIPPYlib slack channel is a good resource to request and receive help with using hIPPYlib. Everyone is invited to read and take part in discussions. Discussions about the development of new features in hIPPYlib also take place here. You can join our Slack community by filling in this form . Please cite as @article{VillaPetraGhattas21, author = {Villa, Umberto and Petra, Noemi and Ghattas, Omar}, title = \"{HIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference}\", year = {2021}, issue_date = {March 2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {47}, number = {2}, issn = {0098-3500}, url = {https://doi.org/10.1145/3428447}, doi = {10.1145/3428447}, journal = {ACM Trans. Math. Softw.}, month = apr, articleno = {16}, numpages = {34} } @article{VillaPetraGhattas18, title = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\", author = {Villa, U. and Petra, N. and Ghattas, O.}, journal = {Journal of Open Source Software}, volume = {3}, number = {30}, page = {940}, doi = {10.21105/joss.00940}, year = {2018} } @article{VillaPetraGhattas16, title = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\", author = {Villa, U. and Petra, N. and Ghattas, O.}, year = {2016}, url = {http://hippylib.github.io}, doi = {10.5281/zenodo.596931} }","title":"Home"},{"location":"#hippylib-inverse-problem-python-library","text":"hIPPYlib implements state-of-the-art scalable adjoint-based algorithms for PDE-based deterministic and Bayesian inverse problems . It builds on FEniCS for the discretization of the PDE and on PETSc for scalable and efficient linear algebra operations and solvers.","title":"hIPPYlib - Inverse Problem PYthon library"},{"location":"#features","text":"Friendly, compact, near-mathematical FEniCS notation to express, differentiate, and discretize the PDE forward model and likelihood function Large scale optimization algorithms, such as globalized inexact Newton-CG method, to solve the inverse problem Randomized algorithms for trace estimation, eigenvalues and singular values decomposition. Scalable sampling of Gaussian random fields Linearized Bayesian inversion with low-rank based representation of the posterior covariance Hessian informed MCMC algorithms to explore the posterior distribution Forward propagation of uncertainty capabilities using Monte Carlo and Taylor expansion control variates See also our tutorial and list of related publications . For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School on Inverse Problems: Systematic Integration of Data with Models under Uncertainty available here . The complete API reference is available here .","title":"Features"},{"location":"#latest-release","text":"Development version Download hippylib-3.1.0.zip Previous releases","title":"Latest Release"},{"location":"#join-us","text":"Dr. Villa is searching for a postdoctoral researcher interested in computational imaging and predictive sciences to join his team at the Oden Institute, The University of Texas at Austin. See here for instructions on how to apply.","title":"Join us!"},{"location":"#contact","text":"Developed by the hIPPYlib team at UT Austin and UC Merced .","title":"Contact"},{"location":"#slack-channel","text":"The hIPPYlib slack channel is a good resource to request and receive help with using hIPPYlib. Everyone is invited to read and take part in discussions. Discussions about the development of new features in hIPPYlib also take place here. You can join our Slack community by filling in this form . Please cite as @article{VillaPetraGhattas21, author = {Villa, Umberto and Petra, Noemi and Ghattas, Omar}, title = \"{HIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference}\", year = {2021}, issue_date = {March 2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {47}, number = {2}, issn = {0098-3500}, url = {https://doi.org/10.1145/3428447}, doi = {10.1145/3428447}, journal = {ACM Trans. Math. Softw.}, month = apr, articleno = {16}, numpages = {34} } @article{VillaPetraGhattas18, title = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\", author = {Villa, U. and Petra, N. and Ghattas, O.}, journal = {Journal of Open Source Software}, volume = {3}, number = {30}, page = {940}, doi = {10.21105/joss.00940}, year = {2018} } @article{VillaPetraGhattas16, title = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\", author = {Villa, U. and Petra, N. and Ghattas, O.}, year = {2016}, url = {http://hippylib.github.io}, doi = {10.5281/zenodo.596931} }","title":"Slack channel"},{"location":"about/","text":"About hIPPYlib Authors Umberto Villa Noemi Petra Omar Ghattas Current contributors Amal Alghamdi (UT Austin), Olalekan Babaniyi (RIT), Joshua Chen (UT Austin), Peng Chen (UT Austin), Argho Datta (WashU), Ki-Tae Kim (UC Merced), Tom O'Leary-Roseberry (UT Austin), Luke Lozenski (WashU), Siddhant Wahal (UT Austin) Past contributors Ilona Ambartsumyan , Ben Crestel, Eldar Khattatov , Vishwas Rao Copyright \u00a9 2016-2018 The University of Texas at Austin, University of California Merced. \u00a9 2019-2020 The University of Texas at Austin, University of California Merced, Washington University in St. Louis. Support O. Ghattas, N. Petra (PIs), U. Villa (Co-PI), Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SSI2, Grants No ACI-1550593, ACI-1550547 (2016-2019). O. Ghattas (PI); G. Biros and Y. Marzouk (Co-PIs), Bayesian Optimal Experimental Design for Inverse Scattering , Air Force Office of Scientific Research, Computational Mathematics program, grant FA9550-12-1-81243 (2017\u20132020). O. Ghattas (PI), G. Biros, M. Girolami, M. Heinkenschloss, R. Moser, A. Philpott, A. Stuart, and K. Willcox (Co-PIs), Inference, Simulation, and Optimization of Complex Systems Under Uncertainty: Theory, Algorithms, and Applications to Turbulent Combustion , DARPA/ARO, Grants No W911NF-15-2-0121 (2016-2017). O. Ghattas (PI), M. Hesse (Co-PI), CDS&E: A Bayesian inference framework for management of CO2 sequestration , National Science Foundation, Division Of Chemical, Bioengineering, Environmental, & Transport Systems, grant CBET-1508713 (2015-2017). License GNU General Public License version 2 (GPL) Older Releases (1.2.0 or older): GNU General Public License version 3 (GPL) Website built with MkDocs , bootstrap , bootswatch , and MathJax . Hosted on GitHub .","title":"About"},{"location":"about/#about-hippylib","text":"","title":"About hIPPYlib"},{"location":"about/#authors","text":"Umberto Villa Noemi Petra Omar Ghattas","title":"Authors"},{"location":"about/#current-contributors","text":"Amal Alghamdi (UT Austin), Olalekan Babaniyi (RIT), Joshua Chen (UT Austin), Peng Chen (UT Austin), Argho Datta (WashU), Ki-Tae Kim (UC Merced), Tom O'Leary-Roseberry (UT Austin), Luke Lozenski (WashU), Siddhant Wahal (UT Austin)","title":"Current contributors"},{"location":"about/#past-contributors","text":"Ilona Ambartsumyan , Ben Crestel, Eldar Khattatov , Vishwas Rao","title":"Past contributors"},{"location":"about/#copyright","text":"\u00a9 2016-2018 The University of Texas at Austin, University of California Merced. \u00a9 2019-2020 The University of Texas at Austin, University of California Merced, Washington University in St. Louis.","title":"Copyright"},{"location":"about/#support","text":"O. Ghattas, N. Petra (PIs), U. Villa (Co-PI), Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SSI2, Grants No ACI-1550593, ACI-1550547 (2016-2019). O. Ghattas (PI); G. Biros and Y. Marzouk (Co-PIs), Bayesian Optimal Experimental Design for Inverse Scattering , Air Force Office of Scientific Research, Computational Mathematics program, grant FA9550-12-1-81243 (2017\u20132020). O. Ghattas (PI), G. Biros, M. Girolami, M. Heinkenschloss, R. Moser, A. Philpott, A. Stuart, and K. Willcox (Co-PIs), Inference, Simulation, and Optimization of Complex Systems Under Uncertainty: Theory, Algorithms, and Applications to Turbulent Combustion , DARPA/ARO, Grants No W911NF-15-2-0121 (2016-2017). O. Ghattas (PI), M. Hesse (Co-PI), CDS&E: A Bayesian inference framework for management of CO2 sequestration , National Science Foundation, Division Of Chemical, Bioengineering, Environmental, & Transport Systems, grant CBET-1508713 (2015-2017).","title":"Support"},{"location":"about/#license","text":"GNU General Public License version 2 (GPL) Older Releases (1.2.0 or older): GNU General Public License version 3 (GPL) Website built with MkDocs , bootstrap , bootswatch , and MathJax . Hosted on GitHub .","title":"License"},{"location":"documentation/","text":"Documentation The complete API reference of hIPPYlib is available at readthedocs . Algorithms implemented in hIPPYlib are described in Umberto Villa, Noemi Petra, and Omar Ghattas. 2021. HIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference . ACM Trans. Math. Softw. 47, 2, Article 16 (March 2021), 34 pages. arXiv Installation of stable releases The latest hIPPYlib release depends on FEniCS versions 2019.1. FEniCS needs to be built with the following dependecies: numpy , scipy , matplotlib , mpi4py PETSc and petsc4py (version 3.7.0 or above) SLEPc and slepc4py (version 3.7.0 or above) PETSc dependencies: parmetis , scotch , suitesparse , superlu_dist , ml , hypre (optional): mshr , jupyter For detailed installation instructions of the latest stable release see here . Custom Docker Images and conda packages for FEniCS 2019.1.0 While hIPPYlib can be used with the Docker images and conda packages from the ufficial FEniCS packages, a customized FEniCS docker image and customized conda packages is available. hIPPYlib 2.3.0 Docker container A Docker image with a working installation of hIPPYlib 2.3.0 and FEniCS 2017.2 is available here . Numerical results presented in the manuscript hIPPYlib: A Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference were obtained using the software in this image.","title":"Documentation"},{"location":"documentation/#documentation","text":"The complete API reference of hIPPYlib is available at readthedocs . Algorithms implemented in hIPPYlib are described in Umberto Villa, Noemi Petra, and Omar Ghattas. 2021. HIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference . ACM Trans. Math. Softw. 47, 2, Article 16 (March 2021), 34 pages. arXiv","title":"Documentation"},{"location":"documentation/#installation-of-stable-releases","text":"The latest hIPPYlib release depends on FEniCS versions 2019.1. FEniCS needs to be built with the following dependecies: numpy , scipy , matplotlib , mpi4py PETSc and petsc4py (version 3.7.0 or above) SLEPc and slepc4py (version 3.7.0 or above) PETSc dependencies: parmetis , scotch , suitesparse , superlu_dist , ml , hypre (optional): mshr , jupyter For detailed installation instructions of the latest stable release see here .","title":"Installation of stable releases"},{"location":"documentation/#custom-docker-images-and-conda-packages-for-fenics-201910","text":"While hIPPYlib can be used with the Docker images and conda packages from the ufficial FEniCS packages, a customized FEniCS docker image and customized conda packages is available.","title":"Custom Docker Images and conda packages for FEniCS 2019.1.0"},{"location":"documentation/#hippylib-230-docker-container","text":"A Docker image with a working installation of hIPPYlib 2.3.0 and FEniCS 2017.2 is available here . Numerical results presented in the manuscript hIPPYlib: A Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference were obtained using the software in this image.","title":"hIPPYlib 2.3.0 Docker container"},{"location":"download/","text":"Download Latest release Download hippylib-3.1.0.zip All releases Filename Version Release Date Notes Documentation hippylib-3.1.0.zip 3.1.0 Dec 2022 Non-Gaussian noise model and interpolation utilities html hippylib-3.0.0.zip 3.0.0 Feb 2020 Support for FEniCS 2019.1. New features and examples html hippylib-2.3.0.zip 2.3.0 Sept 2019 Updated examples html hippylib-2.2.1.zip 2.2.1 March 2019 Bugfixes html hippylib-2.2.0.zip 2.2.0 Dec 2018 Enchantments and install html hippylib-2.1.1.zip 2.1.1 Oct 2018 Publication to JOSS html hippylib-2.1.0.tar.gz 2.1.0 July 2018 Enchantments & Bugfixes html hippylib-2.0.0.tar.gz 2.0.0 June 2018 Major release html hippylib-1.6.0.tar.gz 1.6.0 May 2018 Bugfix hippylib-1.5.0.tar.gz 1.5.0 Jan 2018 Support for FEniCS 2017.2 hippylib-1.4.0.tar.gz 1.4.0 Nov 2017 Support for Python 3 hippylib-1.3.0.tar.gz 1.3.0 June 2017 Support for FEniCS 2017.1; GLPv2 hippylib-1.2.0.tar.gz 1.2.0 Apr 2017 Support for FEniCS 2016.2 hippylib-1.1.0.tar.gz 1.1.0 Nov 2016 Support for FEniCS 2016.1 hippylib-1.0.2.tar.gz 1.0.2 Sept 2016 Enchantment hippylib-1.0.1.tar.gz 1.0.1 Aug 2016 Bugfix hippylib-1.0.0.tar.gz 1.0.0 Aug 2016 Initial release hIPPYlib releases as of 1.2.0 are also archived on Zenodo . See the changelog on readthedocs for further details.","title":"Download"},{"location":"download/#download","text":"","title":"Download"},{"location":"download/#latest-release","text":"Download hippylib-3.1.0.zip","title":"Latest release"},{"location":"download/#all-releases","text":"Filename Version Release Date Notes Documentation hippylib-3.1.0.zip 3.1.0 Dec 2022 Non-Gaussian noise model and interpolation utilities html hippylib-3.0.0.zip 3.0.0 Feb 2020 Support for FEniCS 2019.1. New features and examples html hippylib-2.3.0.zip 2.3.0 Sept 2019 Updated examples html hippylib-2.2.1.zip 2.2.1 March 2019 Bugfixes html hippylib-2.2.0.zip 2.2.0 Dec 2018 Enchantments and install html hippylib-2.1.1.zip 2.1.1 Oct 2018 Publication to JOSS html hippylib-2.1.0.tar.gz 2.1.0 July 2018 Enchantments & Bugfixes html hippylib-2.0.0.tar.gz 2.0.0 June 2018 Major release html hippylib-1.6.0.tar.gz 1.6.0 May 2018 Bugfix hippylib-1.5.0.tar.gz 1.5.0 Jan 2018 Support for FEniCS 2017.2 hippylib-1.4.0.tar.gz 1.4.0 Nov 2017 Support for Python 3 hippylib-1.3.0.tar.gz 1.3.0 June 2017 Support for FEniCS 2017.1; GLPv2 hippylib-1.2.0.tar.gz 1.2.0 Apr 2017 Support for FEniCS 2016.2 hippylib-1.1.0.tar.gz 1.1.0 Nov 2016 Support for FEniCS 2016.1 hippylib-1.0.2.tar.gz 1.0.2 Sept 2016 Enchantment hippylib-1.0.1.tar.gz 1.0.1 Aug 2016 Bugfix hippylib-1.0.0.tar.gz 1.0.0 Aug 2016 Initial release hIPPYlib releases as of 1.2.0 are also archived on Zenodo . See the changelog on readthedocs for further details.","title":"All releases"},{"location":"oral_presentations/","text":"Selected oral presentations N. Petra, Inferring the basal sliding coefficient for the Stokes ice sheet model under rheological uncertainty , Mathematical Modelling in Glaciology, Banff International Research Station, Banff, AB, Candada I. Ambartsumyan, O. Ghattas , Fast methods for Bayesian inverse problems governed by PDE forward models with random coefficient fields , Applied Inverse Problems Conference, July 8-12, 2019, Grenoble, France U. Villa , O. Ghattas, Scalable optimal experimental design for large scale non-linear Bayesian inverse problems , Applied Inverse Problems Conference, July 8-12, 2019, Grenoble, France O. Babaniyi , O. Ghattas, N. Petra and U. Villa, hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems , 2019 FEniCS Conference, Carnegie Institution for Science Department of Terrestrial Magnetism (DTM), Washington DC,June 12-14, 2019. Best postdoc presentation award N. Petra, G. Stadler , Inverse Problems: Integrating Data with Models under Uncertainty , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US K. Wu , P. Chen, O. Ghattas, A Stein Variational Newton Method for Optimal Experiment Design Problems , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US P. Chen , O. Ghattas, U. Villa, Large-scale Optimal Experimental Design for Bayesian Nonlinear Inverse Problems , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US T. O'Leary-Roseberry , J. Chen, P. Chen, U. Villa, O. Ghattas, Large-scale Optimization in Deep Learning for PDE Representation abstract , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US J. J. Lee , O. Ghattas, T. Bui-Thanh, U. Villa, Derivative Informed MCMC Methods for Subsurface Models with Faults , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US U. Villa , O. Ghattas, Scalable Methods for Bayesian Optimal Experimental Design Using Laplace Approximation , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US U. Villa , Learning from data through the lens of mathematical models , Analysis Seminar, Dept. of Mathematics & Statistics, Washington University, January 28, 2019, St Louis, MO, US U. Villa , Large Scale Inverse Problems and Uncertainty Quantification: Computational Tools and Imaging Applications , Electrical & Systems Engineering Seminar, Washington University, January 24, 2019, St Louis, MO, US N. Petra , hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems , Optimization Seminar, University of California, Merced, October 19, 2018, Merced, CA, US U. Villa , O. Ghattas, Maximize the Expected Information Gain in Bayesian Experimental Design Problems: A Fast Optimization Algorithm Based on Laplace Approximation and Randomized Eigensolvers , SIAM UQ, April 16-19, 2018, Garden Grove, CA, US T. O\u2019Leary-Roseberry , A PDE Constrained Optimization Approach to the Solution of the Stefan Problem , Texas Applied Mathematics and Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US U. Villa , hIPPYlib: An Extensible Software Framework for Large-Scale Deterministic and Linearized Bayesian Inverse , Texas Applied Mathematics and Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US U. Villa , Taylor Approximation for PDE-constrained Optimal Control Problems under High-dimensional Uncertainty , SIAM Control, July 10 - 12, 2017, Pittsburgh, Pa, US U. Villa , Derivative-Informed MCMC for Bayesian Calibration of Stochastic PDE Models , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US U. Villa , Hessian-based sampling techniques for Bayesian inverse problems with stochastic PDE forward model , Applied Inverse Problems, May 29 - June 2, 2017, Hangzhou, China U. Villa , Bayesian Calibration of Inadequate Stochastic PDE Models , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US P. Chen , Taylor Approximation for PDE-Constrained Optimal Control Problems Under High-Dimensional Uncertainty: Application to a Turbulence Model , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US B. Crestel , Scalable Solvers for Joint Inversion with Several Structural Coupling Terms , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US Amal Alghamdi , Bayesian Inversion for Subsurface Properties from Poroelastic Forward Models and Surface Deformation Data , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US U. Villa , An Analytical Technique for Forward and Inverse Propagation of Uncertainty , SIAM UQ, April 5-8, 2016, Lausanne, Switzerland","title":"Oral presentations"},{"location":"oral_presentations/#selected-oral-presentations","text":"N. Petra, Inferring the basal sliding coefficient for the Stokes ice sheet model under rheological uncertainty , Mathematical Modelling in Glaciology, Banff International Research Station, Banff, AB, Candada I. Ambartsumyan, O. Ghattas , Fast methods for Bayesian inverse problems governed by PDE forward models with random coefficient fields , Applied Inverse Problems Conference, July 8-12, 2019, Grenoble, France U. Villa , O. Ghattas, Scalable optimal experimental design for large scale non-linear Bayesian inverse problems , Applied Inverse Problems Conference, July 8-12, 2019, Grenoble, France O. Babaniyi , O. Ghattas, N. Petra and U. Villa, hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems , 2019 FEniCS Conference, Carnegie Institution for Science Department of Terrestrial Magnetism (DTM), Washington DC,June 12-14, 2019. Best postdoc presentation award N. Petra, G. Stadler , Inverse Problems: Integrating Data with Models under Uncertainty , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US K. Wu , P. Chen, O. Ghattas, A Stein Variational Newton Method for Optimal Experiment Design Problems , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US P. Chen , O. Ghattas, U. Villa, Large-scale Optimal Experimental Design for Bayesian Nonlinear Inverse Problems , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US T. O'Leary-Roseberry , J. Chen, P. Chen, U. Villa, O. Ghattas, Large-scale Optimization in Deep Learning for PDE Representation abstract , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US J. J. Lee , O. Ghattas, T. Bui-Thanh, U. Villa, Derivative Informed MCMC Methods for Subsurface Models with Faults , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US U. Villa , O. Ghattas, Scalable Methods for Bayesian Optimal Experimental Design Using Laplace Approximation , SIAM CSE, Feb 25-March 1, 2019, Spokane, Wa, US U. Villa , Learning from data through the lens of mathematical models , Analysis Seminar, Dept. of Mathematics & Statistics, Washington University, January 28, 2019, St Louis, MO, US U. Villa , Large Scale Inverse Problems and Uncertainty Quantification: Computational Tools and Imaging Applications , Electrical & Systems Engineering Seminar, Washington University, January 24, 2019, St Louis, MO, US N. Petra , hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems , Optimization Seminar, University of California, Merced, October 19, 2018, Merced, CA, US U. Villa , O. Ghattas, Maximize the Expected Information Gain in Bayesian Experimental Design Problems: A Fast Optimization Algorithm Based on Laplace Approximation and Randomized Eigensolvers , SIAM UQ, April 16-19, 2018, Garden Grove, CA, US T. O\u2019Leary-Roseberry , A PDE Constrained Optimization Approach to the Solution of the Stefan Problem , Texas Applied Mathematics and Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US U. Villa , hIPPYlib: An Extensible Software Framework for Large-Scale Deterministic and Linearized Bayesian Inverse , Texas Applied Mathematics and Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US U. Villa , Taylor Approximation for PDE-constrained Optimal Control Problems under High-dimensional Uncertainty , SIAM Control, July 10 - 12, 2017, Pittsburgh, Pa, US U. Villa , Derivative-Informed MCMC for Bayesian Calibration of Stochastic PDE Models , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US U. Villa , Hessian-based sampling techniques for Bayesian inverse problems with stochastic PDE forward model , Applied Inverse Problems, May 29 - June 2, 2017, Hangzhou, China U. Villa , Bayesian Calibration of Inadequate Stochastic PDE Models , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US P. Chen , Taylor Approximation for PDE-Constrained Optimal Control Problems Under High-Dimensional Uncertainty: Application to a Turbulence Model , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US B. Crestel , Scalable Solvers for Joint Inversion with Several Structural Coupling Terms , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US Amal Alghamdi , Bayesian Inversion for Subsurface Properties from Poroelastic Forward Models and Surface Deformation Data , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US U. Villa , An Analytical Technique for Forward and Inverse Propagation of Uncertainty , SIAM UQ, April 5-8, 2016, Lausanne, Switzerland","title":"Selected oral presentations"},{"location":"outreach/","text":"Outreach Schools and workshops O. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa, Inverse Problems: Systematic Integration of Data with Models under Uncertainty , 2018 Gene Golub SIAM Summer School, June 17-30, 2018. Breckenridge, Colorado, USA. See also announcement flyer N. Petra, SAMSI Optimization Program Summer School , Research Triangle Park, NC, August 8-12, 2016 N. Petra and O. Ghattas, IDEALab: Inverse Problems and Uncertainty Quantification , Brown University, Providence, RD, July 6-10, 2015 Graduate level courses U. Villa @WUSTL, Computational Methods for Imaging Science , Spring 2020 U. Villa @WUSTL, Computational Methods for Imaging Science , Spring 2018 O. Ghattas @UT Austin, Computational and Variational Inverse Problems , Fall 2017 link A. Alexanderian @NC State, Inverse problems , Fall 2016 G. Stadler @NYU, Advanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems , Spring 2016 link N. Petra @UC Merced, Special Topics: Computational and Variational Inverse Problems , Fall 2015 link O. Ghattas @UT Austin, Computational and Variational Inverse Problems , Fall 2015 link","title":"Outreach"},{"location":"outreach/#outreach","text":"","title":"Outreach"},{"location":"outreach/#schools-and-workshops","text":"O. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa, Inverse Problems: Systematic Integration of Data with Models under Uncertainty , 2018 Gene Golub SIAM Summer School, June 17-30, 2018. Breckenridge, Colorado, USA. See also announcement flyer N. Petra, SAMSI Optimization Program Summer School , Research Triangle Park, NC, August 8-12, 2016 N. Petra and O. Ghattas, IDEALab: Inverse Problems and Uncertainty Quantification , Brown University, Providence, RD, July 6-10, 2015","title":"Schools and workshops"},{"location":"outreach/#graduate-level-courses","text":"U. Villa @WUSTL, Computational Methods for Imaging Science , Spring 2020 U. Villa @WUSTL, Computational Methods for Imaging Science , Spring 2018 O. Ghattas @UT Austin, Computational and Variational Inverse Problems , Fall 2017 link A. Alexanderian @NC State, Inverse problems , Fall 2016 G. Stadler @NYU, Advanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems , Spring 2016 link N. Petra @UC Merced, Special Topics: Computational and Variational Inverse Problems , Fall 2015 link O. Ghattas @UT Austin, Computational and Variational Inverse Problems , Fall 2015 link","title":"Graduate level courses"},{"location":"postdoc_position/","text":"Openings Open Postdoc Position at UC Merced There is an opening for a postdoc position in Professor Noemi Petra 's research group in the School of Natural Sciences at the University of California, Merced . The postdoctoral researcher will work under an NSF-funded Collaborative Research (with UC Merced, UT Austin and MIT) entitled: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion (see here for a general overview of the project). The postdoctoral researcher will perform research in the field of large-scale Bayesian inverse problems, and on the implementation of new features in hIPPYlib. The postdoc will contribute to the research dissemination and will also help build the user/developer community by attending and speaking at conferences, workshops and summer schools at local and international events. Interested candidates should contact Noemi Petra at npetra@ucmerced.edu . Apply at https://aprecruit.ucmerced.edu/apply/JPF00738 .","title":"Openings"},{"location":"postdoc_position/#openings","text":"","title":"Openings"},{"location":"postdoc_position/#open-postdoc-position-at-uc-merced","text":"There is an opening for a postdoc position in Professor Noemi Petra 's research group in the School of Natural Sciences at the University of California, Merced . The postdoctoral researcher will work under an NSF-funded Collaborative Research (with UC Merced, UT Austin and MIT) entitled: Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion (see here for a general overview of the project). The postdoctoral researcher will perform research in the field of large-scale Bayesian inverse problems, and on the implementation of new features in hIPPYlib. The postdoc will contribute to the research dissemination and will also help build the user/developer community by attending and speaking at conferences, workshops and summer schools at local and international events. Interested candidates should contact Noemi Petra at npetra@ucmerced.edu . Apply at https://aprecruit.ucmerced.edu/apply/JPF00738 .","title":"Open Postdoc Position at UC Merced"},{"location":"research/","text":"Research Applications Inversion for optical properties of biological tissues in quantitative optoacoustic tomography Statistical treatment of inverse problems constrained by stochastic models Accounting for model errors in inverse problems Bayesian optimal experimental design for inverse problems in acoustic scattering Inversion and control for CO 2 sequestration with poroelastic models Goal-oriented inference for reservoir models with complex features including faults Joint seismic-electromagnetic inversion Inference of basal boundary conditions for ice sheet flow Inversion for material properties of cardiac tissue Inference, prediction and optimization under uncertainty for turbulent combustion Selected publications T. O'Leary-Roseberry, U. Villa, P. Chen, and O. Ghattas: Derivative-Informed Projected Neural Networks for High-Dimensional Parametric Maps Governed by PDEs , Computer Methods in Applied Mechanics and Engineering, accepted, 2021 O. Babaniyi, R. Nicholson, U. Villa, and N. Petra: Inferring the basal sliding coefficient field for the Stokes ice sheet model under rheological uncertainty , The Cryosphere Discuss. [preprint], accepted, 2021. U. Villa, N. Petra, O. Ghattas, hIPPYlib: An extensible software framework for large-scale inverse problems; Part I: Deterministic inversion and linearized Bayesian inference , ACM Trans. Math. Softw. 47, 2, Article 16 (March 2021), 34 pages, 2021 A. Alghamdi, M.~A. Hesse, J. Chen, O. Ghattas, Bayesian Poroelastic Aquifer Characterization from InSAR Surface Deformation Data. Part I: Maximum A Posteriori Estimate , Water Resources Research, e2020WR027391, 2020 K. Koval, A. Alexanderian, G. Stadler, Optimal experimental design under irreducible uncertainty for inverse problems governed by PDEs , arXiv, 2019 S. Wahal, G. Biros, BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented uncertainty quantification. Part I , arXiv, 2019 S. Lan, Adaptive dimension reduction to accelerate infinite-dimensional geometric Markov Chain Monte Carlo , Journal of Computational Physics, 392:71-95, 2019 P. Chen, U. Villa, O. Ghattas, Taylor approximation and variance reduction for PDE-constrained optimal control under uncertainty , Journal of Computational Physics, 385:163--186, 2019 B. Crestel, G. Stadler and O. Ghattas, A comparative study of structural similarity and regularization for joint inverse problems governed by PDEs , Inverse Problems, 35:024003, 2018 A. Attia, A. Alexanderian, A. K. Saibaba, Goal-oriented optimal design of experiments for large-scale Bayesian linear inverse problems , Inverse Problems, 34:095009, 2018 R. Nicholson, N. Petra and Jari P Kaipio. Estimation of the Robin coefficient field in a Poisson problem with uncertain conductivity field , Inverse Problems, Volume 34, Number 11, 2018. P. Chen, K. Wu, J. Chen, T. O'Leary-Roseberry, O. Ghattas, Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions , arXiv, 2018 E. M. Constantinescu, N. Petra, J. Bessac, C. G. Petra, Statistical Treatment of Inverse Problems Constrained by Differential Equations-Based Models with Stochastic Terms , arXiv, 2018 U. Villa, N. Petra, O. Ghattas, hIPPYlib: An extensible software framework for large-scale inverse problems , Journal of Open Source Software (JOSS), 3(30):940, 2018 P. Chen, U. Villa, O. Ghattas, Taylor approximation for PDE-constrained optimization under uncertainty: Application to turbulent jet flow , Proceedings in Applied Mathematics and Mechanics - 89th GAMM Annual Meeting, 18:e201800466, 2018 P. Chen, U. Villa, O. Ghattas, Hessian-based adaptive sparse quadrature for infinite-dimensional Bayesian inverse problems , Computer Methods in Applied Mechanics and Engineering, 327:147-172, 2017 S. Fatehiboroujeni, N. Petra and S. Goyal. Towards Adjoint-Based Inversion of the Lam\u00e9 Parameter Field for Slender Structures With Cantilever Loading , ASME 2016 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, Volume 8: 28th Conference on Mechanical Vibration and Noise Charlotte, North Carolina, USA, August 21\u201324, 2016. Selected Ph.D. thesis T. O'Leary-Roseberry, Efficient and Dimension Independent Methods for Neural Network Surrogate Construction and Training , The University of Texas at Austin, 2020. Adviser O. Ghattas & P. Heimbach A. Alghamdi, Bayesian Inverse Problems for Quasi-Static Poroelasticity with Application to Ground Water Aquifer Characterization from Geodetic Data , The University of Texas at Austin, 2020. Adviser O. Ghattas & M. Hesse S. Fatehiboroujeni, Inverse Approaches for Identification of Constitutive Laws of Slender Structures Motivated by Application to Biological Filaments , University of California, Merced, 2018. Adviser S. Goyal K. A. McCormack, Earthquakes, groundwater and surface deformation: exploring the poroelastic response to megathrust earthquakes , The University of Texas at Austin, 2018. Adviser M. Hesse B. Crestel, Advanced techniques for multi-source, multi-parameter, and multi-physics inverse problems , The University of Texas at Austin, 2017. Adviser O. Ghattas Selected Honor and Master thesis B. Saleh, Scientific Machine Learning: A Neural Network-Based Estimator for Forward Uncertainty Quantification , The University of Texas at Austin, 2018. Adviser O. Ghattas G. Gao, hIPPYLearn: An inexact Newton-CG method for training neural networks with analysis of the Hessian , The University of Texas at Austin, 2017. Supervisor O. Ghattas D. Liu, hIPPYLearn: An inexact stochastic Newton-CG method for training neural networks , The University of Texas at Austin, 2017. Supervisor O. Ghattas Selected poster presentations O. Ghattas, K. Kim, Y. Marzouk, M. Parno, N. Petra, U. Villa, Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-CSSI PI meeting, Feb. 13-14, Seattle, Wa, US I. Ambartsumyan, T. Bui-Thanh, O. Ghattas, E. Khattatov, An Edge-preserving Method for Joint Bayesian Inversion with Non-Gaussian Priors , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US E. Khattatov, O. Ghattas, T. Bui-Thanh, and I. Ambartsumyan, U. Villa, Bayesian Inversion of Fault Properties in Two-phase Flow in Fractured Media , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US A. O. Babaniyi, O. Ghattas, N. Petra, U. Villa, hIPPYlib: An Extensible Software Framework for Large-scale Inverse Problems , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Apr. 30- May 1, 2018, Washington, DC, US K. Koval, G. Stadler, Computational Approaches for Linear Goal-Oriented Bayesian Inverse Problems , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US J. Chen, A. Drach, U. Villa, R. Avazmohammadi, D. Li, O. Ghattas and M. Sacks, Identification of Mechanical Properties of 3D Myocardial Tissue: An Inverse Modeling and Optimal Experimental Design Problem , FEniCS'17, June 12-14, 2017, University of Luxembourg, Luxembourg T. O\u2019Leary-Roseberry, U. Villa, O. Ghattas, P. Heimbach, An Adjoint Capable Solver for the Stefan Problem: a Bilevel Optimization and Level Set Approach , SIAM CSE, Feb. 27 - March 3, 2017, Atlanta, GA, US O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Feb. 21-22, 2017, Arlington, VA, US Did you publish an article or give an oral/poster presentation using hIPPYlib ? Please let us know by creating an issue and we will reference your work on this page.","title":"Research"},{"location":"research/#research","text":"","title":"Research"},{"location":"research/#applications","text":"Inversion for optical properties of biological tissues in quantitative optoacoustic tomography Statistical treatment of inverse problems constrained by stochastic models Accounting for model errors in inverse problems Bayesian optimal experimental design for inverse problems in acoustic scattering Inversion and control for CO 2 sequestration with poroelastic models Goal-oriented inference for reservoir models with complex features including faults Joint seismic-electromagnetic inversion Inference of basal boundary conditions for ice sheet flow Inversion for material properties of cardiac tissue Inference, prediction and optimization under uncertainty for turbulent combustion","title":"Applications"},{"location":"research/#selected-publications","text":"T. O'Leary-Roseberry, U. Villa, P. Chen, and O. Ghattas: Derivative-Informed Projected Neural Networks for High-Dimensional Parametric Maps Governed by PDEs , Computer Methods in Applied Mechanics and Engineering, accepted, 2021 O. Babaniyi, R. Nicholson, U. Villa, and N. Petra: Inferring the basal sliding coefficient field for the Stokes ice sheet model under rheological uncertainty , The Cryosphere Discuss. [preprint], accepted, 2021. U. Villa, N. Petra, O. Ghattas, hIPPYlib: An extensible software framework for large-scale inverse problems; Part I: Deterministic inversion and linearized Bayesian inference , ACM Trans. Math. Softw. 47, 2, Article 16 (March 2021), 34 pages, 2021 A. Alghamdi, M.~A. Hesse, J. Chen, O. Ghattas, Bayesian Poroelastic Aquifer Characterization from InSAR Surface Deformation Data. Part I: Maximum A Posteriori Estimate , Water Resources Research, e2020WR027391, 2020 K. Koval, A. Alexanderian, G. Stadler, Optimal experimental design under irreducible uncertainty for inverse problems governed by PDEs , arXiv, 2019 S. Wahal, G. Biros, BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented uncertainty quantification. Part I , arXiv, 2019 S. Lan, Adaptive dimension reduction to accelerate infinite-dimensional geometric Markov Chain Monte Carlo , Journal of Computational Physics, 392:71-95, 2019 P. Chen, U. Villa, O. Ghattas, Taylor approximation and variance reduction for PDE-constrained optimal control under uncertainty , Journal of Computational Physics, 385:163--186, 2019 B. Crestel, G. Stadler and O. Ghattas, A comparative study of structural similarity and regularization for joint inverse problems governed by PDEs , Inverse Problems, 35:024003, 2018 A. Attia, A. Alexanderian, A. K. Saibaba, Goal-oriented optimal design of experiments for large-scale Bayesian linear inverse problems , Inverse Problems, 34:095009, 2018 R. Nicholson, N. Petra and Jari P Kaipio. Estimation of the Robin coefficient field in a Poisson problem with uncertain conductivity field , Inverse Problems, Volume 34, Number 11, 2018. P. Chen, K. Wu, J. Chen, T. O'Leary-Roseberry, O. Ghattas, Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions , arXiv, 2018 E. M. Constantinescu, N. Petra, J. Bessac, C. G. Petra, Statistical Treatment of Inverse Problems Constrained by Differential Equations-Based Models with Stochastic Terms , arXiv, 2018 U. Villa, N. Petra, O. Ghattas, hIPPYlib: An extensible software framework for large-scale inverse problems , Journal of Open Source Software (JOSS), 3(30):940, 2018 P. Chen, U. Villa, O. Ghattas, Taylor approximation for PDE-constrained optimization under uncertainty: Application to turbulent jet flow , Proceedings in Applied Mathematics and Mechanics - 89th GAMM Annual Meeting, 18:e201800466, 2018 P. Chen, U. Villa, O. Ghattas, Hessian-based adaptive sparse quadrature for infinite-dimensional Bayesian inverse problems , Computer Methods in Applied Mechanics and Engineering, 327:147-172, 2017 S. Fatehiboroujeni, N. Petra and S. Goyal. Towards Adjoint-Based Inversion of the Lam\u00e9 Parameter Field for Slender Structures With Cantilever Loading , ASME 2016 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, Volume 8: 28th Conference on Mechanical Vibration and Noise Charlotte, North Carolina, USA, August 21\u201324, 2016.","title":"Selected publications"},{"location":"research/#selected-phd-thesis","text":"T. O'Leary-Roseberry, Efficient and Dimension Independent Methods for Neural Network Surrogate Construction and Training , The University of Texas at Austin, 2020. Adviser O. Ghattas & P. Heimbach A. Alghamdi, Bayesian Inverse Problems for Quasi-Static Poroelasticity with Application to Ground Water Aquifer Characterization from Geodetic Data , The University of Texas at Austin, 2020. Adviser O. Ghattas & M. Hesse S. Fatehiboroujeni, Inverse Approaches for Identification of Constitutive Laws of Slender Structures Motivated by Application to Biological Filaments , University of California, Merced, 2018. Adviser S. Goyal K. A. McCormack, Earthquakes, groundwater and surface deformation: exploring the poroelastic response to megathrust earthquakes , The University of Texas at Austin, 2018. Adviser M. Hesse B. Crestel, Advanced techniques for multi-source, multi-parameter, and multi-physics inverse problems , The University of Texas at Austin, 2017. Adviser O. Ghattas","title":"Selected Ph.D. thesis"},{"location":"research/#selected-honor-and-master-thesis","text":"B. Saleh, Scientific Machine Learning: A Neural Network-Based Estimator for Forward Uncertainty Quantification , The University of Texas at Austin, 2018. Adviser O. Ghattas G. Gao, hIPPYLearn: An inexact Newton-CG method for training neural networks with analysis of the Hessian , The University of Texas at Austin, 2017. Supervisor O. Ghattas D. Liu, hIPPYLearn: An inexact stochastic Newton-CG method for training neural networks , The University of Texas at Austin, 2017. Supervisor O. Ghattas","title":"Selected Honor and Master thesis"},{"location":"research/#selected-poster-presentations","text":"O. Ghattas, K. Kim, Y. Marzouk, M. Parno, N. Petra, U. Villa, Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-CSSI PI meeting, Feb. 13-14, Seattle, Wa, US I. Ambartsumyan, T. Bui-Thanh, O. Ghattas, E. Khattatov, An Edge-preserving Method for Joint Bayesian Inversion with Non-Gaussian Priors , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US E. Khattatov, O. Ghattas, T. Bui-Thanh, and I. Ambartsumyan, U. Villa, Bayesian Inversion of Fault Properties in Two-phase Flow in Fractured Media , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US A. O. Babaniyi, O. Ghattas, N. Petra, U. Villa, hIPPYlib: An Extensible Software Framework for Large-scale Inverse Problems , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Apr. 30- May 1, 2018, Washington, DC, US K. Koval, G. Stadler, Computational Approaches for Linear Goal-Oriented Bayesian Inverse Problems , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US J. Chen, A. Drach, U. Villa, R. Avazmohammadi, D. Li, O. Ghattas and M. Sacks, Identification of Mechanical Properties of 3D Myocardial Tissue: An Inverse Modeling and Optimal Experimental Design Problem , FEniCS'17, June 12-14, 2017, University of Luxembourg, Luxembourg T. O\u2019Leary-Roseberry, U. Villa, O. Ghattas, P. Heimbach, An Adjoint Capable Solver for the Stefan Problem: a Bilevel Optimization and Level Set Approach , SIAM CSE, Feb. 27 - March 3, 2017, Atlanta, GA, US O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Feb. 21-22, 2017, Arlington, VA, US Did you publish an article or give an oral/poster presentation using hIPPYlib ? Please let us know by creating an issue and we will reference your work on this page.","title":"Selected poster presentations"},{"location":"tutorial_v1.6.0/","text":"Tutorial These tutorials are the best place to learn about the basic features and the algorithms in hIPPYlib . FEniCS101 notebook illustrates the use of FEniCS for the solution of a linear boundary value problem. Poisson Deterministic notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting. Subsurface Bayesian notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting. Advection-Diffusion Bayesian notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting. Hessian Spectrum notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem. The interactive ipython notebooks are located in the tutorial folder of the hIPPYlib release. To run the notebooks follow these instructions. Open a FEniCS terminal and type $ cd tutorial $ jupyter notebook A new tab will open in your web-brower showing the notebooks. Click on the notebook you would like to use. To run all the code in the notebook simply click on Cell --> Run All. For more information on installing ipython and using notebooks see here .","title":"Tutorial"},{"location":"tutorial_v1.6.0/#tutorial","text":"These tutorials are the best place to learn about the basic features and the algorithms in hIPPYlib . FEniCS101 notebook illustrates the use of FEniCS for the solution of a linear boundary value problem. Poisson Deterministic notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting. Subsurface Bayesian notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting. Advection-Diffusion Bayesian notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting. Hessian Spectrum notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem. The interactive ipython notebooks are located in the tutorial folder of the hIPPYlib release. To run the notebooks follow these instructions. Open a FEniCS terminal and type $ cd tutorial $ jupyter notebook A new tab will open in your web-brower showing the notebooks. Click on the notebook you would like to use. To run all the code in the notebook simply click on Cell --> Run All. For more information on installing ipython and using notebooks see here .","title":"Tutorial"},{"location":"tutorial_v2.3.0/","text":"Tutorial hIPPYlib 2.3.0 These tutorials are the best place to learn about the basic features and the algorithms in hIPPYlib . For the complete API reference click here . FEniCS101 notebook illustrates the use of FEniCS for the solution of a linear boundary value problem. Poisson Deterministic notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting. Subsurface Bayesian notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting Advection-Diffusion Bayesian notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting. Hessian Spectrum notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem. Tutorials 3 and 4 contain a line-by-line explanation of the model problems presented in hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs; Part I: Deterministic Inversion and Linearized Bayesian Inference . Interactive tutorials hIPPYlib 2.3.0 is available as source code or prebuilt docker image . The interactive ipython notebooks are located in the tutorial folder of the hIPPYlib release. To run the notebooks follow these instructions. Open a FEniCS terminal and type $ cd tutorial $ jupyter notebook A new tab will open in your web-brower showing the notebooks. Click on the notebook you would like to use. To run all the code in the notebook simply click on Cell --> Run All. For more information on installing ipython and using notebooks see here . Additional resources For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School on Inverse Problems: Systematic Integration of Data with Models under Uncertainty available here .","title":"Tutorial hIPPYlib 2.3.0"},{"location":"tutorial_v2.3.0/#tutorial-hippylib-230","text":"These tutorials are the best place to learn about the basic features and the algorithms in hIPPYlib . For the complete API reference click here . FEniCS101 notebook illustrates the use of FEniCS for the solution of a linear boundary value problem. Poisson Deterministic notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting. Subsurface Bayesian notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting Advection-Diffusion Bayesian notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting. Hessian Spectrum notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem. Tutorials 3 and 4 contain a line-by-line explanation of the model problems presented in hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs; Part I: Deterministic Inversion and Linearized Bayesian Inference .","title":"Tutorial hIPPYlib 2.3.0"},{"location":"tutorial_v2.3.0/#interactive-tutorials","text":"hIPPYlib 2.3.0 is available as source code or prebuilt docker image . The interactive ipython notebooks are located in the tutorial folder of the hIPPYlib release. To run the notebooks follow these instructions. Open a FEniCS terminal and type $ cd tutorial $ jupyter notebook A new tab will open in your web-brower showing the notebooks. Click on the notebook you would like to use. To run all the code in the notebook simply click on Cell --> Run All. For more information on installing ipython and using notebooks see here .","title":"Interactive tutorials"},{"location":"tutorial_v2.3.0/#additional-resources","text":"For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School on Inverse Problems: Systematic Integration of Data with Models under Uncertainty available here .","title":"Additional resources"},{"location":"tutorial_v3.0.0/","text":"Tutorial - Version 3.0.0 These tutorials are the best place to learn about the basic features and the algorithms in hIPPYlib . For the complete API reference click here . Other versions 2.3.0 , 1.6.0 FEniCS101 notebook illustrates the use of FEniCS for the solution of a linear boundary value problem. Poisson Deterministic notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting. Subsurface Bayesian notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting. Advection-Diffusion Bayesian notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting. Hessian Spectrum notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem. Interactive tutorials The interactive ipython notebooks are in the tutorial folder of the hIPPYlib repository. To run the notebooks follow these instructions. Open a FEniCS terminal and type $ cd tutorial $ jupyter notebook A new tab will open in your web-browser showing the notebooks. Click on the notebook you would like to use. To run all the notebook code, simply click Cell --> Run All. For more information on installing ipython and using notebooks see here . Additional resources For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School on Inverse Problems: Systematic Integration of Data with Models under Uncertainty available here . These tutorials require hIPPYlib versions 2.1.1 - 2.3.0.","title":"Tutorial"},{"location":"tutorial_v3.0.0/#tutorial-version-300","text":"These tutorials are the best place to learn about the basic features and the algorithms in hIPPYlib . For the complete API reference click here . Other versions 2.3.0 , 1.6.0 FEniCS101 notebook illustrates the use of FEniCS for the solution of a linear boundary value problem. Poisson Deterministic notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting. Subsurface Bayesian notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting. Advection-Diffusion Bayesian notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting. Hessian Spectrum notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.","title":"Tutorial - Version 3.0.0"},{"location":"tutorial_v3.0.0/#interactive-tutorials","text":"The interactive ipython notebooks are in the tutorial folder of the hIPPYlib repository. To run the notebooks follow these instructions. Open a FEniCS terminal and type $ cd tutorial $ jupyter notebook A new tab will open in your web-browser showing the notebooks. Click on the notebook you would like to use. To run all the notebook code, simply click Cell --> Run All. For more information on installing ipython and using notebooks see here .","title":"Interactive tutorials"},{"location":"tutorial_v3.0.0/#additional-resources","text":"For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School on Inverse Problems: Systematic Integration of Data with Models under Uncertainty available here . These tutorials require hIPPYlib versions 2.1.1 - 2.3.0.","title":"Additional resources"},{"location":"tutorials_v1.6.0/1_FEniCS101/","text":"FEniCS101 Tutorial In this tutorial we consider the boundary value problem (BVP) \\begin{eqnarray*} - \\nabla \\cdot (k \\nabla u) = f & \\text{ in } \\Omega,\\\\ u = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\ k \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom}, \\end{eqnarray*} where \\Omega = (0,1) \\times (0,1) , \\Gamma_D and and \\Gamma_N are the union of the left and right, and top and bottom boundaries of \\Omega , respectively. Here \\begin{eqnarray*} k(x,y) = 1 & \\text{ on } \\Omega\\\\ f(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\ u_0(x,y) = 0 & \\text{ on } \\Gamma_D, \\\\ \\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right. & \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array} \\end{eqnarray*} The exact solution is u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). Weak formulation Let us define the Hilbert spaces V_{u_0}, V_0 \\in \\Omega as V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\}, V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}. To obtain the weak formulation, we multiply the PDE by an arbitrary function v \\in V_0 and integrate over the domain \\Omega leading to -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. Then, integration by parts the non-conforming term gives \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. Finally by recalling that v = 0 on \\Gamma_D and that k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma on \\Gamma_N , we find the weak formulation: Find * u \\in V_{u_0} such that* \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. 1. Load modules To start we load the following modules: dolfin: the python/C++ interface to FEniCS math : the python module for mathematical functions numpy : a python package for linear algebra matplotlib : a python package used for plotting the results from __future__ import absolute_import, division, print_function from dolfin import * import math import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False) 2. Define the mesh and the finite element space We construct a triangulation (mesh) \\mathcal{T}_h of the computational domain \\Omega := [0, 1]^2 with n elements in each direction. On the mesh \\mathcal{T}_h , we then define the finite element space V_h \\subset H^1(\\Omega) consisting of globally continuous piecewise polinomials functions. The degree variable defines the polinomial degree. n = 16 degree = 1 mesh = UnitSquareMesh(n, n) nb.plot(mesh) Vh = FunctionSpace(mesh, 'Lagrange', degree) print(\"dim(Vh) = \", Vh.dim()) dim(Vh) = 289 3. Define boundary labels To partition the boundary of \\Omega in the subdomains \\Gamma_{\\rm top} , \\Gamma_{\\rm bottom} , \\Gamma_{\\rm left} , \\Gamma_{\\rm right} we assign a unique label boundary_parts to each of part of \\partial \\Omega . class TopBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1] - 1) < DOLFIN_EPS class BottomBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1]) < DOLFIN_EPS class LeftBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0]) < DOLFIN_EPS class RightBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0] - 1) < DOLFIN_EPS boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) 4. Define the coefficients of the PDE and the boundary conditions We first define the coefficients of the PDE using the Constant and Expression classes. Constant is used to define coefficients that do not depend on the space coordinates, Expression is used to define coefficients that are a known function of the space coordinates x[0] (x-axis direction) and x[1] (y-axis direction). In the finite element method community, Dirichlet boundary conditions are also known as essential boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class DirichletBC to indicate this type of condition. On the other hand, Newman boundary conditions are also known as natural boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure ds[i] to integrate over the portion of the boundary marked with label i . u_L = Constant(0.) u_R = Constant(0.) sigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5) sigma_top = Constant(0.) f = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts) 5. Define and solve the variational problem We also define two special types of functions: the TrialFunction u and the TestFunction v . These special types of function are used by FEniCS to generate the finite element vectors and matrices which stem from the weak formulation of the PDE. More specifically, by denoting by \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)} the finite element basis for the space V_h , a function u_h \\in V_h can be written as u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), where {\\rm u}_i represents the coefficients in the finite element expansion of u_h . We then define the bilinear form a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx ; the linear form L(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds . We can then solve the variational problem Find u_h \\in V_h such that a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h using directly the built-in solve method in FEniCS. NOTE: As an alternative one can also assemble the finite element matrix A and the right hand side b that stems from the discretization of a and L , and then solve the linear system A {\\rm u} = {\\rm b}, where {\\rm u} is the vector collecting the coefficient of the finite element expasion of u_h , the entries of the matrix A are such that A_{ij} = a(\\phi_j, \\phi_i) , the entries of the right hand side b are such that b_i = L(\\phi_i) . u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) #solve(a == L, uh, bcs=bcs) A, b = assemble_system(a,L, bcs=bcs) solve(A, uh.vector(), b, \"cg\") nb.plot(uh) 6. Compute the discretization error For this problem, the exact solution is known. We can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution u_h and the exact solution u_{\\rm ex} ) \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, and \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5) grad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) print(\"|| u_h - u_e ||_L2 = \", err_L2) print(\"|| u_h - u_e ||_H1 = \", err_H1) || u_h - u_e ||_L2 = 0.00880525372208 || u_h - u_e ||_H1 = 0.396718952514 7. Convergence of the finite element method We now verify numerically a well-known convergence result for the finite element method. Let denote with s the polynomial degree of the finite element space, and assume that the solution u_{\\rm ex} is at least in H^{s+1}(\\Omega) . Then we have \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. In the code below, the function compute(n, degree) solves the PDE using a mesh with n elements in each direction and finite element spaces of polinomial order degree . The figure below shows the discretization errors in the H^1 and L^2 as a function of the mesh size h ( h = \\frac{1}{n} ) for piecewise linear (P1, s=1 ) and piecewise quadratic (P2, s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular: for piecewise linear finite element P1 we observe first order convergence in the H^1 -norm and second order convergence in the L^2 -norm; for piecewise quadratic finite element P2 we observe second order convergence in the H^1 -norm and third order convergence in the L^2 -norm. def compute(n, degree): mesh = UnitSquareMesh(n, n) Vh = FunctionSpace(mesh, 'Lagrange', degree) boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts) u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) solve(a == L, uh, bcs=bcs) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) return err_L2, err_H1 nref = 5 n = 8*np.power(2,np.arange(0,nref)) h = 1./n err_L2_P1 = np.zeros(nref) err_H1_P1 = np.zeros(nref) err_L2_P2 = np.zeros(nref) err_H1_P2 = np.zeros(nref) for i in range(nref): err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1) err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2) plt.figure(figsize=(15,5)) plt.subplot(121) plt.loglog(h, err_H1_P1, '-or', label=\"H1 error\") plt.loglog(h, err_L2_P1, '-*b', label=\"L2 error\") plt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label=\"First Order\") plt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label=\"Second Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P1 Finite Element\") plt.legend(loc = 'lower right') plt.subplot(122) plt.loglog(h, err_H1_P2, '-or', label=\"H1 error\") plt.loglog(h, err_L2_P2, '-*b', label=\"L2 error\") plt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label=\"Second Order\") plt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label=\"Third Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P2 Finite Element\") plt.legend(loc='lower right') plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"FEniCS101 Tutorial"},{"location":"tutorials_v1.6.0/1_FEniCS101/#fenics101-tutorial","text":"In this tutorial we consider the boundary value problem (BVP) \\begin{eqnarray*} - \\nabla \\cdot (k \\nabla u) = f & \\text{ in } \\Omega,\\\\ u = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\ k \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom}, \\end{eqnarray*} where \\Omega = (0,1) \\times (0,1) , \\Gamma_D and and \\Gamma_N are the union of the left and right, and top and bottom boundaries of \\Omega , respectively. Here \\begin{eqnarray*} k(x,y) = 1 & \\text{ on } \\Omega\\\\ f(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\ u_0(x,y) = 0 & \\text{ on } \\Gamma_D, \\\\ \\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right. & \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array} \\end{eqnarray*} The exact solution is u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).","title":"FEniCS101 Tutorial"},{"location":"tutorials_v1.6.0/1_FEniCS101/#weak-formulation","text":"Let us define the Hilbert spaces V_{u_0}, V_0 \\in \\Omega as V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\}, V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}. To obtain the weak formulation, we multiply the PDE by an arbitrary function v \\in V_0 and integrate over the domain \\Omega leading to -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. Then, integration by parts the non-conforming term gives \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. Finally by recalling that v = 0 on \\Gamma_D and that k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma on \\Gamma_N , we find the weak formulation: Find * u \\in V_{u_0} such that* \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.","title":"Weak formulation"},{"location":"tutorials_v1.6.0/1_FEniCS101/#1-load-modules","text":"To start we load the following modules: dolfin: the python/C++ interface to FEniCS math : the python module for mathematical functions numpy : a python package for linear algebra matplotlib : a python package used for plotting the results from __future__ import absolute_import, division, print_function from dolfin import * import math import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v1.6.0/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space","text":"We construct a triangulation (mesh) \\mathcal{T}_h of the computational domain \\Omega := [0, 1]^2 with n elements in each direction. On the mesh \\mathcal{T}_h , we then define the finite element space V_h \\subset H^1(\\Omega) consisting of globally continuous piecewise polinomials functions. The degree variable defines the polinomial degree. n = 16 degree = 1 mesh = UnitSquareMesh(n, n) nb.plot(mesh) Vh = FunctionSpace(mesh, 'Lagrange', degree) print(\"dim(Vh) = \", Vh.dim()) dim(Vh) = 289","title":"2. Define the mesh and the finite element space"},{"location":"tutorials_v1.6.0/1_FEniCS101/#3-define-boundary-labels","text":"To partition the boundary of \\Omega in the subdomains \\Gamma_{\\rm top} , \\Gamma_{\\rm bottom} , \\Gamma_{\\rm left} , \\Gamma_{\\rm right} we assign a unique label boundary_parts to each of part of \\partial \\Omega . class TopBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1] - 1) < DOLFIN_EPS class BottomBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1]) < DOLFIN_EPS class LeftBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0]) < DOLFIN_EPS class RightBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0] - 1) < DOLFIN_EPS boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4)","title":"3. Define boundary labels"},{"location":"tutorials_v1.6.0/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions","text":"We first define the coefficients of the PDE using the Constant and Expression classes. Constant is used to define coefficients that do not depend on the space coordinates, Expression is used to define coefficients that are a known function of the space coordinates x[0] (x-axis direction) and x[1] (y-axis direction). In the finite element method community, Dirichlet boundary conditions are also known as essential boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class DirichletBC to indicate this type of condition. On the other hand, Newman boundary conditions are also known as natural boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure ds[i] to integrate over the portion of the boundary marked with label i . u_L = Constant(0.) u_R = Constant(0.) sigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5) sigma_top = Constant(0.) f = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts)","title":"4. Define the coefficients of the PDE and the boundary conditions"},{"location":"tutorials_v1.6.0/1_FEniCS101/#5-define-and-solve-the-variational-problem","text":"We also define two special types of functions: the TrialFunction u and the TestFunction v . These special types of function are used by FEniCS to generate the finite element vectors and matrices which stem from the weak formulation of the PDE. More specifically, by denoting by \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)} the finite element basis for the space V_h , a function u_h \\in V_h can be written as u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), where {\\rm u}_i represents the coefficients in the finite element expansion of u_h . We then define the bilinear form a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx ; the linear form L(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds . We can then solve the variational problem Find u_h \\in V_h such that a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h using directly the built-in solve method in FEniCS. NOTE: As an alternative one can also assemble the finite element matrix A and the right hand side b that stems from the discretization of a and L , and then solve the linear system A {\\rm u} = {\\rm b}, where {\\rm u} is the vector collecting the coefficient of the finite element expasion of u_h , the entries of the matrix A are such that A_{ij} = a(\\phi_j, \\phi_i) , the entries of the right hand side b are such that b_i = L(\\phi_i) . u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) #solve(a == L, uh, bcs=bcs) A, b = assemble_system(a,L, bcs=bcs) solve(A, uh.vector(), b, \"cg\") nb.plot(uh)","title":"5. Define and solve the variational problem"},{"location":"tutorials_v1.6.0/1_FEniCS101/#6-compute-the-discretization-error","text":"For this problem, the exact solution is known. We can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution u_h and the exact solution u_{\\rm ex} ) \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, and \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5) grad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) print(\"|| u_h - u_e ||_L2 = \", err_L2) print(\"|| u_h - u_e ||_H1 = \", err_H1) || u_h - u_e ||_L2 = 0.00880525372208 || u_h - u_e ||_H1 = 0.396718952514","title":"6. Compute the discretization error"},{"location":"tutorials_v1.6.0/1_FEniCS101/#7-convergence-of-the-finite-element-method","text":"We now verify numerically a well-known convergence result for the finite element method. Let denote with s the polynomial degree of the finite element space, and assume that the solution u_{\\rm ex} is at least in H^{s+1}(\\Omega) . Then we have \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. In the code below, the function compute(n, degree) solves the PDE using a mesh with n elements in each direction and finite element spaces of polinomial order degree . The figure below shows the discretization errors in the H^1 and L^2 as a function of the mesh size h ( h = \\frac{1}{n} ) for piecewise linear (P1, s=1 ) and piecewise quadratic (P2, s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular: for piecewise linear finite element P1 we observe first order convergence in the H^1 -norm and second order convergence in the L^2 -norm; for piecewise quadratic finite element P2 we observe second order convergence in the H^1 -norm and third order convergence in the L^2 -norm. def compute(n, degree): mesh = UnitSquareMesh(n, n) Vh = FunctionSpace(mesh, 'Lagrange', degree) boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts) u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) solve(a == L, uh, bcs=bcs) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) return err_L2, err_H1 nref = 5 n = 8*np.power(2,np.arange(0,nref)) h = 1./n err_L2_P1 = np.zeros(nref) err_H1_P1 = np.zeros(nref) err_L2_P2 = np.zeros(nref) err_H1_P2 = np.zeros(nref) for i in range(nref): err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1) err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2) plt.figure(figsize=(15,5)) plt.subplot(121) plt.loglog(h, err_H1_P1, '-or', label=\"H1 error\") plt.loglog(h, err_L2_P1, '-*b', label=\"L2 error\") plt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label=\"First Order\") plt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label=\"Second Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P1 Finite Element\") plt.legend(loc = 'lower right') plt.subplot(122) plt.loglog(h, err_H1_P2, '-or', label=\"H1 error\") plt.loglog(h, err_L2_P2, '-*b', label=\"L2 error\") plt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label=\"Second Order\") plt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label=\"Third Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P2 Finite Element\") plt.legend(loc='lower right') plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"7. Convergence of the finite element method"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/","text":"Coefficient field inversion in an elliptic partial differential equation We consider the estimation of a coefficient in an elliptic partial differential equation as a model problem. Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity. It can also be interpreted as finding a membrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n , n\\in\\{1,2,3\\} be an open, bounded domain and consider the following problem: \\min_{a} J(a):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx, where u is the solution of \\begin{split} \\quad -\\nabla\\cdot(\\exp(a)\\nabla u) &= f \\text{ in }\\Omega,\\\\ u &= 0 \\text{ on }\\partial\\Omega. \\end{split} Here a\\in U_{ad}:=\\{a\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\} the unknown coefficient field, u_d denotes (possibly noisy) data, f\\in H^{-1}(\\Omega) a given force, and \\gamma\\ge 0 the regularization parameter. The variational (or weak) form of the state equation: Find u\\in H_0^1(\\Omega) such that (\\exp(a)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), where H_0^1(\\Omega) is the space of functions vanishing on \\partial\\Omega with square integrable derivatives. Here, (\\cdot\\,,\\cdot) denotes the L^2 -inner product, i.e, for scalar functions u,v defined on \\Omega we denote (u,v) := \\int_\\Omega u(x) v(x) \\,dx . Optimality System: The Lagrangian functional \\mathscr{L}:H^1(\\Omega)\\times H_0^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} , which we use as a tool to derive the optimality system, is given by \\mathscr{L}(a,u,p):= \\frac{1}{2}(u-u_d,u-u_d) + \\frac{\\gamma}{2}(\\nabla a, \\nabla a) + (\\exp(a)\\nabla u,\\nabla p) - (f,p). The Lagrange multiplier theory shows that, at a solution all variations of the Lagrangian functional with respect to all variables must vanish. These variations of \\mathscr{L} with respect to (p,u,a) in directions (\\tilde{u}, \\tilde{p}, \\tilde{a}) are given by \\begin{alignat}{2} \\mathscr{L}_p(a,u,p)(\\tilde{p}) &= (\\exp(a)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) &&= 0,\\\\ \\mathscr{L}_u(a,u,p)(\\tilde{u}) &= (\\exp(a)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) && = 0,\\\\ \\mathscr{L}_a(a,u,p)(\\tilde{a}) &= \\gamma(\\nabla a, \\nabla \\tilde{a}) + (\\tilde{a}\\exp(a)\\nabla u, \\nabla p) &&= 0, \\end{alignat} where the variations (\\tilde{u}, \\tilde{p}, \\tilde{a}) are taken from the same spaces as (u,p,a) . The gradient of the cost functional \\mathcal{J}(a) therefore is \\mathcal{G}(a)(\\tilde a) = \\gamma(\\nabla a, \\nabla \\tilde{a}) + (\\tilde{a}\\exp(a)\\nabla u, \\nabla \\tilde{p}). Inexact Newton-CG: Newton's method requires second-order variational derivatives of the Lagrangian . Written in abstract form, it computes an update direction (\\hat a_k, \\hat u_k,\\hat p_k) from the following Newton step for the Lagrangian functional: \\mathscr{L}''(a_k, u_k, p_k)\\left[(\\tilde a, \\tilde u, \\tilde p),(\\hat a_k, \\hat u_k, \\hat p_k)\\right] = -\\mathscr{L}'(a_k,u_k,p_k)(\\tilde a, \\tilde u, \\tilde p), for all variations (\\tilde a, \\tilde u, \\tilde p) , where \\mathscr{L}' and \\mathscr{L}'' denote the first and second variations of the Lagrangian. For the elliptic parameter inversion problem, this Newton step (written in variatonal form) is as follows: Find (\\hat u_k, \\hat a_k,\\hat p_k) as the solution of the linear system \\begin{array}{llll} (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u, \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla p_k, \\nabla \\tilde u)\\\\ (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\ (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla \\tilde p) & &= - (\\exp(a_k) \\nabla u_k, \\nabla \\tilde p) + (f, \\tilde p), \\end{array} for all (\\tilde u, \\tilde a, \\tilde p) . Discrete Newton system: \\def\\tu{\\tilde u} \\def\\btu{\\bf \\tilde u} \\def\\ta{\\tilde a} \\def\\bta{\\bf \\tilde a} \\def\\tp{\\tilde p} \\def\\btp{\\bf \\tilde p} \\def\\hu{\\hat u} \\def\\bhu{\\bf \\hat u} \\def\\ha{\\hat a} \\def\\bha{\\bf \\hat a} \\def\\hp{\\hat p} \\def\\bhp{\\bf \\hat p} The discretized Newton step: denote the vectors corresponding to the discretization of the functions \\ha_k,\\hu_k, \\hp_k by \\bf \\bha_k, \\bhu_k and \\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system: \\begin{bmatrix} \\bf W_{\\scriptsize\\mbox{uu}} & \\bf W_{\\scriptsize\\mbox{ua}} & \\bf A^T \\\\ \\bf W_{\\scriptsize\\mbox{au}} & \\bf R + \\bf R_{\\scriptsize\\mbox{aa}}& \\bf C^T \\\\ \\bf A & \\bf C & 0 \\end{bmatrix} \\left[ \\begin{array}{c} \\bhu_k \\\\ \\bha_k \\\\ \\bhp_k \\end{array} \\right] = -\\left[ \\begin{array}{ccc} \\bf{g}_u\\\\ \\bf{g}_a\\\\ \\bf{g}_p \\end{array} \\right], where \\bf W_{\\scriptsize \\mbox{uu}} , \\bf W_{\\scriptsize\\mbox{ua}} , \\bf W_{\\scriptsize\\mbox{au}} , and \\bf R are the components of the Hessian matrix of the Lagrangian, \\bf A and \\bf C are the Jacobian of the state equation with respect to the state and the control variables, respectively and \\bf g_u , \\bf g_a , and \\bf g_p are the discrete gradients of the Lagrangian with respect to \\bf u , \\bf a and \\bf p , respectively. Reduced Hessian apply: To eliminate the incremental state and adjoint variables, \\bhu_k and \\bhp_k , from the first and last equations we use \\begin{align} \\bhu_k &= -\\bf A^{-1} \\bf C \\, \\bha_k,\\\\ \\bhp_k &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha_k). \\end{align} This results in the following reduced linear system for the Newton step \\bf H \\, \\bha_k = -\\bf{g}_a, with the reduced Hessian \\bf H applied to a vector \\bha given by \\bf H \\bha = \\underbrace{(\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})}_{\\text{Hessian of the regularization}} \\bha + \\underbrace{(\\bf C^{T}\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bf A^{-1} \\bf C - \\bf W_{\\scriptsize\\mbox{ua}}) - \\bf W_{\\scriptsize\\mbox{au}} \\bf A^{-1} \\bf C)}_{\\text{Hessian of the data misfit}}\\;\\bha. Goals: By the end of this notebook, you should be able to: solve the forward and adjoint Poisson equations understand the inverse method framework visualise and understand the results modify the problem and code Mathematical tools used: Finite element method Derivation of gradiant and Hessian via the adjoint method inexact Newton-CG Armijo line search List of software used: FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , a python package used for plotting the results Numpy , a python package for linear algebra Set up Import dependencies from __future__ import absolute_import, division, print_function from dolfin import * import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False) np.random.seed(seed=1) Model set up: As in the introduction, the first thing we need to do is set up the numerical model. In this cell, we set the mesh, the finite element functions u, p, g corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization. # create mesh and define function spaces nx = 64 ny = 64 mesh = UnitSquareMesh(nx, ny) Va = FunctionSpace(mesh, 'Lagrange', 1) Vu = FunctionSpace(mesh, 'Lagrange', 2) # The true and inverted parameter atrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Va) a = interpolate(Expression(\"log(2.0)\", degree=1),Va) # define function for state and adjoint u = Function(Vu) p = Function(Vu) # define Trial and Test Functions u_trial, p_trial, a_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Va) u_test, p_test, a_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Va) # initialize input functions f = Constant(\"1.0\") u0 = Constant(\"0.0\") # plot plt.figure(figsize=(15,5)) nb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on') nb.plot(atrue,subplot_loc=122, mytitle=\"True parameter field\") plt.show() # set up dirichlet boundary conditions def boundary(x,on_boundary): return on_boundary bc_state = DirichletBC(Vu, u0, boundary) bc_adj = DirichletBC(Vu, Constant(0.), boundary) Set up synthetic observations: Propose a coefficient field a_{\\text true} shown above The weak form of the pde: Find u\\in H_0^1(\\Omega) such that \\underbrace{(\\exp(a_{\\text true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) . Perturb the solution: u = u + \\eta , where \\eta \\sim \\mathcal{N}(0, \\sigma) # noise level noise_level = 0.05 # weak form for setting up the synthetic observations a_goal = inner(exp(atrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_goal = f * u_test * dx # solve the forward/state problem to generate synthetic observations goal_A, goal_b = assemble_system(a_goal, L_goal, bc_state) utrue = Function(Vu) solve(goal_A, utrue.vector(), goal_b) ud = Function(Vu) ud.assign(utrue) # perturb state solution and create synthetic measurements ud # ud = u + ||u||/SNR * random.normal MAX = ud.vector().norm(\"linf\") noise = Vector() goal_A.init_vector(noise,1) noise.set_local( noise_level * MAX * np.random.normal(0, 1, Vu.dim())) bc_adj.apply(noise) ud.vector().axpy(1., noise) # plot nb.multi1_plot([utrue, ud], [\"State solution with atrue\", \"Synthetic observations\"]) plt.show() The cost function evaluation: J(a):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx}_{\\text reg} In the code below, W and R are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively. # regularization parameter gamma = 1e-8 # weak for for setting up the misfit and regularization compoment of the cost W_equ = inner(u_trial, u_test) * dx R_equ = gamma * inner(nabla_grad(a_trial), nabla_grad(a_test)) * dx W = assemble(W_equ) R = assemble(R_equ) # refine cost function def cost(u, ud, a, W, R): diff = u.vector() - ud.vector() reg = 0.5 * a.vector().inner(R*a.vector() ) misfit = 0.5 * diff.inner(W * diff) return [reg + misfit, misfit, reg] Setting up the state equations, right hand side for the adjoint and the necessary matrices: \\begin{array}{llll} (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u, \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla p_k, \\nabla \\tilde u)\\\\ (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\ (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla \\tilde p) & &= - (\\exp(a_k) \\nabla u_k, \\nabla \\tilde p) + (f, \\tilde p), \\end{array} # weak form for setting up the state equation a_state = inner(exp(a) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_state = f * u_test * dx # weak form for setting up the adjoint equation a_adj = inner(exp(a) * nabla_grad(p_trial), nabla_grad(p_test)) * dx L_adj = -inner(u - ud, p_test) * dx # weak form for setting up matrices Wua_equ = inner(exp(a) * a_trial * nabla_grad(p_test), nabla_grad(p)) * dx C_equ = inner(exp(a) * a_trial * nabla_grad(u), nabla_grad(u_test)) * dx Raa_equ = inner(exp(a) * a_trial * a_test * nabla_grad(u), nabla_grad(p)) * dx M_equ = inner(a_trial, a_test) * dx # assemble matrix M M = assemble(M_equ) Initial guess We solve the state equation and compute the cost functional for the initial guess of the parameter a_ini # solve state equation state_A, state_b = assemble_system (a_state, L_state, bc_state) solve (state_A, u.vector(), state_b) # evaluate cost [cost_old, misfit_old, reg_old] = cost(u, ud, a, W, R) # plot plt.figure(figsize=(15,5)) nb.plot(a,subplot_loc=121, mytitle=\"a_ini\", vmin=atrue.vector().min(), vmax=atrue.vector().max()) nb.plot(u,subplot_loc=122, mytitle=\"u(a_ini)\") plt.show() The reduced Hessian apply to a vector v: Here we describe how to apply the reduced Hessian operator to a vector v. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined. For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm. The Hessian apply reads: \\begin{align} \\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu + \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha) & \\text{adjoint}\\\\ \\bf H \\bf v &= (\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})\\bf v + \\bf C^T \\bhp + \\bf W_{\\scriptsize\\mbox{au}} \\bhu. \\end{align} The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha , \\bf R_{\\scriptsize\\mbox{aa}}\\bf v , and \\bf W_{\\scriptsize\\mbox{au}} \\bhu : \\begin{align} \\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bf A^{-T} \\bf W_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\ \\bf H_{\\rm GN} \\bf v &= \\bf R \\bf v + \\bf C^T \\bhp. \\end{align} # Class HessianOperator to perform Hessian apply to a vector class HessianOperator(): cgiter = 0 def __init__(self, R, Raa, C, A, adj_A, W, Wua, use_gaussnewton=False): self.R = R self.Raa = Raa self.C = C self.A = A self.adj_A = adj_A self.W = W self.Wua = Wua self.use_gaussnewton = use_gaussnewton # incremental state self.du = Vector() self.A.init_vector(self.du,0) #incremental adjoint self.dp = Vector() self.adj_A.init_vector(self.dp,0) # auxiliary vectors self.CT_dp = Vector() self.C.init_vector(self.CT_dp, 1) self.Wua_du = Vector() self.Wua.init_vector(self.Wua_du, 1) def init_vector(self, v, dim): self.R.init_vector(v,dim) # Hessian performed on x, output as generic vector y def mult(self, v, y): self.cgiter += 1 y.zero() if self.use_gaussnewton: self.mult_GaussNewton(v,y) else: self.mult_Newton(v,y) # define (Gauss-Newton) Hessian apply H * v def mult_GaussNewton(self, v, y): #incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) #incremental adjoint rhs = - (self.W * self.du) bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1, self.CT_dp) # define (Newton) Hessian apply H * v def mult_Newton(self, v, y): #incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) #incremental adjoint rhs = -(self.W * self.du) - self.Wua * v bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) #Reg/Prior term self.R.mult(v,y) y.axpy(1., Raa*v) #Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1., self.CT_dp) self.Wua.transpmult(self.du, self.Wua_du) y.axpy(1., self.Wua_du) The inexact Newton-CG optimization with Armijo line search: We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search. The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ). First, we compute the gradient by solving the state and adjoint equation for the current parameter a , and then substituing the current state u , parameter a and adjoint p variables in the weak form expression of the gradient: (g, \\tilde{a}) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +(\\tilde{a}\\nabla u, \\nabla p). Then, we compute the Newton direction \\delta a by iteratively solving {\\bf H} {\\delta a} = - {\\bf g} . The Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug (to avoid negative curvature) criteria. Finally, the Armijo line search uses backtracking to find \\alpha such that a sufficient reduction in the cost functional is achieved. More specifically, we use backtracking to find \\alpha such that: J( a + \\alpha \\delta a ) \\leq J(a) + \\alpha c_{\\rm armijo} (\\delta a,g). # define parameters for the optimization tol = 1e-8 c = 1e-4 maxiter = 12 plot_on = False # initialize iter counters iter = 1 total_cg_iter = 0 converged = False # initializations g, a_delta = Vector(), Vector() R.init_vector(a_delta,0) R.init_vector(g,0) a_prev = Function(Va) print(\"Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg\") while iter < maxiter and not converged: # assemble matrix C C = assemble(C_equ) # solve the adoint problem adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj) solve(adjoint_A, p.vector(), adjoint_RHS) # assemble W_ua and R Wua = assemble (Wua_equ) Raa = assemble (Raa_equ) # evaluate the gradient CT_p = Vector() C.init_vector(CT_p,1) C.transpmult(p.vector(), CT_p) MG = CT_p + R * a.vector() solve(M, g, MG) # calculate the norm of the gradient grad2 = g.inner(MG) gradnorm = sqrt(grad2) # set the CG tolerance (use Eisenstat\u2013Walker termination criterion) if iter == 1: gradnorm_ini = gradnorm tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini)) # define the Hessian apply operator (with preconditioner) Hess_Apply = HessianOperator(R, Raa, C, state_A, adjoint_A, W, Wua, use_gaussnewton=(iter<6) ) P = R + gamma * M Psolver = PETScKrylovSolver(\"cg\", amg_method()) Psolver.set_operator(P) solver = CGSolverSteihaug() solver.set_operator(Hess_Apply) solver.set_preconditioner(Psolver) solver.parameters[\"rel_tolerance\"] = tolcg solver.parameters[\"zero_initial_guess\"] = True solver.parameters[\"print_level\"] = -1 # solve the Newton system H a_delta = - MG solver.solve(a_delta, -MG) total_cg_iter += Hess_Apply.cgiter # linesearch alpha = 1 descent = 0 no_backtrack = 0 a_prev.assign(a) while descent == 0 and no_backtrack < 10: a.vector().axpy(alpha, a_delta ) # solve the state/forward problem state_A, state_b = assemble_system(a_state, L_state, bc_state) solve(state_A, u.vector(), state_b) # evaluate cost [cost_new, misfit_new, reg_new] = cost(u, ud, a, W, R) # check if Armijo conditions are satisfied if cost_new < cost_old + alpha * c * MG.inner(a_delta): cost_old = cost_new descent = 1 else: no_backtrack += 1 alpha *= 0.5 a.assign(a_prev) # reset a # calculate sqrt(-G * D) graddir = sqrt(- MG.inner(a_delta) ) sp = \"\" print(\"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\ (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\ graddir, sp, gradnorm, sp, alpha, sp, tolcg) ) if plot_on: nb.multi1_plot([a,u,p], [\"a\",\"u\",\"p\"], same_colorbar=False) plt.show() # check for convergence if gradnorm < tol and iter > 1: converged = True print(\"Newton's method converged in \",iter,\" iterations\") print(\"Total number of CG iterations: \", total_cg_iter) iter += 1 if not converged: print(\"Newton's method did not converge in \", maxiter, \" iterations\") Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg 1 1 1.12708e-05 1.12708e-05 1.33979e-11 1.56540e-02 3.79427e-04 1.00 5.000e-01 2 1 7.79732e-07 7.79695e-07 3.67737e-11 4.68278e-03 5.35002e-05 1.00 3.755e-01 3 1 3.10620e-07 3.10571e-07 4.91259e-11 9.71633e-04 7.13895e-06 1.00 1.372e-01 4 5 1.92183e-07 1.62405e-07 2.97780e-08 4.51694e-04 1.00276e-06 1.00 5.141e-02 5 1 1.86913e-07 1.57119e-07 2.97941e-08 1.02668e-04 6.12750e-07 1.00 4.019e-02 6 12 1.80408e-07 1.37719e-07 4.26890e-08 1.15975e-04 2.24111e-07 1.00 2.430e-02 7 5 1.80331e-07 1.38935e-07 4.13963e-08 1.23223e-05 4.17399e-08 1.00 1.049e-02 8 15 1.80330e-07 1.39056e-07 4.12734e-08 1.74451e-06 3.43216e-09 1.00 3.008e-03 Newton's method converged in 8 iterations Total number of CG iterations: 41 nb.multi1_plot([atrue, a], [\"atrue\", \"a\"]) nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Coefficient field inversion in an elliptic partial differential equation"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#coefficient-field-inversion-in-an-elliptic-partial-differential-equation","text":"We consider the estimation of a coefficient in an elliptic partial differential equation as a model problem. Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity. It can also be interpreted as finding a membrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n , n\\in\\{1,2,3\\} be an open, bounded domain and consider the following problem: \\min_{a} J(a):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx, where u is the solution of \\begin{split} \\quad -\\nabla\\cdot(\\exp(a)\\nabla u) &= f \\text{ in }\\Omega,\\\\ u &= 0 \\text{ on }\\partial\\Omega. \\end{split} Here a\\in U_{ad}:=\\{a\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\} the unknown coefficient field, u_d denotes (possibly noisy) data, f\\in H^{-1}(\\Omega) a given force, and \\gamma\\ge 0 the regularization parameter.","title":"Coefficient field inversion in an elliptic partial differential equation"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#the-variational-or-weak-form-of-the-state-equation","text":"Find u\\in H_0^1(\\Omega) such that (\\exp(a)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), where H_0^1(\\Omega) is the space of functions vanishing on \\partial\\Omega with square integrable derivatives. Here, (\\cdot\\,,\\cdot) denotes the L^2 -inner product, i.e, for scalar functions u,v defined on \\Omega we denote (u,v) := \\int_\\Omega u(x) v(x) \\,dx .","title":"The variational (or weak) form of the state equation:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#optimality-system","text":"The Lagrangian functional \\mathscr{L}:H^1(\\Omega)\\times H_0^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} , which we use as a tool to derive the optimality system, is given by \\mathscr{L}(a,u,p):= \\frac{1}{2}(u-u_d,u-u_d) + \\frac{\\gamma}{2}(\\nabla a, \\nabla a) + (\\exp(a)\\nabla u,\\nabla p) - (f,p). The Lagrange multiplier theory shows that, at a solution all variations of the Lagrangian functional with respect to all variables must vanish. These variations of \\mathscr{L} with respect to (p,u,a) in directions (\\tilde{u}, \\tilde{p}, \\tilde{a}) are given by \\begin{alignat}{2} \\mathscr{L}_p(a,u,p)(\\tilde{p}) &= (\\exp(a)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) &&= 0,\\\\ \\mathscr{L}_u(a,u,p)(\\tilde{u}) &= (\\exp(a)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) && = 0,\\\\ \\mathscr{L}_a(a,u,p)(\\tilde{a}) &= \\gamma(\\nabla a, \\nabla \\tilde{a}) + (\\tilde{a}\\exp(a)\\nabla u, \\nabla p) &&= 0, \\end{alignat} where the variations (\\tilde{u}, \\tilde{p}, \\tilde{a}) are taken from the same spaces as (u,p,a) . The gradient of the cost functional \\mathcal{J}(a) therefore is \\mathcal{G}(a)(\\tilde a) = \\gamma(\\nabla a, \\nabla \\tilde{a}) + (\\tilde{a}\\exp(a)\\nabla u, \\nabla \\tilde{p}).","title":"Optimality System:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#inexact-newton-cg","text":"Newton's method requires second-order variational derivatives of the Lagrangian . Written in abstract form, it computes an update direction (\\hat a_k, \\hat u_k,\\hat p_k) from the following Newton step for the Lagrangian functional: \\mathscr{L}''(a_k, u_k, p_k)\\left[(\\tilde a, \\tilde u, \\tilde p),(\\hat a_k, \\hat u_k, \\hat p_k)\\right] = -\\mathscr{L}'(a_k,u_k,p_k)(\\tilde a, \\tilde u, \\tilde p), for all variations (\\tilde a, \\tilde u, \\tilde p) , where \\mathscr{L}' and \\mathscr{L}'' denote the first and second variations of the Lagrangian. For the elliptic parameter inversion problem, this Newton step (written in variatonal form) is as follows: Find (\\hat u_k, \\hat a_k,\\hat p_k) as the solution of the linear system \\begin{array}{llll} (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u, \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla p_k, \\nabla \\tilde u)\\\\ (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\ (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla \\tilde p) & &= - (\\exp(a_k) \\nabla u_k, \\nabla \\tilde p) + (f, \\tilde p), \\end{array} for all (\\tilde u, \\tilde a, \\tilde p) .","title":"Inexact Newton-CG:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#discrete-newton-system","text":"\\def\\tu{\\tilde u} \\def\\btu{\\bf \\tilde u} \\def\\ta{\\tilde a} \\def\\bta{\\bf \\tilde a} \\def\\tp{\\tilde p} \\def\\btp{\\bf \\tilde p} \\def\\hu{\\hat u} \\def\\bhu{\\bf \\hat u} \\def\\ha{\\hat a} \\def\\bha{\\bf \\hat a} \\def\\hp{\\hat p} \\def\\bhp{\\bf \\hat p} The discretized Newton step: denote the vectors corresponding to the discretization of the functions \\ha_k,\\hu_k, \\hp_k by \\bf \\bha_k, \\bhu_k and \\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system: \\begin{bmatrix} \\bf W_{\\scriptsize\\mbox{uu}} & \\bf W_{\\scriptsize\\mbox{ua}} & \\bf A^T \\\\ \\bf W_{\\scriptsize\\mbox{au}} & \\bf R + \\bf R_{\\scriptsize\\mbox{aa}}& \\bf C^T \\\\ \\bf A & \\bf C & 0 \\end{bmatrix} \\left[ \\begin{array}{c} \\bhu_k \\\\ \\bha_k \\\\ \\bhp_k \\end{array} \\right] = -\\left[ \\begin{array}{ccc} \\bf{g}_u\\\\ \\bf{g}_a\\\\ \\bf{g}_p \\end{array} \\right], where \\bf W_{\\scriptsize \\mbox{uu}} , \\bf W_{\\scriptsize\\mbox{ua}} , \\bf W_{\\scriptsize\\mbox{au}} , and \\bf R are the components of the Hessian matrix of the Lagrangian, \\bf A and \\bf C are the Jacobian of the state equation with respect to the state and the control variables, respectively and \\bf g_u , \\bf g_a , and \\bf g_p are the discrete gradients of the Lagrangian with respect to \\bf u , \\bf a and \\bf p , respectively.","title":"Discrete Newton system:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#reduced-hessian-apply","text":"To eliminate the incremental state and adjoint variables, \\bhu_k and \\bhp_k , from the first and last equations we use \\begin{align} \\bhu_k &= -\\bf A^{-1} \\bf C \\, \\bha_k,\\\\ \\bhp_k &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha_k). \\end{align} This results in the following reduced linear system for the Newton step \\bf H \\, \\bha_k = -\\bf{g}_a, with the reduced Hessian \\bf H applied to a vector \\bha given by \\bf H \\bha = \\underbrace{(\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})}_{\\text{Hessian of the regularization}} \\bha + \\underbrace{(\\bf C^{T}\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bf A^{-1} \\bf C - \\bf W_{\\scriptsize\\mbox{ua}}) - \\bf W_{\\scriptsize\\mbox{au}} \\bf A^{-1} \\bf C)}_{\\text{Hessian of the data misfit}}\\;\\bha.","title":"Reduced Hessian apply:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#goals","text":"By the end of this notebook, you should be able to: solve the forward and adjoint Poisson equations understand the inverse method framework visualise and understand the results modify the problem and code","title":"Goals:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#mathematical-tools-used","text":"Finite element method Derivation of gradiant and Hessian via the adjoint method inexact Newton-CG Armijo line search","title":"Mathematical tools used:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#list-of-software-used","text":"FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , a python package used for plotting the results Numpy , a python package for linear algebra","title":"List of software used:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#set-up","text":"","title":"Set up"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#import-dependencies","text":"from __future__ import absolute_import, division, print_function from dolfin import * import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False) np.random.seed(seed=1)","title":"Import dependencies"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#model-set-up","text":"As in the introduction, the first thing we need to do is set up the numerical model. In this cell, we set the mesh, the finite element functions u, p, g corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization. # create mesh and define function spaces nx = 64 ny = 64 mesh = UnitSquareMesh(nx, ny) Va = FunctionSpace(mesh, 'Lagrange', 1) Vu = FunctionSpace(mesh, 'Lagrange', 2) # The true and inverted parameter atrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Va) a = interpolate(Expression(\"log(2.0)\", degree=1),Va) # define function for state and adjoint u = Function(Vu) p = Function(Vu) # define Trial and Test Functions u_trial, p_trial, a_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Va) u_test, p_test, a_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Va) # initialize input functions f = Constant(\"1.0\") u0 = Constant(\"0.0\") # plot plt.figure(figsize=(15,5)) nb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on') nb.plot(atrue,subplot_loc=122, mytitle=\"True parameter field\") plt.show() # set up dirichlet boundary conditions def boundary(x,on_boundary): return on_boundary bc_state = DirichletBC(Vu, u0, boundary) bc_adj = DirichletBC(Vu, Constant(0.), boundary)","title":"Model set up:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#set-up-synthetic-observations","text":"Propose a coefficient field a_{\\text true} shown above The weak form of the pde: Find u\\in H_0^1(\\Omega) such that \\underbrace{(\\exp(a_{\\text true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) . Perturb the solution: u = u + \\eta , where \\eta \\sim \\mathcal{N}(0, \\sigma) # noise level noise_level = 0.05 # weak form for setting up the synthetic observations a_goal = inner(exp(atrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_goal = f * u_test * dx # solve the forward/state problem to generate synthetic observations goal_A, goal_b = assemble_system(a_goal, L_goal, bc_state) utrue = Function(Vu) solve(goal_A, utrue.vector(), goal_b) ud = Function(Vu) ud.assign(utrue) # perturb state solution and create synthetic measurements ud # ud = u + ||u||/SNR * random.normal MAX = ud.vector().norm(\"linf\") noise = Vector() goal_A.init_vector(noise,1) noise.set_local( noise_level * MAX * np.random.normal(0, 1, Vu.dim())) bc_adj.apply(noise) ud.vector().axpy(1., noise) # plot nb.multi1_plot([utrue, ud], [\"State solution with atrue\", \"Synthetic observations\"]) plt.show()","title":"Set up synthetic observations:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#the-cost-function-evaluation","text":"J(a):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx}_{\\text reg} In the code below, W and R are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively. # regularization parameter gamma = 1e-8 # weak for for setting up the misfit and regularization compoment of the cost W_equ = inner(u_trial, u_test) * dx R_equ = gamma * inner(nabla_grad(a_trial), nabla_grad(a_test)) * dx W = assemble(W_equ) R = assemble(R_equ) # refine cost function def cost(u, ud, a, W, R): diff = u.vector() - ud.vector() reg = 0.5 * a.vector().inner(R*a.vector() ) misfit = 0.5 * diff.inner(W * diff) return [reg + misfit, misfit, reg]","title":"The cost function evaluation:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#setting-up-the-state-equations-right-hand-side-for-the-adjoint-and-the-necessary-matrices","text":"\\begin{array}{llll} (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u, \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla p_k, \\nabla \\tilde u)\\\\ (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde a \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\ (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla \\tilde p) & &= - (\\exp(a_k) \\nabla u_k, \\nabla \\tilde p) + (f, \\tilde p), \\end{array} # weak form for setting up the state equation a_state = inner(exp(a) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_state = f * u_test * dx # weak form for setting up the adjoint equation a_adj = inner(exp(a) * nabla_grad(p_trial), nabla_grad(p_test)) * dx L_adj = -inner(u - ud, p_test) * dx # weak form for setting up matrices Wua_equ = inner(exp(a) * a_trial * nabla_grad(p_test), nabla_grad(p)) * dx C_equ = inner(exp(a) * a_trial * nabla_grad(u), nabla_grad(u_test)) * dx Raa_equ = inner(exp(a) * a_trial * a_test * nabla_grad(u), nabla_grad(p)) * dx M_equ = inner(a_trial, a_test) * dx # assemble matrix M M = assemble(M_equ)","title":"Setting up the state equations, right hand side for the adjoint and the necessary matrices:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#initial-guess","text":"We solve the state equation and compute the cost functional for the initial guess of the parameter a_ini # solve state equation state_A, state_b = assemble_system (a_state, L_state, bc_state) solve (state_A, u.vector(), state_b) # evaluate cost [cost_old, misfit_old, reg_old] = cost(u, ud, a, W, R) # plot plt.figure(figsize=(15,5)) nb.plot(a,subplot_loc=121, mytitle=\"a_ini\", vmin=atrue.vector().min(), vmax=atrue.vector().max()) nb.plot(u,subplot_loc=122, mytitle=\"u(a_ini)\") plt.show()","title":"Initial guess"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#the-reduced-hessian-apply-to-a-vector-v","text":"Here we describe how to apply the reduced Hessian operator to a vector v. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined. For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm. The Hessian apply reads: \\begin{align} \\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu + \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha) & \\text{adjoint}\\\\ \\bf H \\bf v &= (\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})\\bf v + \\bf C^T \\bhp + \\bf W_{\\scriptsize\\mbox{au}} \\bhu. \\end{align} The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha , \\bf R_{\\scriptsize\\mbox{aa}}\\bf v , and \\bf W_{\\scriptsize\\mbox{au}} \\bhu : \\begin{align} \\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bf A^{-T} \\bf W_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\ \\bf H_{\\rm GN} \\bf v &= \\bf R \\bf v + \\bf C^T \\bhp. \\end{align} # Class HessianOperator to perform Hessian apply to a vector class HessianOperator(): cgiter = 0 def __init__(self, R, Raa, C, A, adj_A, W, Wua, use_gaussnewton=False): self.R = R self.Raa = Raa self.C = C self.A = A self.adj_A = adj_A self.W = W self.Wua = Wua self.use_gaussnewton = use_gaussnewton # incremental state self.du = Vector() self.A.init_vector(self.du,0) #incremental adjoint self.dp = Vector() self.adj_A.init_vector(self.dp,0) # auxiliary vectors self.CT_dp = Vector() self.C.init_vector(self.CT_dp, 1) self.Wua_du = Vector() self.Wua.init_vector(self.Wua_du, 1) def init_vector(self, v, dim): self.R.init_vector(v,dim) # Hessian performed on x, output as generic vector y def mult(self, v, y): self.cgiter += 1 y.zero() if self.use_gaussnewton: self.mult_GaussNewton(v,y) else: self.mult_Newton(v,y) # define (Gauss-Newton) Hessian apply H * v def mult_GaussNewton(self, v, y): #incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) #incremental adjoint rhs = - (self.W * self.du) bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1, self.CT_dp) # define (Newton) Hessian apply H * v def mult_Newton(self, v, y): #incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) #incremental adjoint rhs = -(self.W * self.du) - self.Wua * v bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) #Reg/Prior term self.R.mult(v,y) y.axpy(1., Raa*v) #Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1., self.CT_dp) self.Wua.transpmult(self.du, self.Wua_du) y.axpy(1., self.Wua_du)","title":"The reduced Hessian apply to a vector v:"},{"location":"tutorials_v1.6.0/2_PoissonDeterministic/#the-inexact-newton-cg-optimization-with-armijo-line-search","text":"We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search. The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ). First, we compute the gradient by solving the state and adjoint equation for the current parameter a , and then substituing the current state u , parameter a and adjoint p variables in the weak form expression of the gradient: (g, \\tilde{a}) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +(\\tilde{a}\\nabla u, \\nabla p). Then, we compute the Newton direction \\delta a by iteratively solving {\\bf H} {\\delta a} = - {\\bf g} . The Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug (to avoid negative curvature) criteria. Finally, the Armijo line search uses backtracking to find \\alpha such that a sufficient reduction in the cost functional is achieved. More specifically, we use backtracking to find \\alpha such that: J( a + \\alpha \\delta a ) \\leq J(a) + \\alpha c_{\\rm armijo} (\\delta a,g). # define parameters for the optimization tol = 1e-8 c = 1e-4 maxiter = 12 plot_on = False # initialize iter counters iter = 1 total_cg_iter = 0 converged = False # initializations g, a_delta = Vector(), Vector() R.init_vector(a_delta,0) R.init_vector(g,0) a_prev = Function(Va) print(\"Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg\") while iter < maxiter and not converged: # assemble matrix C C = assemble(C_equ) # solve the adoint problem adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj) solve(adjoint_A, p.vector(), adjoint_RHS) # assemble W_ua and R Wua = assemble (Wua_equ) Raa = assemble (Raa_equ) # evaluate the gradient CT_p = Vector() C.init_vector(CT_p,1) C.transpmult(p.vector(), CT_p) MG = CT_p + R * a.vector() solve(M, g, MG) # calculate the norm of the gradient grad2 = g.inner(MG) gradnorm = sqrt(grad2) # set the CG tolerance (use Eisenstat\u2013Walker termination criterion) if iter == 1: gradnorm_ini = gradnorm tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini)) # define the Hessian apply operator (with preconditioner) Hess_Apply = HessianOperator(R, Raa, C, state_A, adjoint_A, W, Wua, use_gaussnewton=(iter<6) ) P = R + gamma * M Psolver = PETScKrylovSolver(\"cg\", amg_method()) Psolver.set_operator(P) solver = CGSolverSteihaug() solver.set_operator(Hess_Apply) solver.set_preconditioner(Psolver) solver.parameters[\"rel_tolerance\"] = tolcg solver.parameters[\"zero_initial_guess\"] = True solver.parameters[\"print_level\"] = -1 # solve the Newton system H a_delta = - MG solver.solve(a_delta, -MG) total_cg_iter += Hess_Apply.cgiter # linesearch alpha = 1 descent = 0 no_backtrack = 0 a_prev.assign(a) while descent == 0 and no_backtrack < 10: a.vector().axpy(alpha, a_delta ) # solve the state/forward problem state_A, state_b = assemble_system(a_state, L_state, bc_state) solve(state_A, u.vector(), state_b) # evaluate cost [cost_new, misfit_new, reg_new] = cost(u, ud, a, W, R) # check if Armijo conditions are satisfied if cost_new < cost_old + alpha * c * MG.inner(a_delta): cost_old = cost_new descent = 1 else: no_backtrack += 1 alpha *= 0.5 a.assign(a_prev) # reset a # calculate sqrt(-G * D) graddir = sqrt(- MG.inner(a_delta) ) sp = \"\" print(\"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\ (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\ graddir, sp, gradnorm, sp, alpha, sp, tolcg) ) if plot_on: nb.multi1_plot([a,u,p], [\"a\",\"u\",\"p\"], same_colorbar=False) plt.show() # check for convergence if gradnorm < tol and iter > 1: converged = True print(\"Newton's method converged in \",iter,\" iterations\") print(\"Total number of CG iterations: \", total_cg_iter) iter += 1 if not converged: print(\"Newton's method did not converge in \", maxiter, \" iterations\") Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg 1 1 1.12708e-05 1.12708e-05 1.33979e-11 1.56540e-02 3.79427e-04 1.00 5.000e-01 2 1 7.79732e-07 7.79695e-07 3.67737e-11 4.68278e-03 5.35002e-05 1.00 3.755e-01 3 1 3.10620e-07 3.10571e-07 4.91259e-11 9.71633e-04 7.13895e-06 1.00 1.372e-01 4 5 1.92183e-07 1.62405e-07 2.97780e-08 4.51694e-04 1.00276e-06 1.00 5.141e-02 5 1 1.86913e-07 1.57119e-07 2.97941e-08 1.02668e-04 6.12750e-07 1.00 4.019e-02 6 12 1.80408e-07 1.37719e-07 4.26890e-08 1.15975e-04 2.24111e-07 1.00 2.430e-02 7 5 1.80331e-07 1.38935e-07 4.13963e-08 1.23223e-05 4.17399e-08 1.00 1.049e-02 8 15 1.80330e-07 1.39056e-07 4.12734e-08 1.74451e-06 3.43216e-09 1.00 3.008e-03 Newton's method converged in 8 iterations Total number of CG iterations: 41 nb.multi1_plot([atrue, a], [\"atrue\", \"a\"]) nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"The inexact Newton-CG optimization with Armijo line search:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/","text":"\\def\\data{\\bf d_\\rm{obs}} \\def\\vec{\\bf} \\def\\m{\\bf m} \\def\\map{\\bf m_{\\text{MAP}}} \\def\\postcov{\\bf \\Gamma_{\\text{post}}} \\def\\prcov{\\bf \\Gamma_{\\text{prior}}} \\def\\matrix{\\bf} \\def\\Hmisfit{\\bf H_{\\text{misfit}}} \\def\\HT{\\tilde{\\bf H}_{\\text{misfit}}} \\def\\diag{diag} \\def\\Vr{\\matrix V_r} \\def\\Wr{\\matrix W_r} \\def\\Ir{\\matrix I_r} \\def\\Dr{\\matrix D_r} \\def\\H{\\matrix H} Example: Bayesian quantification of parameter uncertainty: Estimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by an elliptic PDE via the Bayesian inference framework. Hence, we state the inverse problem as a problem of statistical inference over the space of uncertain parameters, which are to be inferred from data and a physical model. The resulting solution to the statistical inverse problem is a posterior distribution that assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true'' parameter field that gave rise to the observed data. For simplicity, in what follows we give finite-dimensional expressions (i.e., after discretization of the parameter space) for the Bayesian formulation of the inverse problem. Bayes' Theorem: The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m) over the parameter space, which encodes any knowledge or assumptions about the parameter space that we may wish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m) , which explicitly represents the probability that a given set of parameters \\m might give rise to the observed data \\vec{d}_{\\text{obs}} \\in \\mathbb{R}^m , namely: <script type=\"math/tex; mode=display\">\\begin{align} \\pi_{\\text{post}}(\\m | \\data) \\propto \\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m). \\end{align} Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions. Gaussian prior and noise: The prior: We consider a Gaussian prior with mean \\vec m_{\\text prior} and covariance \\bf \\Gamma_{\\text{prior}} . The covariance is given by the discretization of the inverse of differential operator \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where \\gamma , \\delta > 0 control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem The likelihood: \\data = \\bf{f}(\\m) + \\bf{e }, \\;\\;\\; \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} ) \\pi_{\\text like}(\\data \\; | \\; \\m) = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right) Here \\bf f is the parameter-to-observable map that takes a parameter vector \\m and maps it to the space observation vector \\data . The posterior: \\pi_{\\text{post}}(\\m \\; | \\; \\data) \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right) The Gaussian approximation of the posterior: \\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}}) The mean of this posterior distribution, \\vec{\\map} , is the parameter vector maximizing the posterior, and is known as the maximum a posteriori (MAP) point. It can be found by minimizing the negative log of the posterior, which amounts to solving a deterministic inverse problem) with appropriately weighted norms, \\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\; \\Big( \\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} +\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\Big). The posterior covariance matrix is then given by the inverse of the Hessian matrix of \\mathcal{J} at \\map , namely \\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1} The prior-preconditioned Hessian of the data misfit: \\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2} The generalized eigenvalue problem: \\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda}, where \\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} contains the generalized eigenvalues and the columns of \\matrix V\\in \\mathbb R^{n\\times n} the generalized eigenvectors such that \\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I} . Randomized eigensolvers to construct the approximate spectral decomposition: When the generalized eigenvalues \\{\\lambda_i\\} decay rapidly, we can extract a low-rank approximation of \\Hmisfit by retaining only the r largest eigenvalues and corresponding eigenvectors, \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2}, Here, \\matrix{V}_r \\in \\mathbb{R}^{n\\times r} contains only the r generalized eigenvectors of \\Hmisfit that correspond to the r largest eigenvalues, which are assembled into the diagonal matrix \\matrix{\\Lambda}_r = \\diag (\\lambda_i) \\in \\mathbb{R}^{r \\times r} . The approximate posterior covariance:: Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \\begin{align} \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1} = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T + \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i + 1}\\right), \\end{align} where \\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in \\mathbb{R}^{r\\times r} . The last term in this expression captures the error due to truncation in terms of the discarded eigenvalues; this provides a criterion for truncating the spectrum, namely that r is chosen such that \\lambda_r is small relative to 1. Therefore we can approximate the posterior covariance as \\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T Drawing samples from a Gaussian distribution with covariance \\H^{-1} Let \\bf x be a sample for the prior distribution, i.e. \\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1}) as {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r + \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1} + \\bf I \\big\\} {\\bf x} This tutorial shows: description of the inverse problem (the forward problem, the prior, and the misfit functional) convergence of the inexact Newton-CG algorithm low-rank-based approximation of the posterior covariance (built on a low-rank approximation of the Hessian of the data misfit) how to construct the low-rank approximation of the Hessian of the data misfit how to apply the inverse and square-root inverse Hessian to a vector efficiently samples from the Gaussian approximation of the posterior Goals: By the end of this notebook, you should be able to: understand the Bayesian inverse framework visualise and understand the results modify the problem and code Mathematical tools used: Finite element method Derivation of gradiant and Hessian via the adjoint method inexact Newton-CG Armijo line search Bayes' formula randomized eigensolvers List of software used: FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , A great python package that I used for plotting many of the results Numpy , A python package for linear algebra. While extensive, this is mostly used to compute means and sums in this notebook. 1. Load modules from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import nb import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(seed=1) 2. Generate the true parameter This function generates a random field with a prescribed anysotropic covariance function. def true_model(Vh, gamma, delta, anis_diff): prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff ) noise = dl.Vector() prior.init_vector(noise,\"noise\") noise_size = get_local_size(noise) noise.set_local( np.random.randn( noise_size ) ) atrue = dl.Vector() prior.init_vector(atrue, 0) prior.sample(noise,atrue) return atrue 3. Set up the mesh and finite element spaces We compute a two dimensional mesh of a unit square with nx by ny elements. We define a P2 finite element space for the state and adjoint variable and P1 for the parameter . ndim = 2 nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh2, Vh1, Vh2] print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641 4. Set up the forward problem To set up the forward problem we use the PDEVariationalProblem class, which requires the following inputs - the finite element spaces for the state, parameter, and adjoint variables Vh - the pde in weak form pde_varf - the boundary conditions bc for the forward problem and bc0 for the adjoint and incremental problems. The PDEVariationalProblem class offer the following functionality: - solving the forward/adjoint and incremental problems - evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables. def u_boundary(x, on_boundary): return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS) u_bdr = dl.Expression(\"x[1]\", degree=1) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) f = dl.Constant(0.0) def pde_varf(u,a,p): return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) 4. Set up the prior To obtain the synthetic true paramter a_{\\rm true} we generate a realization of a Gaussian random field with zero average and covariance matrix \\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2} , where \\widetilde{\\mathcal{A}} is a differential operator of the form \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. Here \\Theta is an s.p.d. anisotropic tensor of the form \\Theta = \\begin{bmatrix} \\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\ (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2. \\end{bmatrix} For the prior model, we assume that we can measure the log-permeability coefficient at N locations, and we denote with a^1_{\\rm true} , \\ldots , a^N_{\\rm true} such measures. We also introduce the mollifier functions \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N, and we let \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M}, where p is a penalization costant (10 for this problem) and \\mathcal{M} = \\sum_{i=1}^N \\delta_i I . We then compute a_{\\rm pr} , the mean of the prior measure, as a regularized least-squares fit of these point observations by solving a_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle. Finally the prior distribution is \\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior}) , with \\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2} . gamma = .1 delta = .5 anis_diff = dl.Expression(code_AnisTensor2D, degree=1) anis_diff.theta0 = 2. anis_diff.theta1 = .5 anis_diff.alpha = math.pi/4 atrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff) locations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]]) pen = 1e1 prior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen) print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format( delta, gamma,2) ) objs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)] mytitles = [\"True Parameter\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() model = Model(pde,prior, misfit) Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2 5. Set up the misfit functional and generate synthetic observations To setup the observation operator, we generate ntargets random locations where to evaluate the value of the state. To generate the synthetic observation, we first solve the forward problem using the true parameter a_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise. rel_noise is the signal to noise ratio. ntargets = 300 rel_noise = 0.01 targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) print(\"Number of observation points: {0}\".format(ntargets)) misfit = PointwiseStateObservation(Vh[STATE], targets) utrue = pde.generate_state() x = [utrue, atrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX randn_perturb(misfit.d, noise_std_dev) misfit.noise_variance = noise_std_dev*noise_std_dev vmax = max( utrue.max(), misfit.d.max() ) vmin = min( utrue.min(), misfit.d.min() ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax) nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax) plt.show() Number of observation points: 300 6. Set up the model and test gradient and Hessian The model is defined by three component: - the PDEVariationalProblem pde which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems. - the Prior prior which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator) - the Misfit misfit which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables. To test gradient and the Hessian of the model we use forward finite differences. model = Model(pde, prior, misfit) a0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER]) modelVerify(model, a0.vector(), 1e-12) (yy, H xx) - (xx, H yy) = 7.57910091653e-15 7. Compute the MAP point We used the globalized Newtown-CG method to compute the MAP point. a0 = prior.mean.copy() solver = ReducedSpaceNewtonCG(model) solver.parameters[\"rel_tolerance\"] = 1e-9 solver.parameters[\"abs_tolerance\"] = 1e-12 solver.parameters[\"max_iter\"] = 25 solver.parameters[\"inner_rel_tolerance\"] = 1e-15 solver.parameters[\"c_armijo\"] = 1e-4 solver.parameters[\"GN_iter\"] = 5 x = solver.solve(a0) if solver.converged: print(\"\\nConverged in \", solver.it, \" iterations.\") else: print(\"\\nNot Converged\") print(\"Termination reason: \", solver.termination_reasons[solver.reason]) print(\"Final gradient norm: \", solver.final_grad_norm) print(\"Final cost: \", solver.final_cost) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\") nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\") plt.show() It cg_it cost misfit reg (g,da) ||g||L2 alpha tolcg 1 1 1.206547e+03 1.206233e+03 3.147385e-01 -1.570552e+04 1.042537e+05 1.000000e+00 5.000000e-01 2 3 3.459350e+02 3.446814e+02 1.253641e+00 -1.846772e+03 1.431429e+04 1.000000e+00 3.705435e-01 3 1 2.746765e+02 2.733672e+02 1.309257e+00 -1.424615e+02 1.003736e+04 1.000000e+00 3.102873e-01 4 7 1.691763e+02 1.647605e+02 4.415777e+00 -2.128491e+02 3.871034e+03 1.000000e+00 1.926938e-01 5 6 1.573210e+02 1.522926e+02 5.028367e+00 -2.345647e+01 1.821115e+03 1.000000e+00 1.321670e-01 6 14 1.424926e+02 1.297495e+02 1.274310e+01 -2.988285e+01 1.157426e+03 1.000000e+00 1.053661e-01 7 2 1.421601e+02 1.294134e+02 1.274672e+01 -6.643211e-01 7.325602e+02 1.000000e+00 8.382545e-02 8 22 1.407914e+02 1.253200e+02 1.547141e+01 -2.734442e+00 4.409802e+02 1.000000e+00 6.503749e-02 9 14 1.407826e+02 1.253781e+02 1.540453e+01 -1.744746e-02 5.057992e+01 1.000000e+00 2.202639e-02 10 29 1.407817e+02 1.253307e+02 1.545107e+01 -1.819642e-03 1.502434e+01 1.000000e+00 1.200472e-02 11 38 1.407817e+02 1.253304e+02 1.545139e+01 -2.502663e-07 1.390539e-01 1.000000e+00 1.154904e-03 12 54 1.407817e+02 1.253303e+02 1.545139e+01 -1.380451e-12 2.625467e-04 1.000000e+00 5.018311e-05 Converged in 12 iterations. Termination reason: Norm of the gradient less than tolerance Final gradient norm: 5.41022518768e-09 Final cost: 140.781736843 8. Compute the low rank Gaussian approximation of the posterior We used the double pass algorithm to compute a low-rank decomposition of the Hessian Misfit. In particular, we solve \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}. The Figure shows the largest k generalized eigenvectors of the Hessian misfit. The effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1). The effective rank is independent of the mesh size. model.setPointForHessianEvaluations(x) Hmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], gauss_newton_approx=False, misfit_only=True) k = 50 p = 20 print(\"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p)) Omega = np.random.randn(get_local_size(x[PARAMETER]), k+p) d, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior(prior, d, U) posterior.mean = x[PARAMETER] plt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15]) Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20. 9. Prior and posterior pointwise variance fields compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print(\"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr)) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200) objs = [dl.Function(Vh[PARAMETER], pr_pw_variance), dl.Function(Vh[PARAMETER], post_pw_variance)] mytitles = [\"Prior variance\", \"Posterior variance\"] nb.multi1_plot(objs, mytitles, logscale=True) plt.show() Posterior trace 1.058859e-01; Prior trace 3.949847e-01; Correction trace 2.890987e-01 10. Generate samples from Prior and Posterior nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") noise_size = get_local_size(noise) s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\") s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): noise.set_local( np.random.randn( noise_size ) ) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121, mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122, mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Index"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#example-bayesian-quantification-of-parameter-uncertainty","text":"","title":"Example: Bayesian quantification of parameter uncertainty:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#estimating-the-gaussian-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde","text":"In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by an elliptic PDE via the Bayesian inference framework. Hence, we state the inverse problem as a problem of statistical inference over the space of uncertain parameters, which are to be inferred from data and a physical model. The resulting solution to the statistical inverse problem is a posterior distribution that assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true'' parameter field that gave rise to the observed data. For simplicity, in what follows we give finite-dimensional expressions (i.e., after discretization of the parameter space) for the Bayesian formulation of the inverse problem.","title":"Estimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#bayes-theorem","text":"The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m) over the parameter space, which encodes any knowledge or assumptions about the parameter space that we may wish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m) , which explicitly represents the probability that a given set of parameters \\m might give rise to the observed data \\vec{d}_{\\text{obs}} \\in \\mathbb{R}^m , namely: <script type=\"math/tex; mode=display\">\\begin{align} \\pi_{\\text{post}}(\\m | \\data) \\propto \\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m). \\end{align} Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.","title":"Bayes' Theorem:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#gaussian-prior-and-noise","text":"","title":"Gaussian prior and noise:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-prior","text":"We consider a Gaussian prior with mean \\vec m_{\\text prior} and covariance \\bf \\Gamma_{\\text{prior}} . The covariance is given by the discretization of the inverse of differential operator \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where \\gamma , \\delta > 0 control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem","title":"The prior:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-likelihood","text":"\\data = \\bf{f}(\\m) + \\bf{e }, \\;\\;\\; \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} ) \\pi_{\\text like}(\\data \\; | \\; \\m) = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right) Here \\bf f is the parameter-to-observable map that takes a parameter vector \\m and maps it to the space observation vector \\data .","title":"The likelihood:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-posterior","text":"\\pi_{\\text{post}}(\\m \\; | \\; \\data) \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)","title":"The posterior:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-gaussian-approximation-of-the-posterior-mathcalnvecmapbf-gamma_textpost","text":"The mean of this posterior distribution, \\vec{\\map} , is the parameter vector maximizing the posterior, and is known as the maximum a posteriori (MAP) point. It can be found by minimizing the negative log of the posterior, which amounts to solving a deterministic inverse problem) with appropriately weighted norms, \\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\; \\Big( \\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} +\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\Big). The posterior covariance matrix is then given by the inverse of the Hessian matrix of \\mathcal{J} at \\map , namely \\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}","title":"The Gaussian approximation of the posterior: \\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-prior-preconditioned-hessian-of-the-data-misfit","text":"\\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}","title":"The prior-preconditioned Hessian of the data misfit:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-generalized-eigenvalue-problem","text":"\\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda}, where \\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} contains the generalized eigenvalues and the columns of \\matrix V\\in \\mathbb R^{n\\times n} the generalized eigenvectors such that \\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I} .","title":"The generalized eigenvalue problem:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#randomized-eigensolvers-to-construct-the-approximate-spectral-decomposition","text":"When the generalized eigenvalues \\{\\lambda_i\\} decay rapidly, we can extract a low-rank approximation of \\Hmisfit by retaining only the r largest eigenvalues and corresponding eigenvectors, \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2}, Here, \\matrix{V}_r \\in \\mathbb{R}^{n\\times r} contains only the r generalized eigenvectors of \\Hmisfit that correspond to the r largest eigenvalues, which are assembled into the diagonal matrix \\matrix{\\Lambda}_r = \\diag (\\lambda_i) \\in \\mathbb{R}^{r \\times r} .","title":"Randomized eigensolvers to construct the approximate spectral decomposition:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#the-approximate-posterior-covariance","text":"Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \\begin{align} \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1} = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T + \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i + 1}\\right), \\end{align} where \\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in \\mathbb{R}^{r\\times r} . The last term in this expression captures the error due to truncation in terms of the discarded eigenvalues; this provides a criterion for truncating the spectrum, namely that r is chosen such that \\lambda_r is small relative to 1. Therefore we can approximate the posterior covariance as \\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T","title":"The approximate posterior covariance::"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#drawing-samples-from-a-gaussian-distribution-with-covariance-h-1","text":"Let \\bf x be a sample for the prior distribution, i.e. \\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1}) as {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r + \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1} + \\bf I \\big\\} {\\bf x}","title":"Drawing samples from a Gaussian distribution with covariance \\H^{-1}"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#this-tutorial-shows","text":"description of the inverse problem (the forward problem, the prior, and the misfit functional) convergence of the inexact Newton-CG algorithm low-rank-based approximation of the posterior covariance (built on a low-rank approximation of the Hessian of the data misfit) how to construct the low-rank approximation of the Hessian of the data misfit how to apply the inverse and square-root inverse Hessian to a vector efficiently samples from the Gaussian approximation of the posterior","title":"This tutorial shows:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#goals","text":"By the end of this notebook, you should be able to: understand the Bayesian inverse framework visualise and understand the results modify the problem and code","title":"Goals:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#mathematical-tools-used","text":"Finite element method Derivation of gradiant and Hessian via the adjoint method inexact Newton-CG Armijo line search Bayes' formula randomized eigensolvers","title":"Mathematical tools used:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#list-of-software-used","text":"FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , A great python package that I used for plotting many of the results Numpy , A python package for linear algebra. While extensive, this is mostly used to compute means and sums in this notebook.","title":"List of software used:"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#1-load-modules","text":"from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import nb import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(seed=1)","title":"1. Load modules"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#2-generate-the-true-parameter","text":"This function generates a random field with a prescribed anysotropic covariance function. def true_model(Vh, gamma, delta, anis_diff): prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff ) noise = dl.Vector() prior.init_vector(noise,\"noise\") noise_size = get_local_size(noise) noise.set_local( np.random.randn( noise_size ) ) atrue = dl.Vector() prior.init_vector(atrue, 0) prior.sample(noise,atrue) return atrue","title":"2. Generate the true parameter"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces","text":"We compute a two dimensional mesh of a unit square with nx by ny elements. We define a P2 finite element space for the state and adjoint variable and P1 for the parameter . ndim = 2 nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh2, Vh1, Vh2] print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641","title":"3. Set up the mesh and finite element spaces"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#4-set-up-the-forward-problem","text":"To set up the forward problem we use the PDEVariationalProblem class, which requires the following inputs - the finite element spaces for the state, parameter, and adjoint variables Vh - the pde in weak form pde_varf - the boundary conditions bc for the forward problem and bc0 for the adjoint and incremental problems. The PDEVariationalProblem class offer the following functionality: - solving the forward/adjoint and incremental problems - evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables. def u_boundary(x, on_boundary): return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS) u_bdr = dl.Expression(\"x[1]\", degree=1) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) f = dl.Constant(0.0) def pde_varf(u,a,p): return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)","title":"4. Set up the forward problem"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#4-set-up-the-prior","text":"To obtain the synthetic true paramter a_{\\rm true} we generate a realization of a Gaussian random field with zero average and covariance matrix \\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2} , where \\widetilde{\\mathcal{A}} is a differential operator of the form \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. Here \\Theta is an s.p.d. anisotropic tensor of the form \\Theta = \\begin{bmatrix} \\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\ (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2. \\end{bmatrix} For the prior model, we assume that we can measure the log-permeability coefficient at N locations, and we denote with a^1_{\\rm true} , \\ldots , a^N_{\\rm true} such measures. We also introduce the mollifier functions \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N, and we let \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M}, where p is a penalization costant (10 for this problem) and \\mathcal{M} = \\sum_{i=1}^N \\delta_i I . We then compute a_{\\rm pr} , the mean of the prior measure, as a regularized least-squares fit of these point observations by solving a_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle. Finally the prior distribution is \\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior}) , with \\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2} . gamma = .1 delta = .5 anis_diff = dl.Expression(code_AnisTensor2D, degree=1) anis_diff.theta0 = 2. anis_diff.theta1 = .5 anis_diff.alpha = math.pi/4 atrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff) locations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]]) pen = 1e1 prior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen) print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format( delta, gamma,2) ) objs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)] mytitles = [\"True Parameter\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() model = Model(pde,prior, misfit) Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2","title":"4. Set up the prior"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations","text":"To setup the observation operator, we generate ntargets random locations where to evaluate the value of the state. To generate the synthetic observation, we first solve the forward problem using the true parameter a_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise. rel_noise is the signal to noise ratio. ntargets = 300 rel_noise = 0.01 targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) print(\"Number of observation points: {0}\".format(ntargets)) misfit = PointwiseStateObservation(Vh[STATE], targets) utrue = pde.generate_state() x = [utrue, atrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX randn_perturb(misfit.d, noise_std_dev) misfit.noise_variance = noise_std_dev*noise_std_dev vmax = max( utrue.max(), misfit.d.max() ) vmin = min( utrue.min(), misfit.d.min() ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax) nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax) plt.show() Number of observation points: 300","title":"5. Set up the misfit functional and generate synthetic observations"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian","text":"The model is defined by three component: - the PDEVariationalProblem pde which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems. - the Prior prior which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator) - the Misfit misfit which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables. To test gradient and the Hessian of the model we use forward finite differences. model = Model(pde, prior, misfit) a0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER]) modelVerify(model, a0.vector(), 1e-12) (yy, H xx) - (xx, H yy) = 7.57910091653e-15","title":"6. Set up the model and test gradient and Hessian"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#7-compute-the-map-point","text":"We used the globalized Newtown-CG method to compute the MAP point. a0 = prior.mean.copy() solver = ReducedSpaceNewtonCG(model) solver.parameters[\"rel_tolerance\"] = 1e-9 solver.parameters[\"abs_tolerance\"] = 1e-12 solver.parameters[\"max_iter\"] = 25 solver.parameters[\"inner_rel_tolerance\"] = 1e-15 solver.parameters[\"c_armijo\"] = 1e-4 solver.parameters[\"GN_iter\"] = 5 x = solver.solve(a0) if solver.converged: print(\"\\nConverged in \", solver.it, \" iterations.\") else: print(\"\\nNot Converged\") print(\"Termination reason: \", solver.termination_reasons[solver.reason]) print(\"Final gradient norm: \", solver.final_grad_norm) print(\"Final cost: \", solver.final_cost) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\") nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\") plt.show() It cg_it cost misfit reg (g,da) ||g||L2 alpha tolcg 1 1 1.206547e+03 1.206233e+03 3.147385e-01 -1.570552e+04 1.042537e+05 1.000000e+00 5.000000e-01 2 3 3.459350e+02 3.446814e+02 1.253641e+00 -1.846772e+03 1.431429e+04 1.000000e+00 3.705435e-01 3 1 2.746765e+02 2.733672e+02 1.309257e+00 -1.424615e+02 1.003736e+04 1.000000e+00 3.102873e-01 4 7 1.691763e+02 1.647605e+02 4.415777e+00 -2.128491e+02 3.871034e+03 1.000000e+00 1.926938e-01 5 6 1.573210e+02 1.522926e+02 5.028367e+00 -2.345647e+01 1.821115e+03 1.000000e+00 1.321670e-01 6 14 1.424926e+02 1.297495e+02 1.274310e+01 -2.988285e+01 1.157426e+03 1.000000e+00 1.053661e-01 7 2 1.421601e+02 1.294134e+02 1.274672e+01 -6.643211e-01 7.325602e+02 1.000000e+00 8.382545e-02 8 22 1.407914e+02 1.253200e+02 1.547141e+01 -2.734442e+00 4.409802e+02 1.000000e+00 6.503749e-02 9 14 1.407826e+02 1.253781e+02 1.540453e+01 -1.744746e-02 5.057992e+01 1.000000e+00 2.202639e-02 10 29 1.407817e+02 1.253307e+02 1.545107e+01 -1.819642e-03 1.502434e+01 1.000000e+00 1.200472e-02 11 38 1.407817e+02 1.253304e+02 1.545139e+01 -2.502663e-07 1.390539e-01 1.000000e+00 1.154904e-03 12 54 1.407817e+02 1.253303e+02 1.545139e+01 -1.380451e-12 2.625467e-04 1.000000e+00 5.018311e-05 Converged in 12 iterations. Termination reason: Norm of the gradient less than tolerance Final gradient norm: 5.41022518768e-09 Final cost: 140.781736843","title":"7. Compute the MAP point"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#8-compute-the-low-rank-gaussian-approximation-of-the-posterior","text":"We used the double pass algorithm to compute a low-rank decomposition of the Hessian Misfit. In particular, we solve \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}. The Figure shows the largest k generalized eigenvectors of the Hessian misfit. The effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1). The effective rank is independent of the mesh size. model.setPointForHessianEvaluations(x) Hmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], gauss_newton_approx=False, misfit_only=True) k = 50 p = 20 print(\"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p)) Omega = np.random.randn(get_local_size(x[PARAMETER]), k+p) d, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior(prior, d, U) posterior.mean = x[PARAMETER] plt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15]) Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.","title":"8. Compute the low rank Gaussian approximation of the posterior"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#9-prior-and-posterior-pointwise-variance-fields","text":"compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print(\"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr)) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200) objs = [dl.Function(Vh[PARAMETER], pr_pw_variance), dl.Function(Vh[PARAMETER], post_pw_variance)] mytitles = [\"Prior variance\", \"Posterior variance\"] nb.multi1_plot(objs, mytitles, logscale=True) plt.show() Posterior trace 1.058859e-01; Prior trace 3.949847e-01; Correction trace 2.890987e-01","title":"9. Prior and posterior pointwise variance fields"},{"location":"tutorials_v1.6.0/3_SubsurfaceBayesian/#10-generate-samples-from-prior-and-posterior","text":"nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") noise_size = get_local_size(noise) s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\") s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): noise.set_local( np.random.randn( noise_size ) ) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121, mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122, mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"10. Generate samples from Prior and Posterior"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/","text":"\\def\\D{\\mathcal{D}} \\def\\ipar{m} \\def\\R{\\mathbb{R}} \\def\\del{\\partial} \\def\\vec{\\bf} \\def\\priorm{\\mu_0} \\def\\C{\\mathcal{C}} \\def\\Acal{\\mathcal{A}} \\def\\postm{\\mu_{\\rm{post}}} \\def\\iparpost{\\ipar_\\text{post}} \\def\\obs{\\vec{d}} \\def\\yobs{\\obs^{\\text{obs}}} \\def\\obsop{\\mathcal{B}} \\def\\dd{\\vec{\\bar{d}}} \\def\\iFF{\\mathcal{F}} \\def\\iFFadj{\\mathcal{F}^*} \\def\\ncov{\\Gamma_{\\mathrm{noise}}} Example: Bayesian initial condition inversion in an advection-diffusion problem In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements. The Bayesian inverse problem: Following the Bayesian framework, we utilize a Gaussian prior measure \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) , with \\C_0=\\Acal^{-2} where \\Acal is an elliptic differential operator as described in the PoissonBayesian example, and use an additive Gaussian noise model. Therefore, the solution of the Bayesian inverse problem is the posterior measure, \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post}) with \\iparpost and \\C_\\text{post} . The posterior mean \\iparpost is characterized as the minimizer of \\begin{aligned} & \\mathcal{J}(\\ipar) := \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs \\right\\|^2_{\\ncov^{-1}} + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)}, \\end{aligned} which can also be interpreted as the regularized functional to be minimized in deterministic inversion. The observation operator \\mathcal{B} extracts the values of the forward solution u on a set of locations \\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D at times \\{t_1, \\ldots, t_N\\} \\subset [0, T] . The posterior covariance \\C_{\\text{post}} is the inverse of the Hessian of \\mathcal{J}(\\ipar) , i.e., \\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}. The forward problem: The PDE in the parameter-to-observable map \\iFF models diffusive transport in a domain \\D \\subset \\R^d ( d \\in \\{2, 3\\} ): \\begin{split} u_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0 & \\quad \\text{in } \\D\\times(0,T),\\\\ u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\ \\kappa \\nabla u\\cdot \\vec{n} &= 0 & \\quad \\text{on } \\partial\\D \\times (0,T). \\end{split} Here, \\kappa > 0 is the diffusion coefficient and T > 0 is the final time. The velocity field \\vec{v} is computed by solving the following steady-state Navier-Stokes equation with the side walls driving the flow: \\begin{aligned} - \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\ \\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\ \\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D. \\end{aligned} Here, q is pressure, \\text{Re} is the Reynolds number. The Dirichlet boundary data \\vec{g} \\in \\R^d is given by \\vec{g} = \\vec{e}_2 on the left wall of the domain, \\vec{g}=-\\vec{e}_2 on the right wall, and \\vec{g} = \\vec{0} everywhere else. The adjoint problem: \\begin{aligned} -p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\ p(\\cdot, T) &= 0 & &\\text{ in } \\D,\\\\ (\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n} &= 0 & &\\text{ on } \\partial\\D\\times (0,T). \\end{aligned} 1. Load modules from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" ) from model_ad_diff import TimeDependentAD import nb import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(1) 2. Construct the velocity field def v_boundary(x,on_boundary): return on_boundary def q_boundary(x,on_boundary): return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS def computeVelocityField(mesh): Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2) Wh = dl.FunctionSpace(mesh, 'Lagrange', 1) if dlversion() <= (1,6,0): XW = dl.MixedFunctionSpace([Xh, Wh]) else: mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()]) XW = dl.FunctionSpace(mesh, mixed_element) Re = 1e2 g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1) bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary) bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise') bcs = [bc1, bc2] vq = dl.Function(XW) (v,q) = dl.split(vq) (v_test, q_test) = dl.TestFunctions (XW) def strain(v): return dl.sym(dl.nabla_grad(v)) F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test) - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\": {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}}) plt.figure(figsize=(15,5)) vh = dl.project(v,Xh) qh = dl.project(q,Wh) nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\") nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\") plt.show() return v 3. Set up the mesh and finite element spaces mesh = dl.refine( dl.Mesh(\"ad_20.xml\") ) wind_velocity = computeVelocityField(mesh) Vh = dl.FunctionSpace(mesh, \"Lagrange\", 1) print(\"Number of dofs: {0}\".format( Vh.dim() ) ) Number of dofs: 2023 4. Set up model (prior, true/proposed initial condition) #gamma = 1 #delta = 10 #prior = LaplacianPrior(Vh, gamma, delta) gamma = 1 delta = 8 prior = BiLaplacianPrior(Vh, gamma, delta) prior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector() true_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))', degree=5), Vh).vector() problem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior) objs = [dl.Function(Vh,true_initial_condition), dl.Function(Vh,prior.mean)] mytitles = [\"True Initial Condition\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() 5. Generate the synthetic observations rel_noise = 0.001 utrue = problem.generate_vector(STATE) x = [utrue, true_initial_condition, None] problem.solveFwd(x[STATE], x, 1e-9) MAX = utrue.norm(\"linf\", \"linf\") noise_std_dev = rel_noise * MAX problem.ud.copy(utrue) problem.ud.randn_perturb(noise_std_dev) problem.noise_variance = noise_std_dev*noise_std_dev nb.show_solution(Vh, true_initial_condition, utrue, \"Solution\") 6. Test the gradient and the Hessian of the cost (negative log posterior) a0 = true_initial_condition.copy() modelVerify(problem, a0, 1e-12, is_quadratic=True) (yy, H xx) - (xx, H yy) = -4.75341826113e-14 7. Evaluate the gradient [u,a,p] = problem.generate_vector() problem.solveFwd(u, [u,a,p], 1e-12) problem.solveAdj(p, [u,a,p], 1e-12) mg = problem.generate_vector(PARAMETER) grad_norm = problem.evalGradientParameter([u,a,p], mg) print(\"(g,g) = \", grad_norm) (g,g) = 1.66716039169e+12 8. The Gaussian approximation of the posterior H = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) k = 80 p = 20 print(\"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p)) Omega = np.random.randn(get_local_size(a), k+p) d, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior( prior, d, U ) plt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh, U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60]) Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20. 9. Compute the MAP point H.misfit_only = False solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( posterior.Hlr ) solver.parameters[\"print_level\"] = 1 solver.parameters[\"rel_tolerance\"] = 1e-6 solver.solve(a, -mg) problem.solveFwd(u, [u,a,p], 1e-12) total_cost, reg_cost, misfit_cost = problem.cost([u,a,p]) print(\"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost)) posterior.mean = a plt.figure(figsize=(7.5,5)) nb.plot(dl.Function(Vh, a), mytitle=\"Initial Condition\") plt.show() nb.show_solution(Vh, a, u, \"Solution\") Iterartion : 0 (B r, r) = 30140.7469425 Iteration : 1 (B r, r) = 0.0653733954192 Iteration : 2 (B r, r) = 6.28002367536e-06 Iteration : 3 (B r, r) = 9.57007003125e-10 Relative/Absolute residual less than tol Converged in 3 iterations with final norm 3.09355297858e-05 Total cost 84.2612; Reg Cost 68.8823; Misfit 15.3789 10. Prior and posterior pointwise variance fields compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print(\"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr)) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300) objs = [dl.Function(Vh, pr_pw_variance), dl.Function(Vh, post_pw_variance)] mytitles = [\"Prior Variance\", \"Posterior Variance\"] nb.multi1_plot(objs, mytitles, logscale=True) plt.show() Posterior trace 0.000602854; Prior trace 0.0285673; Correction trace 0.0279644 11. Draw samples from the prior and posterior distributions nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") noise_size = get_local_size(noise) s_prior = dl.Function(Vh, name=\"sample_prior\") s_post = dl.Function(Vh, name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): noise.set_local( np.random.randn( noise_size ) ) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Index"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#example-bayesian-initial-condition-inversion-in-an-advection-diffusion-problem","text":"In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.","title":"Example: Bayesian initial condition inversion in an advection-diffusion problem"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#the-bayesian-inverse-problem","text":"Following the Bayesian framework, we utilize a Gaussian prior measure \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) , with \\C_0=\\Acal^{-2} where \\Acal is an elliptic differential operator as described in the PoissonBayesian example, and use an additive Gaussian noise model. Therefore, the solution of the Bayesian inverse problem is the posterior measure, \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post}) with \\iparpost and \\C_\\text{post} . The posterior mean \\iparpost is characterized as the minimizer of \\begin{aligned} & \\mathcal{J}(\\ipar) := \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs \\right\\|^2_{\\ncov^{-1}} + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)}, \\end{aligned} which can also be interpreted as the regularized functional to be minimized in deterministic inversion. The observation operator \\mathcal{B} extracts the values of the forward solution u on a set of locations \\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D at times \\{t_1, \\ldots, t_N\\} \\subset [0, T] . The posterior covariance \\C_{\\text{post}} is the inverse of the Hessian of \\mathcal{J}(\\ipar) , i.e., \\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.","title":"The Bayesian inverse problem:"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#the-forward-problem","text":"The PDE in the parameter-to-observable map \\iFF models diffusive transport in a domain \\D \\subset \\R^d ( d \\in \\{2, 3\\} ): \\begin{split} u_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0 & \\quad \\text{in } \\D\\times(0,T),\\\\ u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\ \\kappa \\nabla u\\cdot \\vec{n} &= 0 & \\quad \\text{on } \\partial\\D \\times (0,T). \\end{split} Here, \\kappa > 0 is the diffusion coefficient and T > 0 is the final time. The velocity field \\vec{v} is computed by solving the following steady-state Navier-Stokes equation with the side walls driving the flow: \\begin{aligned} - \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\ \\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\ \\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D. \\end{aligned} Here, q is pressure, \\text{Re} is the Reynolds number. The Dirichlet boundary data \\vec{g} \\in \\R^d is given by \\vec{g} = \\vec{e}_2 on the left wall of the domain, \\vec{g}=-\\vec{e}_2 on the right wall, and \\vec{g} = \\vec{0} everywhere else.","title":"The forward problem:"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#the-adjoint-problem","text":"\\begin{aligned} -p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\ p(\\cdot, T) &= 0 & &\\text{ in } \\D,\\\\ (\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n} &= 0 & &\\text{ on } \\partial\\D\\times (0,T). \\end{aligned}","title":"The adjoint problem:"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#1-load-modules","text":"from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" ) from model_ad_diff import TimeDependentAD import nb import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(1)","title":"1. Load modules"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#2-construct-the-velocity-field","text":"def v_boundary(x,on_boundary): return on_boundary def q_boundary(x,on_boundary): return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS def computeVelocityField(mesh): Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2) Wh = dl.FunctionSpace(mesh, 'Lagrange', 1) if dlversion() <= (1,6,0): XW = dl.MixedFunctionSpace([Xh, Wh]) else: mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()]) XW = dl.FunctionSpace(mesh, mixed_element) Re = 1e2 g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1) bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary) bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise') bcs = [bc1, bc2] vq = dl.Function(XW) (v,q) = dl.split(vq) (v_test, q_test) = dl.TestFunctions (XW) def strain(v): return dl.sym(dl.nabla_grad(v)) F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test) - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\": {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}}) plt.figure(figsize=(15,5)) vh = dl.project(v,Xh) qh = dl.project(q,Wh) nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\") nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\") plt.show() return v","title":"2. Construct the velocity field"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces","text":"mesh = dl.refine( dl.Mesh(\"ad_20.xml\") ) wind_velocity = computeVelocityField(mesh) Vh = dl.FunctionSpace(mesh, \"Lagrange\", 1) print(\"Number of dofs: {0}\".format( Vh.dim() ) ) Number of dofs: 2023","title":"3. Set up the mesh and finite element spaces"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition","text":"#gamma = 1 #delta = 10 #prior = LaplacianPrior(Vh, gamma, delta) gamma = 1 delta = 8 prior = BiLaplacianPrior(Vh, gamma, delta) prior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector() true_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))', degree=5), Vh).vector() problem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior) objs = [dl.Function(Vh,true_initial_condition), dl.Function(Vh,prior.mean)] mytitles = [\"True Initial Condition\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show()","title":"4. Set up model (prior, true/proposed initial condition)"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations","text":"rel_noise = 0.001 utrue = problem.generate_vector(STATE) x = [utrue, true_initial_condition, None] problem.solveFwd(x[STATE], x, 1e-9) MAX = utrue.norm(\"linf\", \"linf\") noise_std_dev = rel_noise * MAX problem.ud.copy(utrue) problem.ud.randn_perturb(noise_std_dev) problem.noise_variance = noise_std_dev*noise_std_dev nb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")","title":"5. Generate the synthetic observations"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior","text":"a0 = true_initial_condition.copy() modelVerify(problem, a0, 1e-12, is_quadratic=True) (yy, H xx) - (xx, H yy) = -4.75341826113e-14","title":"6. Test the gradient and the Hessian of the cost (negative log posterior)"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#7-evaluate-the-gradient","text":"[u,a,p] = problem.generate_vector() problem.solveFwd(u, [u,a,p], 1e-12) problem.solveAdj(p, [u,a,p], 1e-12) mg = problem.generate_vector(PARAMETER) grad_norm = problem.evalGradientParameter([u,a,p], mg) print(\"(g,g) = \", grad_norm) (g,g) = 1.66716039169e+12","title":"7. Evaluate the gradient"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#8-the-gaussian-approximation-of-the-posterior","text":"H = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) k = 80 p = 20 print(\"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p)) Omega = np.random.randn(get_local_size(a), k+p) d, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior( prior, d, U ) plt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh, U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60]) Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.","title":"8. The Gaussian approximation of the posterior"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#9-compute-the-map-point","text":"H.misfit_only = False solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( posterior.Hlr ) solver.parameters[\"print_level\"] = 1 solver.parameters[\"rel_tolerance\"] = 1e-6 solver.solve(a, -mg) problem.solveFwd(u, [u,a,p], 1e-12) total_cost, reg_cost, misfit_cost = problem.cost([u,a,p]) print(\"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost)) posterior.mean = a plt.figure(figsize=(7.5,5)) nb.plot(dl.Function(Vh, a), mytitle=\"Initial Condition\") plt.show() nb.show_solution(Vh, a, u, \"Solution\") Iterartion : 0 (B r, r) = 30140.7469425 Iteration : 1 (B r, r) = 0.0653733954192 Iteration : 2 (B r, r) = 6.28002367536e-06 Iteration : 3 (B r, r) = 9.57007003125e-10 Relative/Absolute residual less than tol Converged in 3 iterations with final norm 3.09355297858e-05 Total cost 84.2612; Reg Cost 68.8823; Misfit 15.3789","title":"9. Compute the MAP point"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields","text":"compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print(\"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr)) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300) objs = [dl.Function(Vh, pr_pw_variance), dl.Function(Vh, post_pw_variance)] mytitles = [\"Prior Variance\", \"Posterior Variance\"] nb.multi1_plot(objs, mytitles, logscale=True) plt.show() Posterior trace 0.000602854; Prior trace 0.0285673; Correction trace 0.0279644","title":"10. Prior and posterior pointwise variance fields"},{"location":"tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions","text":"nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") noise_size = get_local_size(noise) s_prior = dl.Function(Vh, name=\"sample_prior\") s_post = dl.Function(Vh, name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): noise.set_local( np.random.randn( noise_size ) ) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"11. Draw samples from the prior and posterior distributions"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/","text":"Spectrum of the Reduced Hessian The linear source inversion problem We consider the following linear source inversion problem. Find the state u \\in H^1_{\\Gamma_D}(\\Omega) and the source ( parameter ) a \\in H^1(\\Omega) that solves \\begin{align*} {} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\ {\\rm s.t.} & {} &{} \\\\ {} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\ {} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\ {} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\ \\end{align*} Here: u_d is a n_{\\rm obs} finite dimensional vector that denotes noisy observations of the state u in n_{\\rm obs} locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . More specifically, u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where \\eta_i are i.i.d. \\mathcal{N}(0, \\sigma^2) . B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}} is the linear operator that evaluates the state u at the observation locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . \\delta and \\gamma are the parameters of the regularization penalizing the L^2(\\Omega) and H^1(\\Omega) norm of a-a_0 , respectively. k , {\\bf v} , c are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively. \\Gamma_D \\subset \\partial \\Omega , \\Gamma_N \\subset \\partial \\Omega represents the subdomain of \\partial\\Omega where we impose Dirichlet or Neumann boundary conditions, respectively. 1. Load modules from __future__ import absolute_import, division, print_function import dolfin as dl import numpy as np import matplotlib.pyplot as plt %matplotlib inline import nb import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) 2. The linear source inversion problem def pde_varf(u,a,p): return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\ + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\ + c*u*p*dl.dx \\ - a*p*dl.dx def u_boundary(x, on_boundary): return on_boundary and x[1] < dl.DOLFIN_EPS def solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True): np.random.seed(seed=2) mesh = dl.UnitSquareMesh(nx, ny) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh1, Vh1, Vh1] if verbose: print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) u_bdr = dl.Constant(0.0) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) atrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector() a0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector() pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) if verbose: print(\"Number of observation points: {0}\".format(targets.shape[0])) misfit = PointwiseStateObservation(Vh[STATE], targets) reg = LaplacianPrior(Vh[PARAMETER], gamma, delta) #Generate synthetic observations utrue = pde.generate_state() x = [utrue, atrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX randn_perturb(misfit.d, noise_std_dev) misfit.noise_variance = noise_std_dev*noise_std_dev if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle = \"True source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132) nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133) plt.show() model = Model(pde, reg, misfit) u = model.generate_vector(STATE) a = a0.copy() p = model.generate_vector(ADJOINT) x = [u,a,p] mg = model.generate_vector(PARAMETER) model.solveFwd(u, x) model.solveAdj(p, x) model.evalGradientParameter(x, mg) model.setPointForHessianEvaluations(x) H = ReducedHessian(model, 1e-12) solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( reg.Rsolver ) solver.parameters[\"print_level\"] = -1 solver.parameters[\"rel_tolerance\"] = 1e-9 solver.solve(a, -mg) if solver.converged: if verbose: print(\"CG converged in \", solver.iter, \" iterations.\") else: print(\"CG did not converged.\") raise model.solveFwd(u, x, 1e-12) total_cost, reg_cost, misfit_cost = model.cost(x) if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], a), mytitle = \"Reconstructed source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132) nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133) plt.show() H.misfit_only = True k_evec = 80 p_evec = 5 if verbose: print(\"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec)) Omega = np.random.randn(get_local_size(a), k_evec+p_evec) d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec) if verbose: plt.figure() nb.plot_eigenvalues(d, mytitle=\"Generalized Eigenvalues\") nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15]) plt.show() return d, U, Vh[PARAMETER], solver.iter 3. Solution of the source inversion problem ndim = 2 nx = 32 ny = 32 ntargets = 300 np.random.seed(seed=1) targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) rel_noise = 0.01 gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) d, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta) Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089 Number of observation points: 300 CG converged in 74 iterations. Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5. 4. Mesh independence of the spectrum of the preconditioned Hessian gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) n = [16,32,64] d1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False) d2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False) d3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False) print(\"Number of Iterations: \", niter1, niter2, niter3) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(d1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131) nb.plot_eigenvalues(d2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132) nb.plot_eigenvalues(d3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133) nb.plot_eigenvectors(Va1, U1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5]) nb.plot_eigenvectors(Va2, U2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5]) nb.plot_eigenvectors(Va3, U3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5]) plt.show() Number of Iterations: 62 74 76 5. Dependence on the noise level We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization. gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) rel_noise = [1e-3,1e-2,1e-1] d1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False) d2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False) d3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False) print(\"Number of Iterations: \", niter1, niter2, niter3) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(d1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131) nb.plot_eigenvalues(d2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132) nb.plot_eigenvalues(d3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133) nb.plot_eigenvectors(Va1, U1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5]) nb.plot_eigenvectors(Va2, U2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5]) nb.plot_eigenvectors(Va3, U3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5]) plt.show() Number of Iterations: 161 74 21 6. Dependence on the PDE coefficients Assume a constant reaction term c = 1 , and we consider different values for the diffusivity coefficient k . The smaller the value of k the slower the decay in the spectrum. rel_noise = 0.01 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(1.0) d1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.1) d2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.01) d3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) print(\"Number of Iterations: \", niter1, niter2, niter3) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(d1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131) nb.plot_eigenvalues(d2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132) nb.plot_eigenvalues(d3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133) nb.plot_eigenvectors(Va1, U1, mytitle=\"k=1. Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Va2, U2, mytitle=\"k=0.1 Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Va3, U3, mytitle=\"k=0.01 Eigen\", which=[0,1,5]) plt.show() Number of Iterations: 88 147 256 Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Spectrum of the Reduced Hessian"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#spectrum-of-the-reduced-hessian","text":"","title":"Spectrum of the Reduced Hessian"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#the-linear-source-inversion-problem","text":"We consider the following linear source inversion problem. Find the state u \\in H^1_{\\Gamma_D}(\\Omega) and the source ( parameter ) a \\in H^1(\\Omega) that solves \\begin{align*} {} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\ {\\rm s.t.} & {} &{} \\\\ {} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\ {} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\ {} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\ \\end{align*} Here: u_d is a n_{\\rm obs} finite dimensional vector that denotes noisy observations of the state u in n_{\\rm obs} locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . More specifically, u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where \\eta_i are i.i.d. \\mathcal{N}(0, \\sigma^2) . B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}} is the linear operator that evaluates the state u at the observation locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . \\delta and \\gamma are the parameters of the regularization penalizing the L^2(\\Omega) and H^1(\\Omega) norm of a-a_0 , respectively. k , {\\bf v} , c are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively. \\Gamma_D \\subset \\partial \\Omega , \\Gamma_N \\subset \\partial \\Omega represents the subdomain of \\partial\\Omega where we impose Dirichlet or Neumann boundary conditions, respectively.","title":"The linear source inversion problem"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#1-load-modules","text":"from __future__ import absolute_import, division, print_function import dolfin as dl import numpy as np import matplotlib.pyplot as plt %matplotlib inline import nb import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#2-the-linear-source-inversion-problem","text":"def pde_varf(u,a,p): return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\ + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\ + c*u*p*dl.dx \\ - a*p*dl.dx def u_boundary(x, on_boundary): return on_boundary and x[1] < dl.DOLFIN_EPS def solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True): np.random.seed(seed=2) mesh = dl.UnitSquareMesh(nx, ny) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh1, Vh1, Vh1] if verbose: print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) u_bdr = dl.Constant(0.0) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) atrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector() a0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector() pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) if verbose: print(\"Number of observation points: {0}\".format(targets.shape[0])) misfit = PointwiseStateObservation(Vh[STATE], targets) reg = LaplacianPrior(Vh[PARAMETER], gamma, delta) #Generate synthetic observations utrue = pde.generate_state() x = [utrue, atrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX randn_perturb(misfit.d, noise_std_dev) misfit.noise_variance = noise_std_dev*noise_std_dev if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle = \"True source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132) nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133) plt.show() model = Model(pde, reg, misfit) u = model.generate_vector(STATE) a = a0.copy() p = model.generate_vector(ADJOINT) x = [u,a,p] mg = model.generate_vector(PARAMETER) model.solveFwd(u, x) model.solveAdj(p, x) model.evalGradientParameter(x, mg) model.setPointForHessianEvaluations(x) H = ReducedHessian(model, 1e-12) solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( reg.Rsolver ) solver.parameters[\"print_level\"] = -1 solver.parameters[\"rel_tolerance\"] = 1e-9 solver.solve(a, -mg) if solver.converged: if verbose: print(\"CG converged in \", solver.iter, \" iterations.\") else: print(\"CG did not converged.\") raise model.solveFwd(u, x, 1e-12) total_cost, reg_cost, misfit_cost = model.cost(x) if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], a), mytitle = \"Reconstructed source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132) nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133) plt.show() H.misfit_only = True k_evec = 80 p_evec = 5 if verbose: print(\"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec)) Omega = np.random.randn(get_local_size(a), k_evec+p_evec) d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec) if verbose: plt.figure() nb.plot_eigenvalues(d, mytitle=\"Generalized Eigenvalues\") nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15]) plt.show() return d, U, Vh[PARAMETER], solver.iter","title":"2. The linear source inversion problem"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#3-solution-of-the-source-inversion-problem","text":"ndim = 2 nx = 32 ny = 32 ntargets = 300 np.random.seed(seed=1) targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) rel_noise = 0.01 gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) d, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta) Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089 Number of observation points: 300 CG converged in 74 iterations. Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.","title":"3. Solution of the source inversion problem"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian","text":"gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) n = [16,32,64] d1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False) d2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False) d3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False) print(\"Number of Iterations: \", niter1, niter2, niter3) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(d1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131) nb.plot_eigenvalues(d2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132) nb.plot_eigenvalues(d3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133) nb.plot_eigenvectors(Va1, U1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5]) nb.plot_eigenvectors(Va2, U2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5]) nb.plot_eigenvectors(Va3, U3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5]) plt.show() Number of Iterations: 62 74 76","title":"4. Mesh independence of the spectrum of the preconditioned Hessian"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#5-dependence-on-the-noise-level","text":"We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization. gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) rel_noise = [1e-3,1e-2,1e-1] d1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False) d2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False) d3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False) print(\"Number of Iterations: \", niter1, niter2, niter3) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(d1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131) nb.plot_eigenvalues(d2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132) nb.plot_eigenvalues(d3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133) nb.plot_eigenvectors(Va1, U1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5]) nb.plot_eigenvectors(Va2, U2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5]) nb.plot_eigenvectors(Va3, U3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5]) plt.show() Number of Iterations: 161 74 21","title":"5. Dependence on the noise level"},{"location":"tutorials_v1.6.0/5_HessianSpectrum/#6-dependence-on-the-pde-coefficients","text":"Assume a constant reaction term c = 1 , and we consider different values for the diffusivity coefficient k . The smaller the value of k the slower the decay in the spectrum. rel_noise = 0.01 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(1.0) d1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.1) d2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.01) d3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) print(\"Number of Iterations: \", niter1, niter2, niter3) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(d1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131) nb.plot_eigenvalues(d2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132) nb.plot_eigenvalues(d3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133) nb.plot_eigenvectors(Va1, U1, mytitle=\"k=1. Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Va2, U2, mytitle=\"k=0.1 Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Va3, U3, mytitle=\"k=0.01 Eigen\", which=[0,1,5]) plt.show() Number of Iterations: 88 147 256 Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"6. Dependence on the PDE coefficients"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/","text":"FEniCS101 Tutorial In this tutorial we consider the boundary value problem (BVP) \\begin{eqnarray*} - \\nabla \\cdot (k \\nabla u) = f & \\text{ in } \\Omega,\\\\ u = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\cup \\Gamma_{\\rm right},\\\\ k \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\cup \\Gamma_{\\rm bottom}, \\end{eqnarray*} where \\Omega = (0,1) \\times (0,1) , \\Gamma_D and \\Gamma_N are the union of the left and right, and top and bottom boundaries of \\Omega , respectively. Here \\begin{eqnarray*} k(x,y) = 1 & \\text{ on } \\Omega\\\\ f(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\ u_0(x,y) = 0 & \\text{ on } \\Gamma_D, \\\\ \\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right. & \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array} \\end{eqnarray*} The exact solution is u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). Weak formulation Let us define the Hilbert spaces V_{u_0}, V_0 \\in \\Omega as V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\}, V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}. To obtain the weak formulation, we multiply the PDE by an arbitrary function v \\in V_0 and integrate over the domain \\Omega leading to -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. Then, integration by parts the non-conforming term gives \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. Finally by recalling that v = 0 on \\Gamma_D and that k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma on \\Gamma_N , we find the weak formulation: Find * u \\in V_{u_0} such that* \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. 1. Load modules To start we load the following modules: dolfin: the python/C++ interface to FEniCS math : the python module for mathematical functions numpy : a python package for linear algebra matplotlib : a python package used for plotting the results from __future__ import absolute_import, division, print_function from dolfin import * import math import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False) 2. Define the mesh and the finite element space We construct a triangulation (mesh) \\mathcal{T}_h of the computational domain \\Omega := [0, 1]^2 with n elements in each direction. On the mesh \\mathcal{T}_h , we then define the finite element space V_h \\subset H^1(\\Omega) consisting of globally continuous piecewise polynomials. The degree variable defines the polynomial degree. n = 16 degree = 1 mesh = UnitSquareMesh(n, n) nb.plot(mesh) Vh = FunctionSpace(mesh, 'Lagrange', degree) print( \"dim(Vh) = \", Vh.dim() ) dim(Vh) = 289 3. Define boundary labels To partition the boundary of \\Omega in the subdomains \\Gamma_{\\rm top} , \\Gamma_{\\rm bottom} , \\Gamma_{\\rm left} , \\Gamma_{\\rm right} we assign a unique label boundary_parts to each of part of \\partial \\Omega . class TopBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1] - 1) < DOLFIN_EPS class BottomBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1]) < DOLFIN_EPS class LeftBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0]) < DOLFIN_EPS class RightBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0] - 1) < DOLFIN_EPS boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) 4. Define the coefficients of the PDE and the boundary conditions We first define the coefficients of the PDE using the Constant and Expression classes. Constant is used to define coefficients that do not depend on the space coordinates, Expression is used to define coefficients that are a known function of the space coordinates x[0] (x-axis direction) and x[1] (y-axis direction). In the finite element method community, Dirichlet boundary conditions are also known as essential boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class DirichletBC to indicate this type of condition. On the other hand, Newman boundary conditions are also known as natural boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure ds[i] to integrate over the portion of the boundary marked with label i . u_L = Constant(0.) u_R = Constant(0.) sigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5) sigma_top = Constant(0.) f = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts) 5. Define and solve the variational problem We also define two special types of functions: the TrialFunction u and the TestFunction v . These special types of function are used by FEniCS to generate the finite element vectors and matrices which stem from the weak formulation of the PDE. More specifically, by denoting by \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)} the finite element basis for the space V_h , a function u_h \\in V_h can be written as u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), where {\\rm u}_i represents the coefficients in the finite element expansion of u_h . We then define the bilinear form a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h \\, dx ; the linear form L(v_h) = \\int_\\Omega f v_h \\, dx + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h \\, ds + \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h \\,ds . We can then solve the variational problem Find u_h \\in V_h such that a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h using directly the built-in solve method in FEniCS. NOTE: As an alternative one can also assemble the finite element matrix A and the right hand side b that stems from the discretization of a and L , and then solve the linear system A {\\rm u} = {\\rm b}, where {\\rm u} is the vector collecting the coefficients of the finite element expasion of u_h , the entries of the matrix A are such that A_{ij} = a(\\phi_j, \\phi_i) , the entries of the right hand side b are such that b_i = L(\\phi_i) . u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) #solve(a == L, uh, bcs=bcs) A, b = assemble_system(a,L, bcs=bcs) solve(A, uh.vector(), b, \"cg\") nb.plot(uh) <matplotlib.collections.TriMesh at 0x11e2f0748> 6. Compute the discretization error For this problem, the exact solution is known. We can therefore compute the following norms of the discretization error (i.e. the difference between the finite element solution u_h and the exact solution u_{\\rm ex} ) \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, and \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5) grad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) print (\"|| u_h - u_e ||_L2 = \", err_L2) print (\"|| u_h - u_e ||_H1 = \", err_H1) || u_h - u_e ||_L2 = 0.008805253722075506 || u_h - u_e ||_H1 = 0.39671895251414124 7. Convergence of the finite element method We now verify numerically a well-known convergence result for the finite element method. Let s denote the polynomial degree of the finite element space, and assume that the solution u_{\\rm ex} is at least in H^{s+1}(\\Omega) . Then we have \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. In the code below, the function compute(n, degree) solves the PDE using a mesh with n elements in each direction and finite element spaces of polynomial order degree . The figure below shows the discretization errors in the H^1 and L^2 as a function of the mesh size h ( h = \\frac{1}{n} ) for piecewise linear (P1, s=1 ) and piecewise quadratic (P2, s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular: for piecewise linear finite element P1 we observe first order convergence in the H^1 -norm and second order convergence in the L^2 -norm; for piecewise quadratic finite element P2 we observe second order convergence in the H^1 -norm and third order convergence in the L^2 -norm. def compute(n, degree): mesh = UnitSquareMesh(n, n) Vh = FunctionSpace(mesh, 'Lagrange', degree) boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts) u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) solve(a == L, uh, bcs=bcs) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) return err_L2, err_H1 nref = 5 n = 8*np.power(2,np.arange(0,nref)) h = 1./n err_L2_P1 = np.zeros(nref) err_H1_P1 = np.zeros(nref) err_L2_P2 = np.zeros(nref) err_H1_P2 = np.zeros(nref) for i in range(nref): err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1) err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2) plt.figure(figsize=(15,5)) plt.subplot(121) plt.loglog(h, err_H1_P1, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P1, '-*b', label = \"L2 error\") plt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label = \"First Order\") plt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label = \"Second Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P1 Finite Element\") plt.legend(loc='lower right') plt.subplot(122) plt.loglog(h, err_H1_P2, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P2, '-*b', label = \"L2 error\") plt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label = \"Second Order\") plt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label = \"Third Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P2 Finite Element\") plt.legend(loc='lower right') plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"FEniCS101 Tutorial"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#fenics101-tutorial","text":"In this tutorial we consider the boundary value problem (BVP) \\begin{eqnarray*} - \\nabla \\cdot (k \\nabla u) = f & \\text{ in } \\Omega,\\\\ u = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\cup \\Gamma_{\\rm right},\\\\ k \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\cup \\Gamma_{\\rm bottom}, \\end{eqnarray*} where \\Omega = (0,1) \\times (0,1) , \\Gamma_D and \\Gamma_N are the union of the left and right, and top and bottom boundaries of \\Omega , respectively. Here \\begin{eqnarray*} k(x,y) = 1 & \\text{ on } \\Omega\\\\ f(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\ u_0(x,y) = 0 & \\text{ on } \\Gamma_D, \\\\ \\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right. & \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array} \\end{eqnarray*} The exact solution is u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).","title":"FEniCS101 Tutorial"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#weak-formulation","text":"Let us define the Hilbert spaces V_{u_0}, V_0 \\in \\Omega as V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\}, V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}. To obtain the weak formulation, we multiply the PDE by an arbitrary function v \\in V_0 and integrate over the domain \\Omega leading to -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. Then, integration by parts the non-conforming term gives \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. Finally by recalling that v = 0 on \\Gamma_D and that k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma on \\Gamma_N , we find the weak formulation: Find * u \\in V_{u_0} such that* \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.","title":"Weak formulation"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#1-load-modules","text":"To start we load the following modules: dolfin: the python/C++ interface to FEniCS math : the python module for mathematical functions numpy : a python package for linear algebra matplotlib : a python package used for plotting the results from __future__ import absolute_import, division, print_function from dolfin import * import math import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space","text":"We construct a triangulation (mesh) \\mathcal{T}_h of the computational domain \\Omega := [0, 1]^2 with n elements in each direction. On the mesh \\mathcal{T}_h , we then define the finite element space V_h \\subset H^1(\\Omega) consisting of globally continuous piecewise polynomials. The degree variable defines the polynomial degree. n = 16 degree = 1 mesh = UnitSquareMesh(n, n) nb.plot(mesh) Vh = FunctionSpace(mesh, 'Lagrange', degree) print( \"dim(Vh) = \", Vh.dim() ) dim(Vh) = 289","title":"2. Define the mesh and the finite element space"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#3-define-boundary-labels","text":"To partition the boundary of \\Omega in the subdomains \\Gamma_{\\rm top} , \\Gamma_{\\rm bottom} , \\Gamma_{\\rm left} , \\Gamma_{\\rm right} we assign a unique label boundary_parts to each of part of \\partial \\Omega . class TopBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1] - 1) < DOLFIN_EPS class BottomBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1]) < DOLFIN_EPS class LeftBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0]) < DOLFIN_EPS class RightBoundary(SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0] - 1) < DOLFIN_EPS boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4)","title":"3. Define boundary labels"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions","text":"We first define the coefficients of the PDE using the Constant and Expression classes. Constant is used to define coefficients that do not depend on the space coordinates, Expression is used to define coefficients that are a known function of the space coordinates x[0] (x-axis direction) and x[1] (y-axis direction). In the finite element method community, Dirichlet boundary conditions are also known as essential boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class DirichletBC to indicate this type of condition. On the other hand, Newman boundary conditions are also known as natural boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure ds[i] to integrate over the portion of the boundary marked with label i . u_L = Constant(0.) u_R = Constant(0.) sigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5) sigma_top = Constant(0.) f = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts)","title":"4. Define the coefficients of the PDE and the boundary conditions"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#5-define-and-solve-the-variational-problem","text":"We also define two special types of functions: the TrialFunction u and the TestFunction v . These special types of function are used by FEniCS to generate the finite element vectors and matrices which stem from the weak formulation of the PDE. More specifically, by denoting by \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)} the finite element basis for the space V_h , a function u_h \\in V_h can be written as u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), where {\\rm u}_i represents the coefficients in the finite element expansion of u_h . We then define the bilinear form a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h \\, dx ; the linear form L(v_h) = \\int_\\Omega f v_h \\, dx + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h \\, ds + \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h \\,ds . We can then solve the variational problem Find u_h \\in V_h such that a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h using directly the built-in solve method in FEniCS. NOTE: As an alternative one can also assemble the finite element matrix A and the right hand side b that stems from the discretization of a and L , and then solve the linear system A {\\rm u} = {\\rm b}, where {\\rm u} is the vector collecting the coefficients of the finite element expasion of u_h , the entries of the matrix A are such that A_{ij} = a(\\phi_j, \\phi_i) , the entries of the right hand side b are such that b_i = L(\\phi_i) . u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) #solve(a == L, uh, bcs=bcs) A, b = assemble_system(a,L, bcs=bcs) solve(A, uh.vector(), b, \"cg\") nb.plot(uh) <matplotlib.collections.TriMesh at 0x11e2f0748>","title":"5. Define and solve the variational problem"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#6-compute-the-discretization-error","text":"For this problem, the exact solution is known. We can therefore compute the following norms of the discretization error (i.e. the difference between the finite element solution u_h and the exact solution u_{\\rm ex} ) \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, and \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5) grad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) print (\"|| u_h - u_e ||_L2 = \", err_L2) print (\"|| u_h - u_e ||_H1 = \", err_H1) || u_h - u_e ||_L2 = 0.008805253722075506 || u_h - u_e ||_H1 = 0.39671895251414124","title":"6. Compute the discretization error"},{"location":"tutorials_v2.3.0/1_FEniCS101/1_FEniCS101/#7-convergence-of-the-finite-element-method","text":"We now verify numerically a well-known convergence result for the finite element method. Let s denote the polynomial degree of the finite element space, and assume that the solution u_{\\rm ex} is at least in H^{s+1}(\\Omega) . Then we have \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. In the code below, the function compute(n, degree) solves the PDE using a mesh with n elements in each direction and finite element spaces of polynomial order degree . The figure below shows the discretization errors in the H^1 and L^2 as a function of the mesh size h ( h = \\frac{1}{n} ) for piecewise linear (P1, s=1 ) and piecewise quadratic (P2, s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular: for piecewise linear finite element P1 we observe first order convergence in the H^1 -norm and second order convergence in the L^2 -norm; for piecewise quadratic finite element P2 we observe second order convergence in the H^1 -norm and third order convergence in the L^2 -norm. def compute(n, degree): mesh = UnitSquareMesh(n, n) Vh = FunctionSpace(mesh, 'Lagrange', degree) boundary_parts = FacetFunction(\"size_t\", mesh) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)] ds = Measure(\"ds\", subdomain_data=boundary_parts) u = TrialFunction(Vh) v = TestFunction(Vh) a = inner(nabla_grad(u), nabla_grad(v))*dx L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = Function(Vh) solve(a == L, uh, bcs=bcs) err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) ) err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) ) err_H1 = sqrt( err_L2**2 + err_grad**2) return err_L2, err_H1 nref = 5 n = 8*np.power(2,np.arange(0,nref)) h = 1./n err_L2_P1 = np.zeros(nref) err_H1_P1 = np.zeros(nref) err_L2_P2 = np.zeros(nref) err_H1_P2 = np.zeros(nref) for i in range(nref): err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1) err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2) plt.figure(figsize=(15,5)) plt.subplot(121) plt.loglog(h, err_H1_P1, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P1, '-*b', label = \"L2 error\") plt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label = \"First Order\") plt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label = \"Second Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P1 Finite Element\") plt.legend(loc='lower right') plt.subplot(122) plt.loglog(h, err_H1_P2, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P2, '-*b', label = \"L2 error\") plt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label = \"Second Order\") plt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label = \"Third Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P2 Finite Element\") plt.legend(loc='lower right') plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"7. Convergence of the finite element method"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/","text":"Coefficient field inversion in an elliptic partial differential equation We consider the estimation of a coefficient in an elliptic partial differential equation as a model problem. Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity. It can also be interpreted as finding a membrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n , n\\in\\{1,2,3\\} be an open, bounded domain and consider the following problem: \\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx, where u is the solution of \\begin{split} \\quad -\\nabla\\cdot(\\exp(m)\\nabla u) &= f \\text{ in }\\Omega,\\\\ u &= 0 \\text{ on }\\partial\\Omega. \\end{split} Here m\\in U_{ad}:=\\{m\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\} the unknown coefficient field, u_d denotes (possibly noisy) data, f\\in H^{-1}(\\Omega) a given force, and \\gamma\\ge 0 the regularization parameter. The variational (or weak) form of the state equation: Find u\\in H_0^1(\\Omega) such that (\\exp(m)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), where H_0^1(\\Omega) is the space of functions vanishing on \\partial\\Omega with square integrable derivatives. Here, (\\cdot\\,,\\cdot) denotes the L^2 -inner product, i.e, for scalar functions u,v \\in L^2(\\Omega) we denote (u,v) := \\int_\\Omega u(x) v(x) \\,dx. Gradient evaluation: The Lagrangian functional \\mathscr{L}:H_0^1(\\Omega)\\times H^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} is given by \\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) + \\frac{\\gamma}{2}(\\nabla m, \\nabla m) + (\\exp(m)\\nabla u,\\nabla p) - (f,p). Then the gradient of the cost functional \\mathcal{J}(m) with respect to the parameter m is \\mathcal{G}(m)(\\tilde m) := \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where u \\in H_0^1(\\Omega) is the solution of the forward problem, \\mathscr{L}_p(u,m,p)(\\tilde{p}) := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega), and p \\in H_0^1(\\Omega) is the solution of the adjoint problem, \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega). Hessian action: To evaluate the action \\mathcal{H}(m)(\\hat{m}) of the Hessian in a given direction \\hat{m} , we consider variations of the meta-Lagrangian functional \\begin{aligned} \\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\ {} & \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) & \\text{gradient}\\\\ {} & + (\\exp(m)\\nabla u, \\nabla \\hat{p}) - (f,\\hat{p}) & \\text{forward eq}\\\\ {} & + (\\exp(m)\\nabla p, \\nabla \\hat{u}) + (u-u_d,\\hat{u}) & \\text{adjoint eq}. \\end{aligned} Then the action of the Hessian in a given direction \\hat{m} is \\begin{aligned} (\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\ {} & = (\\tilde{m} \\exp(m) \\nabla \\hat{u}, \\nabla{p}) + \\gamma (\\nabla \\hat{m}, \\nabla \\tilde{m}) + (\\tilde{m} \\hat{m} \\exp(m)\\nabla u, \\nabla p) + (\\tilde{m}\\exp(m) \\nabla u, \\nabla \\hat{p}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), \\end{aligned} where u\\in H^1_0(\\Omega) and p \\in H^1_0(\\Omega) are the solution of the forward and adjoint problem, respectively; \\hat{u} \\in H^1_0(\\Omega) is the solution of the incremental forward problem, \\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{p}) := (\\exp(m) \\nabla \\hat{u}, \\nabla \\tilde{p}) + (\\hat{m} \\exp(m) \\nabla u, \\nabla \\tilde p) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega); and \\hat{p} \\in H^1_0(\\Omega) is the solution of the incremental adjoint problem, \\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{u}) := (\\hat{u}, \\tilde{u}) + (\\hat{m} \\exp(m)\\nabla p, \\nabla \\tilde{u}) + (\\exp(m) \\nabla \\tilde u, \\nabla \\hat{p}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega). Inexact Newton-CG: Written in abstract form, the Newton Method computes an update direction \\hat{m}_k by solving the linear system (\\tilde{m}, \\mathcal{H}(m_k)(\\hat{m}_k) ) = -\\mathcal{G}(m_k)(\\tilde m) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where the evaluation of the gradient \\mathcal{G}(m_k) involve the solution u_k and p_k of the forward and adjoint problem (respectively) for m = m_k . Similarly, the Hessian action \\mathcal{H}(m_k)(\\hat{m}_k) requires to additional solve the incremental forward and adjoint problems. Discrete Newton system: \\def\\tu{\\tilde u} \\def\\tm{\\tilde m} \\def\\tp{\\tilde p} \\def\\hu{\\hat u} \\def\\hp{\\hat p} \\def\\hm{\\hat m} \\def\\bu{{\\bf u}} \\def\\bm{{\\bf m}} \\def\\bp{{\\bf p}} \\def\\btu{{\\bf \\tilde u}} \\def\\btm{{\\bf \\tilde m}} \\def\\btp{{\\bf \\tilde p}} \\def\\bhu{{\\bf \\hat u}} \\def\\bhm{{\\bf \\hat m}} \\def\\bhp{{\\bf \\hat p}} \\def\\bg{{\\bf g}} \\def\\bA{{\\bf A}} \\def\\bC{{\\bf C}} \\def\\bH{{\\bf H}} \\def\\bR{{\\bf R}} \\def\\bW{{\\bf W}} Let us denote the vectors corresponding to the discretization of the functions u_k, m_k, p_k by \\bu_k, \\bm_k, \\bp_k and of the functions \\hu_k, \\hm_k, \\hp_k by \\bhu_k, \\bhm_k,\\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system: \\bH_k \\, \\bhm_k = -\\bg_k. The gradient \\bg_k is computed using the following three steps Given \\bm_k we solve the forward problem \\bA_k \\bu_k = {\\bf f}, where \\bA_k \\bu_k stems from the discretization (\\exp(m_k)\\nabla u_k, \\nabla \\tilde{p}) , and {\\bf f} stands for the discretization of the right hand side f . Given \\bm_k and \\bu_k solve the adjoint problem \\bA_k^T \\bp_k = - \\bW_{\\scriptsize\\mbox{uu}}\\,(\\bu_k-\\bu_d) where \\bA_k^T \\bp_k stems from the discretization of (\\exp(m_k)\\nabla \\tilde{u}, \\nabla p_k) , \\bW_{\\scriptsize\\mbox{uu}} is the mass matrix corresponding to the L^2 inner product in the state space, and \\bu_d stems from the data. Define the gradient \\bg_k = \\bR \\bm_k + \\bC_k^T \\bp_k, where \\bR is the matrix stemming from discretization of the regularization operator \\gamma ( \\nabla \\hat{m}, \\nabla \\tilde{m}) , and \\bC_k stems from discretization of the term (\\tilde{m}\\exp(m_k)\\nabla u_k, \\nabla p_k) . Similarly the action of the Hessian \\bH_k \\, \\bhm_k in a direction \\bhm_k (by using the CG algorithm we only need the action of \\bH_k to solve the Newton step) is given by Solve the incremental forward problem \\bA_k \\bhu_k = -\\bC_k \\bhm_k, where \\bC_k \\bm_k stems from discretization of (\\hat{m} \\exp(m_k) \\nabla u_k, \\nabla \\tilde p) . Solve the incremental adjoint problem \\bA_k^T \\bhp_k = -(\\bW_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k), where \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k stems for the discretization of (\\hat{m}_k \\exp(m_k)\\nabla p_k, \\nabla \\tilde{u}) . Define the Hessian action \\bH_k \\, \\bhm = \\underbrace{(\\bR + \\bW_{\\scriptsize\\mbox{mm}})}_{\\text{Hessian of the regularization}} \\bhm + \\underbrace{(\\bC_k^{T}\\bA_k^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bA_k^{-1} \\bC_k - \\bW_{\\scriptsize\\mbox{um}}) - \\bW_{\\scriptsize\\mbox{mu}} \\bA_k^{-1} \\bC_k)}_{\\text{Hessian of the data misfit}}\\;\\bhm. Goals: By the end of this notebook, you should be able to: solve the forward and adjoint Poisson equations understand the inverse method framework visualise and understand the results modify the problem and code Mathematical tools used: Finite element method Derivation of gradiant and Hessian via the adjoint method Inexact Newton-CG Armijo line search List of software used: FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , a python package used for plotting the results Set up Import dependencies from __future__ import absolute_import, division, print_function from dolfin import * import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging import matplotlib.pyplot as plt %matplotlib inline logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False) Model set up: As in the introduction, the first thing we need to do is set up the numerical model. In this cell, we set the mesh, the finite element functions u, p, g corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization. # create mesh and define function spaces nx = 64 ny = 64 mesh = UnitSquareMesh(nx, ny) Vm = FunctionSpace(mesh, 'Lagrange', 1) Vu = FunctionSpace(mesh, 'Lagrange', 2) # The true and inverted parameter mtrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Vm) m = interpolate(Expression(\"log(2.0)\", degree=1),Vm) # define function for state and adjoint u = Function(Vu) p = Function(Vu) # define Trial and Test Functions u_trial, p_trial, m_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Vm) u_test, p_test, m_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Vm) # initialize input functions f = Constant(\"1.0\") u0 = Constant(\"0.0\") # plot plt.figure(figsize=(15,5)) nb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on') nb.plot(mtrue,subplot_loc=122, mytitle=\"True parameter field\") plt.show() # set up dirichlet boundary conditions def boundary(x,on_boundary): return on_boundary bc_state = DirichletBC(Vu, u0, boundary) bc_adj = DirichletBC(Vu, Constant(0.), boundary) Set up synthetic observations: Propose a coefficient field m_{\\rm true} shown above The weak form of the pde: Find u\\in H_0^1(\\Omega) such that \\underbrace{(\\exp(m_{\\rm true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) . Perturb the solution: u = u + \\eta , where \\eta \\sim \\mathcal{N}(0, \\sigma) # noise level noise_level = 0.05 # weak form for setting up the synthetic observations a_goal = inner(exp(mtrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_goal = f * u_test * dx # solve the forward/state problem to generate synthetic observations goal_A, goal_b = assemble_system(a_goal, L_goal, bc_state) utrue = Function(Vu) solve(goal_A, utrue.vector(), goal_b) ud = Function(Vu) ud.assign(utrue) # perturb state solution and create synthetic measurements ud # ud = u + ||u||/SNR * random.normal MAX = ud.vector().norm(\"linf\") noise = Vector() goal_A.init_vector(noise,1) parRandom.normal(noise_level * MAX, noise) bc_adj.apply(noise) ud.vector().axpy(1., noise) # plot nb.multi1_plot([utrue, ud], [\"State solution with mtrue\", \"Synthetic observations\"]) plt.show() The cost function evaluation: J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text{misfit} } + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text{reg}} In the code below, \\bW and \\bR are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively. # regularization parameter gamma = 1e-8 # weak for for setting up the misfit and regularization compoment of the cost W_equ = inner(u_trial, u_test) * dx R_equ = gamma * inner(nabla_grad(m_trial), nabla_grad(m_test)) * dx W = assemble(W_equ) R = assemble(R_equ) # refine cost function def cost(u, ud, m, W, R): diff = u.vector() - ud.vector() reg = 0.5 * m.vector().inner(R*m.vector() ) misfit = 0.5 * diff.inner(W * diff) return [reg + misfit, misfit, reg] Setting up the state equations, right hand side for the adjoint and the necessary matrices: # weak form for setting up the state equation a_state = inner(exp(m) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_state = f * u_test * dx # weak form for setting up the adjoint equation a_adj = inner(exp(m) * nabla_grad(p_trial), nabla_grad(p_test)) * dx L_adj = -inner(u - ud, p_test) * dx # weak form for setting up matrices Wum_equ = inner(exp(m) * m_trial * nabla_grad(p_test), nabla_grad(p)) * dx C_equ = inner(exp(m) * m_trial * nabla_grad(u), nabla_grad(u_test)) * dx Wmm_equ = inner(exp(m) * m_trial * m_test * nabla_grad(u), nabla_grad(p)) * dx M_equ = inner(m_trial, m_test) * dx # assemble matrix M M = assemble(M_equ) Initial guess We solve the state equation and compute the cost functional for the initial guess of the parameter m_ini # solve state equation state_A, state_b = assemble_system (a_state, L_state, bc_state) solve (state_A, u.vector(), state_b) # evaluate cost [cost_old, misfit_old, reg_old] = cost(u, ud, m, W, R) # plot plt.figure(figsize=(15,5)) nb.plot(m,subplot_loc=121, mytitle=\"m_ini\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max()) nb.plot(u,subplot_loc=122, mytitle=\"u(m_ini)\") plt.show() The reduced Hessian apply to a vector \\bhm : Here we describe how to apply the reduced Hessian operator to a vector \\bhm . For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined. For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm. The Hessian apply reads: \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bhu + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm) & \\text{adjoint}\\\\ \\bH \\bhm &= (\\bR + \\bW_{\\scriptsize\\mbox{mm}})\\bhm + \\bC^T \\bhp + \\bW_{\\scriptsize\\mbox{mu}} \\bhu. \\end{align} The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm , \\bW_{\\scriptsize\\mbox{mm}}\\bf \\bhm , and \\bW_{\\scriptsize\\mbox{mu}} \\bhu : \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bf \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} \\bW_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\ \\bH_{\\rm GN} \\bhm &= \\bR \\bhm + \\bC^T \\bhp. \\end{align} # Class HessianOperator to perform Hessian apply to a vector class HessianOperator(): cgiter = 0 def __init__(self, R, Wmm, C, A, adj_A, W, Wum, gauss_newton_approx=False): self.R = R self.Wmm = Wmm self.C = C self.A = A self.adj_A = adj_A self.W = W self.Wum = Wum self.gauss_newton_approx = gauss_newton_approx # incremental state self.du = Vector() self.A.init_vector(self.du,0) # incremental adjoint self.dp = Vector() self.adj_A.init_vector(self.dp,0) # auxiliary vectors self.CT_dp = Vector() self.C.init_vector(self.CT_dp, 1) self.Wum_du = Vector() self.Wum.init_vector(self.Wum_du, 1) def init_vector(self, v, dim): self.R.init_vector(v,dim) # Hessian performed on v, output as generic vector y def mult(self, v, y): self.cgiter += 1 y.zero() if self.gauss_newton_approx: self.mult_GaussNewton(v,y) else: self.mult_Newton(v,y) # define (Gauss-Newton) Hessian apply H * v def mult_GaussNewton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) # incremental adjoint rhs = - (self.W * self.du) bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1, self.CT_dp) # define (Newton) Hessian apply H * v def mult_Newton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) # incremental adjoint rhs = -(self.W * self.du) - self.Wum * v bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) y.axpy(1., self.Wmm*v) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1., self.CT_dp) self.Wum.transpmult(self.du, self.Wum_du) y.axpy(1., self.Wum_du) The inexact Newton-CG optimization with Armijo line search: We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search. The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ). First, we compute the gradient by solving the state and adjoint equation for the current parameter m , and then substituing the current state u , parameter m and adjoint p variables in the weak form expression of the gradient: (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}\\nabla u, \\nabla p). Then, we compute the Newton direction \\hat m by iteratively solving \\mathcal{H} {\\hat m} = -g . The Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug (to avoid negative curvature) criteria. Finally, the Armijo line search uses backtracking to find \\alpha such that a sufficient reduction in the cost functional is achieved. More specifically, we use backtracking to find \\alpha such that: J( m + \\alpha \\hat m ) \\leq J(m) + \\alpha c_{\\rm armijo} (\\hat m,g). # define parameters for the optimization tol = 1e-8 c = 1e-4 maxiter = 12 plot_on = False # initialize iter counters iter = 1 total_cg_iter = 0 converged = False # initializations g, m_delta = Vector(), Vector() R.init_vector(m_delta,0) R.init_vector(g,0) m_prev = Function(Vm) print (\"Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg\") while iter < maxiter and not converged: # assemble matrix C C = assemble(C_equ) # solve the adoint problem adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj) solve(adjoint_A, p.vector(), adjoint_RHS) # assemble W_ua and R Wum = assemble (Wum_equ) Wmm = assemble (Wmm_equ) # evaluate the gradient CT_p = Vector() C.init_vector(CT_p,1) C.transpmult(p.vector(), CT_p) MG = CT_p + R * m.vector() solve(M, g, MG) # calculate the norm of the gradient grad2 = g.inner(MG) gradnorm = sqrt(grad2) # set the CG tolerance (use Eisenstat\u2013Walker termination criterion) if iter == 1: gradnorm_ini = gradnorm tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini)) # define the Hessian apply operator (with preconditioner) Hess_Apply = HessianOperator(R, Wmm, C, state_A, adjoint_A, W, Wum, gauss_newton_approx=(iter<6) ) P = R + gamma * M Psolver = PETScKrylovSolver(\"cg\", amg_method()) Psolver.set_operator(P) solver = CGSolverSteihaug() solver.set_operator(Hess_Apply) solver.set_preconditioner(Psolver) solver.parameters[\"rel_tolerance\"] = tolcg solver.parameters[\"zero_initial_guess\"] = True solver.parameters[\"print_level\"] = -1 # solve the Newton system H a_delta = - MG solver.solve(m_delta, -MG) total_cg_iter += Hess_Apply.cgiter # linesearch alpha = 1 descent = 0 no_backtrack = 0 m_prev.assign(m) while descent == 0 and no_backtrack < 10: m.vector().axpy(alpha, m_delta ) # solve the state/forward problem state_A, state_b = assemble_system(a_state, L_state, bc_state) solve(state_A, u.vector(), state_b) # evaluate cost [cost_new, misfit_new, reg_new] = cost(u, ud, m, W, R) # check if Armijo conditions are satisfied if cost_new < cost_old + alpha * c * MG.inner(m_delta): cost_old = cost_new descent = 1 else: no_backtrack += 1 alpha *= 0.5 m.assign(m_prev) # reset a # calculate sqrt(-G * D) graddir = sqrt(- MG.inner(m_delta) ) sp = \"\" print( \"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\ (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\ graddir, sp, gradnorm, sp, alpha, sp, tolcg) ) if plot_on: nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False) plt.show() # check for convergence if gradnorm < tol and iter > 1: converged = True print( \"Newton's method converged in \",iter,\" iterations\") print( \"Total number of CG iterations: \", total_cg_iter) iter += 1 if not converged: print( \"Newton's method did not converge in \", maxiter, \" iterations\") Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg 1 1 1.12916e-05 1.12916e-05 1.34131e-11 1.56616e-02 3.79614e-04 1.00 5.000e-01 2 1 7.83203e-07 7.83166e-07 3.68374e-11 4.68686e-03 5.35268e-05 1.00 3.755e-01 3 1 3.12289e-07 3.12240e-07 4.92387e-11 9.73515e-04 7.14567e-06 1.00 1.372e-01 4 6 1.91792e-07 1.61389e-07 3.04037e-08 4.54694e-04 1.00593e-06 1.00 5.148e-02 5 1 1.86420e-07 1.56000e-07 3.04197e-08 1.03668e-04 6.15515e-07 1.00 4.027e-02 6 11 1.80340e-07 1.36887e-07 4.34527e-08 1.12151e-04 2.14951e-07 1.00 2.380e-02 7 5 1.80268e-07 1.38103e-07 4.21646e-08 1.19478e-05 3.96243e-08 1.00 1.022e-02 8 15 1.80266e-07 1.38241e-07 4.20247e-08 1.70777e-06 3.37236e-09 1.00 2.981e-03 Newton's method converged in 8 iterations Total number of CG iterations: 41 nb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"]) nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Coefficient field inversion in an elliptic partial differential equation"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#coefficient-field-inversion-in-an-elliptic-partial-differential-equation","text":"We consider the estimation of a coefficient in an elliptic partial differential equation as a model problem. Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity. It can also be interpreted as finding a membrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n , n\\in\\{1,2,3\\} be an open, bounded domain and consider the following problem: \\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx, where u is the solution of \\begin{split} \\quad -\\nabla\\cdot(\\exp(m)\\nabla u) &= f \\text{ in }\\Omega,\\\\ u &= 0 \\text{ on }\\partial\\Omega. \\end{split} Here m\\in U_{ad}:=\\{m\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\} the unknown coefficient field, u_d denotes (possibly noisy) data, f\\in H^{-1}(\\Omega) a given force, and \\gamma\\ge 0 the regularization parameter.","title":"Coefficient field inversion in an elliptic partial differential equation"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#the-variational-or-weak-form-of-the-state-equation","text":"Find u\\in H_0^1(\\Omega) such that (\\exp(m)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), where H_0^1(\\Omega) is the space of functions vanishing on \\partial\\Omega with square integrable derivatives. Here, (\\cdot\\,,\\cdot) denotes the L^2 -inner product, i.e, for scalar functions u,v \\in L^2(\\Omega) we denote (u,v) := \\int_\\Omega u(x) v(x) \\,dx.","title":"The variational (or weak) form of the state equation:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#gradient-evaluation","text":"The Lagrangian functional \\mathscr{L}:H_0^1(\\Omega)\\times H^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} is given by \\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) + \\frac{\\gamma}{2}(\\nabla m, \\nabla m) + (\\exp(m)\\nabla u,\\nabla p) - (f,p). Then the gradient of the cost functional \\mathcal{J}(m) with respect to the parameter m is \\mathcal{G}(m)(\\tilde m) := \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where u \\in H_0^1(\\Omega) is the solution of the forward problem, \\mathscr{L}_p(u,m,p)(\\tilde{p}) := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega), and p \\in H_0^1(\\Omega) is the solution of the adjoint problem, \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).","title":"Gradient evaluation:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#hessian-action","text":"To evaluate the action \\mathcal{H}(m)(\\hat{m}) of the Hessian in a given direction \\hat{m} , we consider variations of the meta-Lagrangian functional \\begin{aligned} \\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\ {} & \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) & \\text{gradient}\\\\ {} & + (\\exp(m)\\nabla u, \\nabla \\hat{p}) - (f,\\hat{p}) & \\text{forward eq}\\\\ {} & + (\\exp(m)\\nabla p, \\nabla \\hat{u}) + (u-u_d,\\hat{u}) & \\text{adjoint eq}. \\end{aligned} Then the action of the Hessian in a given direction \\hat{m} is \\begin{aligned} (\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\ {} & = (\\tilde{m} \\exp(m) \\nabla \\hat{u}, \\nabla{p}) + \\gamma (\\nabla \\hat{m}, \\nabla \\tilde{m}) + (\\tilde{m} \\hat{m} \\exp(m)\\nabla u, \\nabla p) + (\\tilde{m}\\exp(m) \\nabla u, \\nabla \\hat{p}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), \\end{aligned} where u\\in H^1_0(\\Omega) and p \\in H^1_0(\\Omega) are the solution of the forward and adjoint problem, respectively; \\hat{u} \\in H^1_0(\\Omega) is the solution of the incremental forward problem, \\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{p}) := (\\exp(m) \\nabla \\hat{u}, \\nabla \\tilde{p}) + (\\hat{m} \\exp(m) \\nabla u, \\nabla \\tilde p) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega); and \\hat{p} \\in H^1_0(\\Omega) is the solution of the incremental adjoint problem, \\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{u}) := (\\hat{u}, \\tilde{u}) + (\\hat{m} \\exp(m)\\nabla p, \\nabla \\tilde{u}) + (\\exp(m) \\nabla \\tilde u, \\nabla \\hat{p}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).","title":"Hessian action:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#inexact-newton-cg","text":"Written in abstract form, the Newton Method computes an update direction \\hat{m}_k by solving the linear system (\\tilde{m}, \\mathcal{H}(m_k)(\\hat{m}_k) ) = -\\mathcal{G}(m_k)(\\tilde m) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where the evaluation of the gradient \\mathcal{G}(m_k) involve the solution u_k and p_k of the forward and adjoint problem (respectively) for m = m_k . Similarly, the Hessian action \\mathcal{H}(m_k)(\\hat{m}_k) requires to additional solve the incremental forward and adjoint problems.","title":"Inexact Newton-CG:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#discrete-newton-system","text":"\\def\\tu{\\tilde u} \\def\\tm{\\tilde m} \\def\\tp{\\tilde p} \\def\\hu{\\hat u} \\def\\hp{\\hat p} \\def\\hm{\\hat m} \\def\\bu{{\\bf u}} \\def\\bm{{\\bf m}} \\def\\bp{{\\bf p}} \\def\\btu{{\\bf \\tilde u}} \\def\\btm{{\\bf \\tilde m}} \\def\\btp{{\\bf \\tilde p}} \\def\\bhu{{\\bf \\hat u}} \\def\\bhm{{\\bf \\hat m}} \\def\\bhp{{\\bf \\hat p}} \\def\\bg{{\\bf g}} \\def\\bA{{\\bf A}} \\def\\bC{{\\bf C}} \\def\\bH{{\\bf H}} \\def\\bR{{\\bf R}} \\def\\bW{{\\bf W}} Let us denote the vectors corresponding to the discretization of the functions u_k, m_k, p_k by \\bu_k, \\bm_k, \\bp_k and of the functions \\hu_k, \\hm_k, \\hp_k by \\bhu_k, \\bhm_k,\\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system: \\bH_k \\, \\bhm_k = -\\bg_k. The gradient \\bg_k is computed using the following three steps Given \\bm_k we solve the forward problem \\bA_k \\bu_k = {\\bf f}, where \\bA_k \\bu_k stems from the discretization (\\exp(m_k)\\nabla u_k, \\nabla \\tilde{p}) , and {\\bf f} stands for the discretization of the right hand side f . Given \\bm_k and \\bu_k solve the adjoint problem \\bA_k^T \\bp_k = - \\bW_{\\scriptsize\\mbox{uu}}\\,(\\bu_k-\\bu_d) where \\bA_k^T \\bp_k stems from the discretization of (\\exp(m_k)\\nabla \\tilde{u}, \\nabla p_k) , \\bW_{\\scriptsize\\mbox{uu}} is the mass matrix corresponding to the L^2 inner product in the state space, and \\bu_d stems from the data. Define the gradient \\bg_k = \\bR \\bm_k + \\bC_k^T \\bp_k, where \\bR is the matrix stemming from discretization of the regularization operator \\gamma ( \\nabla \\hat{m}, \\nabla \\tilde{m}) , and \\bC_k stems from discretization of the term (\\tilde{m}\\exp(m_k)\\nabla u_k, \\nabla p_k) . Similarly the action of the Hessian \\bH_k \\, \\bhm_k in a direction \\bhm_k (by using the CG algorithm we only need the action of \\bH_k to solve the Newton step) is given by Solve the incremental forward problem \\bA_k \\bhu_k = -\\bC_k \\bhm_k, where \\bC_k \\bm_k stems from discretization of (\\hat{m} \\exp(m_k) \\nabla u_k, \\nabla \\tilde p) . Solve the incremental adjoint problem \\bA_k^T \\bhp_k = -(\\bW_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k), where \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k stems for the discretization of (\\hat{m}_k \\exp(m_k)\\nabla p_k, \\nabla \\tilde{u}) . Define the Hessian action \\bH_k \\, \\bhm = \\underbrace{(\\bR + \\bW_{\\scriptsize\\mbox{mm}})}_{\\text{Hessian of the regularization}} \\bhm + \\underbrace{(\\bC_k^{T}\\bA_k^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bA_k^{-1} \\bC_k - \\bW_{\\scriptsize\\mbox{um}}) - \\bW_{\\scriptsize\\mbox{mu}} \\bA_k^{-1} \\bC_k)}_{\\text{Hessian of the data misfit}}\\;\\bhm.","title":"Discrete Newton system:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#goals","text":"By the end of this notebook, you should be able to: solve the forward and adjoint Poisson equations understand the inverse method framework visualise and understand the results modify the problem and code","title":"Goals:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#mathematical-tools-used","text":"Finite element method Derivation of gradiant and Hessian via the adjoint method Inexact Newton-CG Armijo line search","title":"Mathematical tools used:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#list-of-software-used","text":"FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , a python package used for plotting the results","title":"List of software used:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#set-up","text":"","title":"Set up"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#import-dependencies","text":"from __future__ import absolute_import, division, print_function from dolfin import * import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging import matplotlib.pyplot as plt %matplotlib inline logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) set_log_active(False)","title":"Import dependencies"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#model-set-up","text":"As in the introduction, the first thing we need to do is set up the numerical model. In this cell, we set the mesh, the finite element functions u, p, g corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization. # create mesh and define function spaces nx = 64 ny = 64 mesh = UnitSquareMesh(nx, ny) Vm = FunctionSpace(mesh, 'Lagrange', 1) Vu = FunctionSpace(mesh, 'Lagrange', 2) # The true and inverted parameter mtrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Vm) m = interpolate(Expression(\"log(2.0)\", degree=1),Vm) # define function for state and adjoint u = Function(Vu) p = Function(Vu) # define Trial and Test Functions u_trial, p_trial, m_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Vm) u_test, p_test, m_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Vm) # initialize input functions f = Constant(\"1.0\") u0 = Constant(\"0.0\") # plot plt.figure(figsize=(15,5)) nb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on') nb.plot(mtrue,subplot_loc=122, mytitle=\"True parameter field\") plt.show() # set up dirichlet boundary conditions def boundary(x,on_boundary): return on_boundary bc_state = DirichletBC(Vu, u0, boundary) bc_adj = DirichletBC(Vu, Constant(0.), boundary)","title":"Model set up:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#set-up-synthetic-observations","text":"Propose a coefficient field m_{\\rm true} shown above The weak form of the pde: Find u\\in H_0^1(\\Omega) such that \\underbrace{(\\exp(m_{\\rm true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) . Perturb the solution: u = u + \\eta , where \\eta \\sim \\mathcal{N}(0, \\sigma) # noise level noise_level = 0.05 # weak form for setting up the synthetic observations a_goal = inner(exp(mtrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_goal = f * u_test * dx # solve the forward/state problem to generate synthetic observations goal_A, goal_b = assemble_system(a_goal, L_goal, bc_state) utrue = Function(Vu) solve(goal_A, utrue.vector(), goal_b) ud = Function(Vu) ud.assign(utrue) # perturb state solution and create synthetic measurements ud # ud = u + ||u||/SNR * random.normal MAX = ud.vector().norm(\"linf\") noise = Vector() goal_A.init_vector(noise,1) parRandom.normal(noise_level * MAX, noise) bc_adj.apply(noise) ud.vector().axpy(1., noise) # plot nb.multi1_plot([utrue, ud], [\"State solution with mtrue\", \"Synthetic observations\"]) plt.show()","title":"Set up synthetic observations:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#the-cost-function-evaluation","text":"J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text{misfit} } + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text{reg}} In the code below, \\bW and \\bR are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively. # regularization parameter gamma = 1e-8 # weak for for setting up the misfit and regularization compoment of the cost W_equ = inner(u_trial, u_test) * dx R_equ = gamma * inner(nabla_grad(m_trial), nabla_grad(m_test)) * dx W = assemble(W_equ) R = assemble(R_equ) # refine cost function def cost(u, ud, m, W, R): diff = u.vector() - ud.vector() reg = 0.5 * m.vector().inner(R*m.vector() ) misfit = 0.5 * diff.inner(W * diff) return [reg + misfit, misfit, reg]","title":"The cost function evaluation:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#setting-up-the-state-equations-right-hand-side-for-the-adjoint-and-the-necessary-matrices","text":"# weak form for setting up the state equation a_state = inner(exp(m) * nabla_grad(u_trial), nabla_grad(u_test)) * dx L_state = f * u_test * dx # weak form for setting up the adjoint equation a_adj = inner(exp(m) * nabla_grad(p_trial), nabla_grad(p_test)) * dx L_adj = -inner(u - ud, p_test) * dx # weak form for setting up matrices Wum_equ = inner(exp(m) * m_trial * nabla_grad(p_test), nabla_grad(p)) * dx C_equ = inner(exp(m) * m_trial * nabla_grad(u), nabla_grad(u_test)) * dx Wmm_equ = inner(exp(m) * m_trial * m_test * nabla_grad(u), nabla_grad(p)) * dx M_equ = inner(m_trial, m_test) * dx # assemble matrix M M = assemble(M_equ)","title":"Setting up the state equations, right hand side for the adjoint and the necessary matrices:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#initial-guess","text":"We solve the state equation and compute the cost functional for the initial guess of the parameter m_ini # solve state equation state_A, state_b = assemble_system (a_state, L_state, bc_state) solve (state_A, u.vector(), state_b) # evaluate cost [cost_old, misfit_old, reg_old] = cost(u, ud, m, W, R) # plot plt.figure(figsize=(15,5)) nb.plot(m,subplot_loc=121, mytitle=\"m_ini\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max()) nb.plot(u,subplot_loc=122, mytitle=\"u(m_ini)\") plt.show()","title":"Initial guess"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#the-reduced-hessian-apply-to-a-vector-bhm","text":"Here we describe how to apply the reduced Hessian operator to a vector \\bhm . For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined. For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm. The Hessian apply reads: \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bhu + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm) & \\text{adjoint}\\\\ \\bH \\bhm &= (\\bR + \\bW_{\\scriptsize\\mbox{mm}})\\bhm + \\bC^T \\bhp + \\bW_{\\scriptsize\\mbox{mu}} \\bhu. \\end{align} The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm , \\bW_{\\scriptsize\\mbox{mm}}\\bf \\bhm , and \\bW_{\\scriptsize\\mbox{mu}} \\bhu : \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bf \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} \\bW_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\ \\bH_{\\rm GN} \\bhm &= \\bR \\bhm + \\bC^T \\bhp. \\end{align} # Class HessianOperator to perform Hessian apply to a vector class HessianOperator(): cgiter = 0 def __init__(self, R, Wmm, C, A, adj_A, W, Wum, gauss_newton_approx=False): self.R = R self.Wmm = Wmm self.C = C self.A = A self.adj_A = adj_A self.W = W self.Wum = Wum self.gauss_newton_approx = gauss_newton_approx # incremental state self.du = Vector() self.A.init_vector(self.du,0) # incremental adjoint self.dp = Vector() self.adj_A.init_vector(self.dp,0) # auxiliary vectors self.CT_dp = Vector() self.C.init_vector(self.CT_dp, 1) self.Wum_du = Vector() self.Wum.init_vector(self.Wum_du, 1) def init_vector(self, v, dim): self.R.init_vector(v,dim) # Hessian performed on v, output as generic vector y def mult(self, v, y): self.cgiter += 1 y.zero() if self.gauss_newton_approx: self.mult_GaussNewton(v,y) else: self.mult_Newton(v,y) # define (Gauss-Newton) Hessian apply H * v def mult_GaussNewton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) # incremental adjoint rhs = - (self.W * self.du) bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1, self.CT_dp) # define (Newton) Hessian apply H * v def mult_Newton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) solve (self.A, self.du, rhs) # incremental adjoint rhs = -(self.W * self.du) - self.Wum * v bc_adj.apply(rhs) solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) y.axpy(1., self.Wmm*v) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1., self.CT_dp) self.Wum.transpmult(self.du, self.Wum_du) y.axpy(1., self.Wum_du)","title":"The reduced Hessian apply to a vector \\bhm:"},{"location":"tutorials_v2.3.0/2_PoissonDeterministic/2_PoissonDeterministic/#the-inexact-newton-cg-optimization-with-armijo-line-search","text":"We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search. The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ). First, we compute the gradient by solving the state and adjoint equation for the current parameter m , and then substituing the current state u , parameter m and adjoint p variables in the weak form expression of the gradient: (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}\\nabla u, \\nabla p). Then, we compute the Newton direction \\hat m by iteratively solving \\mathcal{H} {\\hat m} = -g . The Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug (to avoid negative curvature) criteria. Finally, the Armijo line search uses backtracking to find \\alpha such that a sufficient reduction in the cost functional is achieved. More specifically, we use backtracking to find \\alpha such that: J( m + \\alpha \\hat m ) \\leq J(m) + \\alpha c_{\\rm armijo} (\\hat m,g). # define parameters for the optimization tol = 1e-8 c = 1e-4 maxiter = 12 plot_on = False # initialize iter counters iter = 1 total_cg_iter = 0 converged = False # initializations g, m_delta = Vector(), Vector() R.init_vector(m_delta,0) R.init_vector(g,0) m_prev = Function(Vm) print (\"Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg\") while iter < maxiter and not converged: # assemble matrix C C = assemble(C_equ) # solve the adoint problem adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj) solve(adjoint_A, p.vector(), adjoint_RHS) # assemble W_ua and R Wum = assemble (Wum_equ) Wmm = assemble (Wmm_equ) # evaluate the gradient CT_p = Vector() C.init_vector(CT_p,1) C.transpmult(p.vector(), CT_p) MG = CT_p + R * m.vector() solve(M, g, MG) # calculate the norm of the gradient grad2 = g.inner(MG) gradnorm = sqrt(grad2) # set the CG tolerance (use Eisenstat\u2013Walker termination criterion) if iter == 1: gradnorm_ini = gradnorm tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini)) # define the Hessian apply operator (with preconditioner) Hess_Apply = HessianOperator(R, Wmm, C, state_A, adjoint_A, W, Wum, gauss_newton_approx=(iter<6) ) P = R + gamma * M Psolver = PETScKrylovSolver(\"cg\", amg_method()) Psolver.set_operator(P) solver = CGSolverSteihaug() solver.set_operator(Hess_Apply) solver.set_preconditioner(Psolver) solver.parameters[\"rel_tolerance\"] = tolcg solver.parameters[\"zero_initial_guess\"] = True solver.parameters[\"print_level\"] = -1 # solve the Newton system H a_delta = - MG solver.solve(m_delta, -MG) total_cg_iter += Hess_Apply.cgiter # linesearch alpha = 1 descent = 0 no_backtrack = 0 m_prev.assign(m) while descent == 0 and no_backtrack < 10: m.vector().axpy(alpha, m_delta ) # solve the state/forward problem state_A, state_b = assemble_system(a_state, L_state, bc_state) solve(state_A, u.vector(), state_b) # evaluate cost [cost_new, misfit_new, reg_new] = cost(u, ud, m, W, R) # check if Armijo conditions are satisfied if cost_new < cost_old + alpha * c * MG.inner(m_delta): cost_old = cost_new descent = 1 else: no_backtrack += 1 alpha *= 0.5 m.assign(m_prev) # reset a # calculate sqrt(-G * D) graddir = sqrt(- MG.inner(m_delta) ) sp = \"\" print( \"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\ (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\ graddir, sp, gradnorm, sp, alpha, sp, tolcg) ) if plot_on: nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False) plt.show() # check for convergence if gradnorm < tol and iter > 1: converged = True print( \"Newton's method converged in \",iter,\" iterations\") print( \"Total number of CG iterations: \", total_cg_iter) iter += 1 if not converged: print( \"Newton's method did not converge in \", maxiter, \" iterations\") Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg 1 1 1.12916e-05 1.12916e-05 1.34131e-11 1.56616e-02 3.79614e-04 1.00 5.000e-01 2 1 7.83203e-07 7.83166e-07 3.68374e-11 4.68686e-03 5.35268e-05 1.00 3.755e-01 3 1 3.12289e-07 3.12240e-07 4.92387e-11 9.73515e-04 7.14567e-06 1.00 1.372e-01 4 6 1.91792e-07 1.61389e-07 3.04037e-08 4.54694e-04 1.00593e-06 1.00 5.148e-02 5 1 1.86420e-07 1.56000e-07 3.04197e-08 1.03668e-04 6.15515e-07 1.00 4.027e-02 6 11 1.80340e-07 1.36887e-07 4.34527e-08 1.12151e-04 2.14951e-07 1.00 2.380e-02 7 5 1.80268e-07 1.38103e-07 4.21646e-08 1.19478e-05 3.96243e-08 1.00 1.022e-02 8 15 1.80266e-07 1.38241e-07 4.20247e-08 1.70777e-06 3.37236e-09 1.00 2.981e-03 Newton's method converged in 8 iterations Total number of CG iterations: 41 nb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"]) nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"The inexact Newton-CG optimization with Armijo line search:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/","text":"\\def\\data{ {\\bf d}_\\rm{obs}} \\def\\vec{\\bf} \\def\\m{ {\\bf m}} \\def\\map{{\\bf m}_{\\text{MAP}}} \\def\\postcov{{\\bf \\Gamma}_{\\text{post}}} \\def\\prcov{{\\bf \\Gamma}_{\\text{prior}}} \\def\\matrix{\\bf} \\def\\Hmisfit{{\\bf H}_{\\text{misfit}}} \\def\\HT{{\\tilde{\\bf H}}_{\\text{misfit}}} \\def\\diag{\\operatorname{diag}} \\def\\Vr{{\\matrix V}_r} \\def\\Wr{{\\matrix W}_r} \\def\\Ir{{\\matrix I}_r} \\def\\Dr{{\\matrix D}_r} \\def\\H{{\\matrix H} } Bayesian quantification of parameter uncertainty: Estimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by an elliptic PDE via the Bayesian inference framework. Hence, we state the inverse problem as a problem of statistical inference over the space of uncertain parameters, which are to be inferred from data and a physical model. The resulting solution to the statistical inverse problem is a posterior distribution that assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true'' parameter field that gave rise to the observed data. For simplicity, in what follows we give finite-dimensional expressions (i.e., after discretization of the parameter space) for the Bayesian formulation of the inverse problem. Bayes' Theorem: The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m) over the parameter space, which encodes any knowledge or assumptions about the parameter space that we may wish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\data \\; | \\; \\m) , which explicitly represents the probability that a given set of parameters \\m might give rise to the observed data \\data \\in \\mathbb{R}^m , namely: \\begin{align} \\pi_{\\text{post}}(\\m | \\data) \\propto \\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m). \\end{align} Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions. Gaussian prior and noise: The prior: We consider a Gaussian prior with mean {\\vec m}_{\\text{prior}} and covariance \\prcov . The covariance is given by the discretization of the inverse of differential operator \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where \\gamma , \\delta > 0 control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem. The likelihood: \\data = {\\bf f}(\\m) + {\\bf e }, \\;\\;\\; {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} ) \\pi_{\\text{like}}(\\data \\; | \\; \\m) = \\exp \\left( - \\tfrac{1}{2} ({\\bf f}(\\m) - \\data)^T {\\bf \\Gamma}_{\\text{noise}}^{-1} ({\\bf f}(\\m) - \\data)\\right) Here {\\bf f} is the parameter-to-observable map that takes a parameter vector \\m and maps it to the space observation vector \\data . The posterior: \\pi_{\\text{post}}(\\m \\; | \\; \\data) \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(\\m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\prcov^{-1}} \\right) The Gaussian approximation of the posterior: \\mathcal{N}({\\vec \\map},\\bf \\postcov) The mean of this posterior distribution, {\\vec \\map} , is the parameter vector maximizing the posterior, and is known as the maximum a posteriori (MAP) point. It can be found by minimizing the negative log of the posterior, which amounts to solving a deterministic inverse problem with appropriately weighted norms, \\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\; \\Big( \\frac{1}{2} \\| {\\bf f}(\\m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} +\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\prcov^{-1}} \\Big). The posterior covariance matrix is then given by the inverse of the Hessian matrix of \\mathcal{J} at \\map , namely \\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1} The generalized eigenvalue problem: \\Hmisfit {\\matrix V} = \\prcov^{-1} {\\matrix V} {\\matrix \\Lambda}, where {\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} contains the generalized eigenvalues and the columns of {\\matrix V}\\in \\mathbb R^{n\\times n} the generalized eigenvectors such that {\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I} . Randomized eigensolvers to construct the approximate spectral decomposition: When the generalized eigenvalues \\{\\lambda_i\\} decay rapidly, we can extract a low-rank approximation of \\Hmisfit by retaining only the r largest eigenvalues and corresponding eigenvectors, \\Hmisfit = \\prcov^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\prcov^{-1}, Here, \\Vr \\in \\mathbb{R}^{n\\times r} contains only the r generalized eigenvectors of \\Hmisfit that correspond to the r largest eigenvalues, which are assembled into the diagonal matrix {\\matrix{\\Lambda}}_r = \\diag (\\lambda_i) \\in \\mathbb{R}^{r \\times r} . The approximate posterior covariance: Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \\begin{align} \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1} = \\prcov-\\Vr {\\matrix{D}}_r \\Vr^T + \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i + 1}\\right), \\end{align} where {\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in \\mathbb{R}^{r\\times r} . The last term in this expression captures the error due to truncation in terms of the discarded eigenvalues; this provides a criterion for truncating the spectrum, namely that r is chosen such that \\lambda_r is small relative to 1. Therefore we can approximate the posterior covariance as \\postcov \\approx \\prcov - \\Vr {\\matrix{D}}_r \\Vr^T Drawing samples from a Gaussian distribution with covariance \\H^{-1} Let {\\bf x} be a sample for the prior distribution, i.e. {\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1}) as {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r + \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1} + {\\bf I} \\big\\} {\\bf x} This tutorial shows: Description of the inverse problem (the forward problem, the prior, and the misfit functional) Convergence of the inexact Newton-CG algorithm Low-rank-based approximation of the posterior covariance (built on a low-rank approximation of the Hessian of the data misfit) How to construct the low-rank approximation of the Hessian of the data misfit How to apply the inverse and square-root inverse Hessian to a vector efficiently Samples from the Gaussian approximation of the posterior Goals: By the end of this notebook, you should be able to: Understand the Bayesian inverse framework Visualise and understand the results Modify the problem and code Mathematical tools used: Finite element method Derivation of gradient and Hessian via the adjoint method inexact Newton-CG Armijo line search Bayes' formula randomized eigensolvers List of software used: FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , A great python package that I used for plotting many of the results Numpy , A python package for linear algebra. While extensive, this is mostly used to compute means and sums in this notebook. 1. Load modules from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(seed=1) 2. Generate the true parameter This function generates a random field with a prescribed anysotropic covariance function. def true_model(prior): noise = dl.Vector() prior.init_vector(noise,\"noise\") parRandom.normal(1., noise) mtrue = dl.Vector() prior.init_vector(mtrue, 0) prior.sample(noise,mtrue) return mtrue 3. Set up the mesh and finite element spaces We compute a two dimensional mesh of a unit square with nx by ny elements. We define a P2 finite element space for the state and adjoint variable and P1 for the parameter . ndim = 2 nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh2, Vh1, Vh2] print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641 4. Set up the forward problem To set up the forward problem we use the PDEVariationalProblem class, which requires the following inputs - the finite element spaces for the state, parameter, and adjoint variables Vh - the pde in weak form pde_varf - the boundary conditions bc for the forward problem and bc0 for the adjoint and incremental problems. The PDEVariationalProblem class offer the following functionality: - solving the forward/adjoint and incremental problems - evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables. def u_boundary(x, on_boundary): return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS) u_bdr = dl.Expression(\"x[1]\", degree=1) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) f = dl.Constant(0.0) def pde_varf(u,m,p): return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) 4. Set up the prior To obtain the synthetic true paramter m_{\\rm true} we generate a realization from the prior distribution. Here we assume a Gaussian prior with zero average and covariance matrix \\mathcal{C} = \\mathcal{A}^{-2} . The action of \\mathcal{A} on a field m is given by \\mathcal{A}m = \\left\\{ \\begin{array}{rl} \\gamma \\nabla \\cdot \\left( \\Theta\\nabla m\\right)+ \\delta m & \\text{in } \\Omega\\\\ \\left( \\Theta\\, \\nabla m\\right) \\cdot \\boldsymbol{n} + \\beta m & \\text{on } \\partial\\Omega, \\end{array} \\right. where \\beta \\propto \\sqrt{\\gamma\\delta} is chosen to minimize boundary artifacts. Here \\Theta is an s.p.d. anisotropic tensor of the form \\Theta = \\begin{bmatrix} \\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\ (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2 \\end{bmatrix}. gamma = .1 delta = .5 anis_diff = dl.Expression(code_AnisTensor2D, degree=1) anis_diff.theta0 = 2. anis_diff.theta1 = .5 anis_diff.alpha = math.pi/4 prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True) mtrue = true_model(prior) print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2)) objs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)] mytitles = [\"True Parameter\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() model = Model(pde,prior, misfit) Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2 5. Set up the misfit functional and generate synthetic observations To setup the observation operator \\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t} , we generate n_t ( ntargets in the code below) random locations where to evaluate the value of the state. Under the assumption of Gaussian additive noise, the likelihood function \\pi_{\\rm like} has the form \\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right), where u(m) denotes the solution of the forward model at a given parameter m . The class PointwiseStateObservation implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state u and parameter m . To generate the synthetic observation, we first solve the forward problem using the true parameter m_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise. rel_noise is the signal to noise ratio. ntargets = 50 rel_noise = 0.01 #Targets only on the bottom targets_x = np.random.uniform(0.1,0.9, [ntargets] ) targets_y = np.random.uniform(0.1,0.5, [ntargets] ) targets = np.zeros([ntargets, ndim]) targets[:,0] = targets_x targets[:,1] = targets_y #targets everywhere #targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) print( \"Number of observation points: {0}\".format(ntargets) ) misfit = PointwiseStateObservation(Vh[STATE], targets) utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev vmax = max( utrue.max(), misfit.d.max() ) vmin = min( utrue.min(), misfit.d.min() ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax) nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax) plt.show() Number of observation points: 50 6. Set up the model and test gradient and Hessian The model is defined by three component: - the PDEVariationalProblem pde which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems. - the Prior prior which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator) - the Misfit misfit which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables. To test gradient and the Hessian of the model we use forward finite differences. model = Model(pde, prior, misfit) m0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER]) _ = modelVerify(model, m0.vector(), 1e-12) (yy, H xx) - (xx, H yy) = -2.8560765074414784e-14 7. Compute the MAP point We used the globalized Newtown-CG method to compute the MAP point. m = prior.mean.copy() solver = ReducedSpaceNewtonCG(model) solver.parameters[\"rel_tolerance\"] = 1e-6 solver.parameters[\"abs_tolerance\"] = 1e-12 solver.parameters[\"max_iter\"] = 25 solver.parameters[\"inner_rel_tolerance\"] = 1e-15 solver.parameters[\"GN_iter\"] = 5 solver.parameters[\"globalization\"] = \"LS\" solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4 x = solver.solve([None, m, None]) if solver.converged: print( \"\\nConverged in \", solver.it, \" iterations.\") else: print( \"\\nNot Converged\") print( \"Termination reason: \", solver.termination_reasons[solver.reason] ) print( \"Final gradient norm: \", solver.final_grad_norm ) print( \"Final cost: \", solver.final_cost ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\") nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\") plt.show() It cg_it cost misfit reg (g,dm) ||g||L2 alpha tolcg 1 2 5.676752e+02 5.665943e+02 1.080947e+00 -1.487693e+04 4.008143e+04 1.000000e+00 5.000000e-01 2 2 1.315109e+02 1.292859e+02 2.225043e+00 -8.730801e+02 7.040800e+03 1.000000e+00 4.191210e-01 3 4 5.827697e+01 5.504162e+01 3.235343e+00 -1.565905e+02 2.115036e+03 1.000000e+00 2.297139e-01 4 1 5.257433e+01 4.934371e+01 3.230622e+00 -1.142190e+01 1.827983e+03 1.000000e+00 2.135573e-01 5 6 3.958923e+01 3.507197e+01 4.517257e+00 -3.227624e+01 9.179935e+02 1.000000e+00 1.513381e-01 6 2 3.681315e+01 3.218885e+01 4.624304e+00 -5.501178e+00 7.541736e+02 1.000000e+00 1.371716e-01 7 11 3.276506e+01 2.530117e+01 7.463891e+00 -9.109597e+00 4.788040e+02 1.000000e+00 1.092968e-01 8 1 3.225207e+01 2.478982e+01 7.462243e+00 -1.027328e+00 4.890796e+02 1.000000e+00 1.104634e-01 9 10 3.216228e+01 2.445226e+01 7.710018e+00 -1.799343e-01 7.389810e+01 1.000000e+00 4.293832e-02 10 12 3.216003e+01 2.438903e+01 7.770998e+00 -4.481802e-03 1.291573e+01 1.000000e+00 1.795097e-02 11 18 3.215993e+01 2.438404e+01 7.775888e+00 -2.010595e-04 2.284805e+00 1.000000e+00 7.550105e-03 12 19 3.215993e+01 2.438428e+01 7.775646e+00 -8.282645e-08 6.202767e-02 1.000000e+00 1.244002e-03 Converged in 12 iterations. Termination reason: Norm of the gradient less than tolerance Final gradient norm: 0.0006914733735650482 Final cost: 32.15992905025892 8. Compute the low rank Gaussian approximation of the posterior We used the double pass algorithm to compute a low-rank decomposition of the Hessian Misfit. In particular, we solve \\Hmisfit {\\bf v}_i = \\lambda_i \\prcov^{-1} {\\bf v}_i. The Figure shows the largest k generalized eigenvectors of the Hessian misfit. The effective rank of the Hessian misfit is the number of eigenvalues above the red line ( y=1 ). The effective rank is independent of the mesh size. model.setPointForHessianEvaluations(x, gauss_newton_approx=False) Hmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], misfit_only=True) k = 50 p = 20 print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior(prior, lmbda, V) posterior.mean = x[PARAMETER] plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15]) Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20. 9. Prior and posterior pointwise variance fields compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print( \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200) objs = [dl.Function(Vh[PARAMETER], pr_pw_variance), dl.Function(Vh[PARAMETER], post_pw_variance)] mytitles = [\"Prior variance\", \"Posterior variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 1.014727e+00; Prior trace 1.797376e+00; Correction trace 7.826486e-01 10. Generate samples from Prior and Posterior nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\") s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"3 SubsurfaceBayesian"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#bayesian-quantification-of-parameter-uncertainty","text":"","title":"Bayesian quantification of parameter uncertainty:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#estimating-the-gaussian-approximation-of-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde","text":"In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by an elliptic PDE via the Bayesian inference framework. Hence, we state the inverse problem as a problem of statistical inference over the space of uncertain parameters, which are to be inferred from data and a physical model. The resulting solution to the statistical inverse problem is a posterior distribution that assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true'' parameter field that gave rise to the observed data. For simplicity, in what follows we give finite-dimensional expressions (i.e., after discretization of the parameter space) for the Bayesian formulation of the inverse problem.","title":"Estimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#bayes-theorem","text":"The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m) over the parameter space, which encodes any knowledge or assumptions about the parameter space that we may wish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\data \\; | \\; \\m) , which explicitly represents the probability that a given set of parameters \\m might give rise to the observed data \\data \\in \\mathbb{R}^m , namely: \\begin{align} \\pi_{\\text{post}}(\\m | \\data) \\propto \\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m). \\end{align} Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.","title":"Bayes' Theorem:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#gaussian-prior-and-noise","text":"","title":"Gaussian prior and noise:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#the-prior","text":"We consider a Gaussian prior with mean {\\vec m}_{\\text{prior}} and covariance \\prcov . The covariance is given by the discretization of the inverse of differential operator \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where \\gamma , \\delta > 0 control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem.","title":"The prior:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#the-likelihood","text":"\\data = {\\bf f}(\\m) + {\\bf e }, \\;\\;\\; {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} ) \\pi_{\\text{like}}(\\data \\; | \\; \\m) = \\exp \\left( - \\tfrac{1}{2} ({\\bf f}(\\m) - \\data)^T {\\bf \\Gamma}_{\\text{noise}}^{-1} ({\\bf f}(\\m) - \\data)\\right) Here {\\bf f} is the parameter-to-observable map that takes a parameter vector \\m and maps it to the space observation vector \\data .","title":"The likelihood:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#the-posterior","text":"\\pi_{\\text{post}}(\\m \\; | \\; \\data) \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(\\m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\prcov^{-1}} \\right)","title":"The posterior:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#the-gaussian-approximation-of-the-posterior-mathcalnvec-mapbf-postcov","text":"The mean of this posterior distribution, {\\vec \\map} , is the parameter vector maximizing the posterior, and is known as the maximum a posteriori (MAP) point. It can be found by minimizing the negative log of the posterior, which amounts to solving a deterministic inverse problem with appropriately weighted norms, \\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\; \\Big( \\frac{1}{2} \\| {\\bf f}(\\m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} +\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\prcov^{-1}} \\Big). The posterior covariance matrix is then given by the inverse of the Hessian matrix of \\mathcal{J} at \\map , namely \\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1}","title":"The Gaussian approximation of the posterior: \\mathcal{N}({\\vec \\map},\\bf \\postcov)"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#the-generalized-eigenvalue-problem","text":"\\Hmisfit {\\matrix V} = \\prcov^{-1} {\\matrix V} {\\matrix \\Lambda}, where {\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} contains the generalized eigenvalues and the columns of {\\matrix V}\\in \\mathbb R^{n\\times n} the generalized eigenvectors such that {\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I} .","title":"The generalized eigenvalue problem:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#randomized-eigensolvers-to-construct-the-approximate-spectral-decomposition","text":"When the generalized eigenvalues \\{\\lambda_i\\} decay rapidly, we can extract a low-rank approximation of \\Hmisfit by retaining only the r largest eigenvalues and corresponding eigenvectors, \\Hmisfit = \\prcov^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\prcov^{-1}, Here, \\Vr \\in \\mathbb{R}^{n\\times r} contains only the r generalized eigenvectors of \\Hmisfit that correspond to the r largest eigenvalues, which are assembled into the diagonal matrix {\\matrix{\\Lambda}}_r = \\diag (\\lambda_i) \\in \\mathbb{R}^{r \\times r} .","title":"Randomized eigensolvers to construct the approximate spectral decomposition:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#the-approximate-posterior-covariance","text":"Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \\begin{align} \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1} = \\prcov-\\Vr {\\matrix{D}}_r \\Vr^T + \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i + 1}\\right), \\end{align} where {\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in \\mathbb{R}^{r\\times r} . The last term in this expression captures the error due to truncation in terms of the discarded eigenvalues; this provides a criterion for truncating the spectrum, namely that r is chosen such that \\lambda_r is small relative to 1. Therefore we can approximate the posterior covariance as \\postcov \\approx \\prcov - \\Vr {\\matrix{D}}_r \\Vr^T","title":"The approximate posterior covariance:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#drawing-samples-from-a-gaussian-distribution-with-covariance-h-1","text":"Let {\\bf x} be a sample for the prior distribution, i.e. {\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1}) as {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r + \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1} + {\\bf I} \\big\\} {\\bf x}","title":"Drawing samples from a Gaussian distribution with covariance \\H^{-1}"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#this-tutorial-shows","text":"Description of the inverse problem (the forward problem, the prior, and the misfit functional) Convergence of the inexact Newton-CG algorithm Low-rank-based approximation of the posterior covariance (built on a low-rank approximation of the Hessian of the data misfit) How to construct the low-rank approximation of the Hessian of the data misfit How to apply the inverse and square-root inverse Hessian to a vector efficiently Samples from the Gaussian approximation of the posterior","title":"This tutorial shows:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#goals","text":"By the end of this notebook, you should be able to: Understand the Bayesian inverse framework Visualise and understand the results Modify the problem and code","title":"Goals:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#mathematical-tools-used","text":"Finite element method Derivation of gradient and Hessian via the adjoint method inexact Newton-CG Armijo line search Bayes' formula randomized eigensolvers","title":"Mathematical tools used:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#list-of-software-used","text":"FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , A great python package that I used for plotting many of the results Numpy , A python package for linear algebra. While extensive, this is mostly used to compute means and sums in this notebook.","title":"List of software used:"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#1-load-modules","text":"from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(seed=1)","title":"1. Load modules"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#2-generate-the-true-parameter","text":"This function generates a random field with a prescribed anysotropic covariance function. def true_model(prior): noise = dl.Vector() prior.init_vector(noise,\"noise\") parRandom.normal(1., noise) mtrue = dl.Vector() prior.init_vector(mtrue, 0) prior.sample(noise,mtrue) return mtrue","title":"2. Generate the true parameter"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces","text":"We compute a two dimensional mesh of a unit square with nx by ny elements. We define a P2 finite element space for the state and adjoint variable and P1 for the parameter . ndim = 2 nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh2, Vh1, Vh2] print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641","title":"3. Set up the mesh and finite element spaces"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#4-set-up-the-forward-problem","text":"To set up the forward problem we use the PDEVariationalProblem class, which requires the following inputs - the finite element spaces for the state, parameter, and adjoint variables Vh - the pde in weak form pde_varf - the boundary conditions bc for the forward problem and bc0 for the adjoint and incremental problems. The PDEVariationalProblem class offer the following functionality: - solving the forward/adjoint and incremental problems - evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables. def u_boundary(x, on_boundary): return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS) u_bdr = dl.Expression(\"x[1]\", degree=1) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) f = dl.Constant(0.0) def pde_varf(u,m,p): return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)","title":"4. Set up the forward problem"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#4-set-up-the-prior","text":"To obtain the synthetic true paramter m_{\\rm true} we generate a realization from the prior distribution. Here we assume a Gaussian prior with zero average and covariance matrix \\mathcal{C} = \\mathcal{A}^{-2} . The action of \\mathcal{A} on a field m is given by \\mathcal{A}m = \\left\\{ \\begin{array}{rl} \\gamma \\nabla \\cdot \\left( \\Theta\\nabla m\\right)+ \\delta m & \\text{in } \\Omega\\\\ \\left( \\Theta\\, \\nabla m\\right) \\cdot \\boldsymbol{n} + \\beta m & \\text{on } \\partial\\Omega, \\end{array} \\right. where \\beta \\propto \\sqrt{\\gamma\\delta} is chosen to minimize boundary artifacts. Here \\Theta is an s.p.d. anisotropic tensor of the form \\Theta = \\begin{bmatrix} \\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\ (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2 \\end{bmatrix}. gamma = .1 delta = .5 anis_diff = dl.Expression(code_AnisTensor2D, degree=1) anis_diff.theta0 = 2. anis_diff.theta1 = .5 anis_diff.alpha = math.pi/4 prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True) mtrue = true_model(prior) print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2)) objs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)] mytitles = [\"True Parameter\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() model = Model(pde,prior, misfit) Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2","title":"4. Set up the prior"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations","text":"To setup the observation operator \\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t} , we generate n_t ( ntargets in the code below) random locations where to evaluate the value of the state. Under the assumption of Gaussian additive noise, the likelihood function \\pi_{\\rm like} has the form \\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right), where u(m) denotes the solution of the forward model at a given parameter m . The class PointwiseStateObservation implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state u and parameter m . To generate the synthetic observation, we first solve the forward problem using the true parameter m_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise. rel_noise is the signal to noise ratio. ntargets = 50 rel_noise = 0.01 #Targets only on the bottom targets_x = np.random.uniform(0.1,0.9, [ntargets] ) targets_y = np.random.uniform(0.1,0.5, [ntargets] ) targets = np.zeros([ntargets, ndim]) targets[:,0] = targets_x targets[:,1] = targets_y #targets everywhere #targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) print( \"Number of observation points: {0}\".format(ntargets) ) misfit = PointwiseStateObservation(Vh[STATE], targets) utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev vmax = max( utrue.max(), misfit.d.max() ) vmin = min( utrue.min(), misfit.d.min() ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax) nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax) plt.show() Number of observation points: 50","title":"5. Set up the misfit functional and generate synthetic observations"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian","text":"The model is defined by three component: - the PDEVariationalProblem pde which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems. - the Prior prior which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator) - the Misfit misfit which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables. To test gradient and the Hessian of the model we use forward finite differences. model = Model(pde, prior, misfit) m0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER]) _ = modelVerify(model, m0.vector(), 1e-12) (yy, H xx) - (xx, H yy) = -2.8560765074414784e-14","title":"6. Set up the model and test gradient and Hessian"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#7-compute-the-map-point","text":"We used the globalized Newtown-CG method to compute the MAP point. m = prior.mean.copy() solver = ReducedSpaceNewtonCG(model) solver.parameters[\"rel_tolerance\"] = 1e-6 solver.parameters[\"abs_tolerance\"] = 1e-12 solver.parameters[\"max_iter\"] = 25 solver.parameters[\"inner_rel_tolerance\"] = 1e-15 solver.parameters[\"GN_iter\"] = 5 solver.parameters[\"globalization\"] = \"LS\" solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4 x = solver.solve([None, m, None]) if solver.converged: print( \"\\nConverged in \", solver.it, \" iterations.\") else: print( \"\\nNot Converged\") print( \"Termination reason: \", solver.termination_reasons[solver.reason] ) print( \"Final gradient norm: \", solver.final_grad_norm ) print( \"Final cost: \", solver.final_cost ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\") nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\") plt.show() It cg_it cost misfit reg (g,dm) ||g||L2 alpha tolcg 1 2 5.676752e+02 5.665943e+02 1.080947e+00 -1.487693e+04 4.008143e+04 1.000000e+00 5.000000e-01 2 2 1.315109e+02 1.292859e+02 2.225043e+00 -8.730801e+02 7.040800e+03 1.000000e+00 4.191210e-01 3 4 5.827697e+01 5.504162e+01 3.235343e+00 -1.565905e+02 2.115036e+03 1.000000e+00 2.297139e-01 4 1 5.257433e+01 4.934371e+01 3.230622e+00 -1.142190e+01 1.827983e+03 1.000000e+00 2.135573e-01 5 6 3.958923e+01 3.507197e+01 4.517257e+00 -3.227624e+01 9.179935e+02 1.000000e+00 1.513381e-01 6 2 3.681315e+01 3.218885e+01 4.624304e+00 -5.501178e+00 7.541736e+02 1.000000e+00 1.371716e-01 7 11 3.276506e+01 2.530117e+01 7.463891e+00 -9.109597e+00 4.788040e+02 1.000000e+00 1.092968e-01 8 1 3.225207e+01 2.478982e+01 7.462243e+00 -1.027328e+00 4.890796e+02 1.000000e+00 1.104634e-01 9 10 3.216228e+01 2.445226e+01 7.710018e+00 -1.799343e-01 7.389810e+01 1.000000e+00 4.293832e-02 10 12 3.216003e+01 2.438903e+01 7.770998e+00 -4.481802e-03 1.291573e+01 1.000000e+00 1.795097e-02 11 18 3.215993e+01 2.438404e+01 7.775888e+00 -2.010595e-04 2.284805e+00 1.000000e+00 7.550105e-03 12 19 3.215993e+01 2.438428e+01 7.775646e+00 -8.282645e-08 6.202767e-02 1.000000e+00 1.244002e-03 Converged in 12 iterations. Termination reason: Norm of the gradient less than tolerance Final gradient norm: 0.0006914733735650482 Final cost: 32.15992905025892","title":"7. Compute the MAP point"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#8-compute-the-low-rank-gaussian-approximation-of-the-posterior","text":"We used the double pass algorithm to compute a low-rank decomposition of the Hessian Misfit. In particular, we solve \\Hmisfit {\\bf v}_i = \\lambda_i \\prcov^{-1} {\\bf v}_i. The Figure shows the largest k generalized eigenvectors of the Hessian misfit. The effective rank of the Hessian misfit is the number of eigenvalues above the red line ( y=1 ). The effective rank is independent of the mesh size. model.setPointForHessianEvaluations(x, gauss_newton_approx=False) Hmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], misfit_only=True) k = 50 p = 20 print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior(prior, lmbda, V) posterior.mean = x[PARAMETER] plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15]) Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.","title":"8. Compute the low rank Gaussian approximation of the posterior"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#9-prior-and-posterior-pointwise-variance-fields","text":"compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print( \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200) objs = [dl.Function(Vh[PARAMETER], pr_pw_variance), dl.Function(Vh[PARAMETER], post_pw_variance)] mytitles = [\"Prior variance\", \"Posterior variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 1.014727e+00; Prior trace 1.797376e+00; Correction trace 7.826486e-01","title":"9. Prior and posterior pointwise variance fields"},{"location":"tutorials_v2.3.0/3_SubsurfaceBayesian/3_SubsurfaceBayesian/#10-generate-samples-from-prior-and-posterior","text":"nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\") s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"10. Generate samples from Prior and Posterior"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/","text":"\\def\\D{\\Omega} \\def\\ipar{m} \\def\\R{\\mathbb{R}} \\def\\del{\\partial} \\def\\vec{\\bf} \\def\\priorm{\\mu_0} \\def\\C{\\mathcal{C}} \\def\\Acal{\\mathcal{A}} \\def\\postm{\\mu_{\\rm{post}}} \\def\\iparpost{\\ipar_\\text{post}} \\def\\obs{ {\\vec d}} \\def\\yobs{\\obs^{\\text{obs}}} \\def\\obsop{\\mathcal{B}} \\def\\dd{\\vec{\\bar{d}}} \\def\\iFF{\\mathcal{F}} \\def\\iFFadj{\\mathcal{F}^*} \\def\\ncov{\\Gamma_{\\mathrm{noise}}} Bayesian initial condition inversion in an advection-diffusion problem In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements. The Bayesian inverse problem: Following the Bayesian framework, we utilize a Gaussian prior measure \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) , with \\C_0=\\Acal^{-2} where \\Acal is an elliptic differential operator as described in the PoissonBayesian example, and use an additive Gaussian noise model. Therefore, the solution of the Bayesian inverse problem is the posterior measure, \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post}) with \\iparpost and \\C_\\text{post} . The posterior mean \\iparpost is characterized as the minimizer of \\begin{aligned} & \\mathcal{J}(\\ipar) := \\frac{1}{2} \\left\\| \\obsop u(\\ipar) -\\obs \\right\\|^2_{\\ncov^{-1}} + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)}, \\end{aligned} which can also be interpreted as the regularized functional to be minimized in deterministic inversion. The observation operator \\mathcal{B} extracts the values of the forward solution u on a set of locations \\{{\\vec{x}}_1, \\ldots, {\\vec{x}}_n\\} \\subset \\D at times \\{t_1, \\ldots, t_N\\} \\subset [0, T] . The posterior covariance \\C_{\\text{post}} is the inverse of the Hessian of \\mathcal{J}(\\ipar) , i.e., \\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}. The forward problem: The parameter-to-observable map \\iFF \\,\\ipar := \\obsop\\, u(\\ipar) maps an initial condition \\ipar \\in L^2(\\D) to pointwise spatiotemporal observations of the concentration field u({\\vec x},t) through solution of the advection-diffusion equation given by \\begin{split} u_t - \\kappa\\Delta u + {\\vec v} \\cdot \\nabla u &= 0 & \\quad \\text{in } \\D\\times(0,T),\\\\ u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\ \\kappa \\nabla u\\cdot {\\vec{n}} &= 0 & \\quad \\text{on } \\partial\\D \\times (0,T). \\end{split} Here, \\D \\subset \\R^d ( d \\in \\{2, 3\\} ) is a bounded domain, \\kappa > 0 is the diffusion coefficient and T > 0 is the final time. The velocity field \\vec{v} is computed by solving the following steady-state Navier-Stokes equation with the side walls driving the flow: \\begin{aligned} - \\frac{1}{\\operatorname{Re}} \\Delta {\\vec v} + \\nabla q + {\\vec v} \\cdot \\nabla {\\vec v} &= 0 &\\quad&\\text{ in }\\D,\\\\ \\nabla \\cdot {\\vec v} &= 0 &&\\text{ in }\\D,\\\\ {\\vec v} &= {\\vec g} &&\\text{ on } \\partial\\D. \\end{aligned} Here, q is pressure, \\text{Re} is the Reynolds number. The Dirichlet boundary data {\\vec g} \\in \\R^d is given by {\\vec g} = {\\vec e}_2 on the left wall of the domain, {\\vec g}=-{\\vec e}_2 on the right wall, and {\\vec g} = {\\vec 0} everywhere else. The adjoint problem: The adjoint problem is a final value problem, since p is specified at t = T rather than at t = 0 . Thus, it is solved backwards in time, which amounts to the solution of the advection-diffusion equation \\begin{aligned} -p_t - \\nabla \\cdot (p {\\vec v}) - \\kappa \\Delta p &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\ p(\\cdot, T) &= 0 & &\\text{ in } \\D,\\\\ ({ \\vec{v} }p+\\kappa\\nabla p)\\cdot {\\vec{n}} &= 0 & &\\text{ on } \\partial\\D\\times (0,T). \\end{aligned} Then, the adjoint of the parameter to observable map \\iFF^* is defined by setting \\iFF^*\\obs = p({\\vec x}, 0). 1. Load modules from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" ) from model_ad_diff import TimeDependentAD, SpaceTimePointwiseStateObservation import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) 2. Construct the velocity field def v_boundary(x,on_boundary): return on_boundary def q_boundary(x,on_boundary): return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS def computeVelocityField(mesh): Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2) Wh = dl.FunctionSpace(mesh, 'Lagrange', 1) if dlversion() <= (1,6,0): XW = dl.MixedFunctionSpace([Xh, Wh]) else: mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()]) XW = dl.FunctionSpace(mesh, mixed_element) Re = 1e2 g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1) bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary) bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise') bcs = [bc1, bc2] vq = dl.Function(XW) (v,q) = dl.split(vq) (v_test, q_test) = dl.TestFunctions (XW) def strain(v): return dl.sym(dl.nabla_grad(v)) F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test) - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\": {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}}) plt.figure(figsize=(15,5)) vh = dl.project(v,Xh) qh = dl.project(q,Wh) nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\") nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\") plt.show() return v 3. Set up the mesh and finite element spaces mesh = dl.refine( dl.Mesh(\"ad_20.xml\") ) wind_velocity = computeVelocityField(mesh) Vh = dl.FunctionSpace(mesh, \"Lagrange\", 1) print( \"Number of dofs: {0}\".format( Vh.dim() ) ) Number of dofs: 2023 4. Set up model (prior, true/proposed initial condition) ic_expr = dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))', element=Vh.ufl_element()) true_initial_condition = dl.interpolate(ic_expr, Vh).vector() gamma = 1. delta = 8. prior = BiLaplacianPrior(Vh, gamma, delta, robin_bc=True) print( \"Prior regularization: (delta - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2) ) prior.mean = dl.interpolate(dl.Constant(0.25), Vh).vector() t_init = 0. t_final = 4. t_1 = 1. dt = .1 observation_dt = .2 simulation_times = np.arange(t_init, t_final+.5*dt, dt) observation_times = np.arange(t_1, t_final+.5*dt, observation_dt) targets = np.loadtxt('targets.txt') print (\"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = SpaceTimePointwiseStateObservation(Vh, observation_times, targets) problem = TimeDependentAD(mesh, [Vh,Vh,Vh], prior, misfit, simulation_times, wind_velocity, True) objs = [dl.Function(Vh,true_initial_condition), dl.Function(Vh,prior.mean)] mytitles = [\"True Initial Condition\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() Prior regularization: (delta - gamma*Laplacian)^order: delta=8.0, gamma=1.0, order=2 Number of observation points: 80 5. Generate the synthetic observations rel_noise = 0.01 utrue = problem.generate_vector(STATE) x = [utrue, true_initial_condition, None] problem.solveFwd(x[STATE], x, 1e-9) misfit.observe(x, misfit.d) MAX = misfit.d.norm(\"linf\", \"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev,misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev nb.show_solution(Vh, true_initial_condition, utrue, \"Solution\") 6. Test the gradient and the Hessian of the cost (negative log posterior) m0 = true_initial_condition.copy() _ = modelVerify(problem, m0, 1e-12, is_quadratic=True) (yy, H xx) - (xx, H yy) = -1.174447494183385e-13 7. Evaluate the gradient [u,m,p] = problem.generate_vector() problem.solveFwd(u, [u,m,p], 1e-12) problem.solveAdj(p, [u,m,p], 1e-12) mg = problem.generate_vector(PARAMETER) grad_norm = problem.evalGradientParameter([u,m,p], mg) print( \"(g,g) = \", grad_norm) (g,g) = 2395071437584061.5 8. The Gaussian posterior H = ReducedHessian(problem, 1e-12, misfit_only=True) k = 80 p = 20 print( \"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = singlePassG(H, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior( prior, lmbda, V ) plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh, V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60]) Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20. 9. Compute the MAP point H.misfit_only = False solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( posterior.Hlr ) solver.parameters[\"print_level\"] = 1 solver.parameters[\"rel_tolerance\"] = 1e-6 solver.solve(m, -mg) problem.solveFwd(u, [u,m,p], 1e-12) total_cost, reg_cost, misfit_cost = problem.cost([u,m,p]) print( \"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost) ) posterior.mean = m plt.figure(figsize=(7.5,5)) nb.plot(dl.Function(Vh, m), mytitle=\"Initial Condition\") plt.show() nb.show_solution(Vh, m, u, \"Solution\") Iterartion : 0 (B r, r) = 1225123.9338492027 Iteration : 1 (B r, r) = 74.94198007028052 Iteration : 2 (B r, r) = 0.4132940000407347 Iteration : 3 (B r, r) = 0.005765443940190191 Iteration : 4 (B r, r) = 2.243708962553132e-05 Iteration : 5 (B r, r) = 1.4026897765563288e-07 Relative/Absolute residual less than tol Converged in 5 iterations with final norm 0.0003745250027109444 Total cost 782.818; Reg Cost 153.671; Misfit 629.147 10. Prior and posterior pointwise variance fields compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=300) print( \"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300) objs = [dl.Function(Vh, pr_pw_variance), dl.Function(Vh, post_pw_variance)] mytitles = [\"Prior Variance\", \"Posterior Variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 0.000269147; Prior trace 0.00809706; Correction trace 0.00782792 11. Draw samples from the prior and posterior distributions nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh, name=\"sample_prior\") s_post = dl.Function(Vh, name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"4 AdvectionDiffusionBayesian"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#bayesian-initial-condition-inversion-in-an-advection-diffusion-problem","text":"In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.","title":"Bayesian initial condition inversion in an advection-diffusion problem"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#the-bayesian-inverse-problem","text":"Following the Bayesian framework, we utilize a Gaussian prior measure \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) , with \\C_0=\\Acal^{-2} where \\Acal is an elliptic differential operator as described in the PoissonBayesian example, and use an additive Gaussian noise model. Therefore, the solution of the Bayesian inverse problem is the posterior measure, \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post}) with \\iparpost and \\C_\\text{post} . The posterior mean \\iparpost is characterized as the minimizer of \\begin{aligned} & \\mathcal{J}(\\ipar) := \\frac{1}{2} \\left\\| \\obsop u(\\ipar) -\\obs \\right\\|^2_{\\ncov^{-1}} + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)}, \\end{aligned} which can also be interpreted as the regularized functional to be minimized in deterministic inversion. The observation operator \\mathcal{B} extracts the values of the forward solution u on a set of locations \\{{\\vec{x}}_1, \\ldots, {\\vec{x}}_n\\} \\subset \\D at times \\{t_1, \\ldots, t_N\\} \\subset [0, T] . The posterior covariance \\C_{\\text{post}} is the inverse of the Hessian of \\mathcal{J}(\\ipar) , i.e., \\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.","title":"The Bayesian inverse problem:"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#the-forward-problem","text":"The parameter-to-observable map \\iFF \\,\\ipar := \\obsop\\, u(\\ipar) maps an initial condition \\ipar \\in L^2(\\D) to pointwise spatiotemporal observations of the concentration field u({\\vec x},t) through solution of the advection-diffusion equation given by \\begin{split} u_t - \\kappa\\Delta u + {\\vec v} \\cdot \\nabla u &= 0 & \\quad \\text{in } \\D\\times(0,T),\\\\ u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\ \\kappa \\nabla u\\cdot {\\vec{n}} &= 0 & \\quad \\text{on } \\partial\\D \\times (0,T). \\end{split} Here, \\D \\subset \\R^d ( d \\in \\{2, 3\\} ) is a bounded domain, \\kappa > 0 is the diffusion coefficient and T > 0 is the final time. The velocity field \\vec{v} is computed by solving the following steady-state Navier-Stokes equation with the side walls driving the flow: \\begin{aligned} - \\frac{1}{\\operatorname{Re}} \\Delta {\\vec v} + \\nabla q + {\\vec v} \\cdot \\nabla {\\vec v} &= 0 &\\quad&\\text{ in }\\D,\\\\ \\nabla \\cdot {\\vec v} &= 0 &&\\text{ in }\\D,\\\\ {\\vec v} &= {\\vec g} &&\\text{ on } \\partial\\D. \\end{aligned} Here, q is pressure, \\text{Re} is the Reynolds number. The Dirichlet boundary data {\\vec g} \\in \\R^d is given by {\\vec g} = {\\vec e}_2 on the left wall of the domain, {\\vec g}=-{\\vec e}_2 on the right wall, and {\\vec g} = {\\vec 0} everywhere else.","title":"The forward problem:"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#the-adjoint-problem","text":"The adjoint problem is a final value problem, since p is specified at t = T rather than at t = 0 . Thus, it is solved backwards in time, which amounts to the solution of the advection-diffusion equation \\begin{aligned} -p_t - \\nabla \\cdot (p {\\vec v}) - \\kappa \\Delta p &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\ p(\\cdot, T) &= 0 & &\\text{ in } \\D,\\\\ ({ \\vec{v} }p+\\kappa\\nabla p)\\cdot {\\vec{n}} &= 0 & &\\text{ on } \\partial\\D\\times (0,T). \\end{aligned} Then, the adjoint of the parameter to observable map \\iFF^* is defined by setting \\iFF^*\\obs = p({\\vec x}, 0).","title":"The adjoint problem:"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#1-load-modules","text":"from __future__ import absolute_import, division, print_function import dolfin as dl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" ) from model_ad_diff import TimeDependentAD, SpaceTimePointwiseStateObservation import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#2-construct-the-velocity-field","text":"def v_boundary(x,on_boundary): return on_boundary def q_boundary(x,on_boundary): return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS def computeVelocityField(mesh): Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2) Wh = dl.FunctionSpace(mesh, 'Lagrange', 1) if dlversion() <= (1,6,0): XW = dl.MixedFunctionSpace([Xh, Wh]) else: mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()]) XW = dl.FunctionSpace(mesh, mixed_element) Re = 1e2 g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1) bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary) bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise') bcs = [bc1, bc2] vq = dl.Function(XW) (v,q) = dl.split(vq) (v_test, q_test) = dl.TestFunctions (XW) def strain(v): return dl.sym(dl.nabla_grad(v)) F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test) - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\": {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}}) plt.figure(figsize=(15,5)) vh = dl.project(v,Xh) qh = dl.project(q,Wh) nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\") nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\") plt.show() return v","title":"2. Construct the velocity field"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces","text":"mesh = dl.refine( dl.Mesh(\"ad_20.xml\") ) wind_velocity = computeVelocityField(mesh) Vh = dl.FunctionSpace(mesh, \"Lagrange\", 1) print( \"Number of dofs: {0}\".format( Vh.dim() ) ) Number of dofs: 2023","title":"3. Set up the mesh and finite element spaces"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition","text":"ic_expr = dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))', element=Vh.ufl_element()) true_initial_condition = dl.interpolate(ic_expr, Vh).vector() gamma = 1. delta = 8. prior = BiLaplacianPrior(Vh, gamma, delta, robin_bc=True) print( \"Prior regularization: (delta - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2) ) prior.mean = dl.interpolate(dl.Constant(0.25), Vh).vector() t_init = 0. t_final = 4. t_1 = 1. dt = .1 observation_dt = .2 simulation_times = np.arange(t_init, t_final+.5*dt, dt) observation_times = np.arange(t_1, t_final+.5*dt, observation_dt) targets = np.loadtxt('targets.txt') print (\"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = SpaceTimePointwiseStateObservation(Vh, observation_times, targets) problem = TimeDependentAD(mesh, [Vh,Vh,Vh], prior, misfit, simulation_times, wind_velocity, True) objs = [dl.Function(Vh,true_initial_condition), dl.Function(Vh,prior.mean)] mytitles = [\"True Initial Condition\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() Prior regularization: (delta - gamma*Laplacian)^order: delta=8.0, gamma=1.0, order=2 Number of observation points: 80","title":"4. Set up model (prior, true/proposed initial condition)"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations","text":"rel_noise = 0.01 utrue = problem.generate_vector(STATE) x = [utrue, true_initial_condition, None] problem.solveFwd(x[STATE], x, 1e-9) misfit.observe(x, misfit.d) MAX = misfit.d.norm(\"linf\", \"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev,misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev nb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")","title":"5. Generate the synthetic observations"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior","text":"m0 = true_initial_condition.copy() _ = modelVerify(problem, m0, 1e-12, is_quadratic=True) (yy, H xx) - (xx, H yy) = -1.174447494183385e-13","title":"6. Test the gradient and the Hessian of the cost (negative log posterior)"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#7-evaluate-the-gradient","text":"[u,m,p] = problem.generate_vector() problem.solveFwd(u, [u,m,p], 1e-12) problem.solveAdj(p, [u,m,p], 1e-12) mg = problem.generate_vector(PARAMETER) grad_norm = problem.evalGradientParameter([u,m,p], mg) print( \"(g,g) = \", grad_norm) (g,g) = 2395071437584061.5","title":"7. Evaluate the gradient"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#8-the-gaussian-posterior","text":"H = ReducedHessian(problem, 1e-12, misfit_only=True) k = 80 p = 20 print( \"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = singlePassG(H, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior( prior, lmbda, V ) plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh, V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60]) Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.","title":"8. The Gaussian posterior"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#9-compute-the-map-point","text":"H.misfit_only = False solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( posterior.Hlr ) solver.parameters[\"print_level\"] = 1 solver.parameters[\"rel_tolerance\"] = 1e-6 solver.solve(m, -mg) problem.solveFwd(u, [u,m,p], 1e-12) total_cost, reg_cost, misfit_cost = problem.cost([u,m,p]) print( \"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost) ) posterior.mean = m plt.figure(figsize=(7.5,5)) nb.plot(dl.Function(Vh, m), mytitle=\"Initial Condition\") plt.show() nb.show_solution(Vh, m, u, \"Solution\") Iterartion : 0 (B r, r) = 1225123.9338492027 Iteration : 1 (B r, r) = 74.94198007028052 Iteration : 2 (B r, r) = 0.4132940000407347 Iteration : 3 (B r, r) = 0.005765443940190191 Iteration : 4 (B r, r) = 2.243708962553132e-05 Iteration : 5 (B r, r) = 1.4026897765563288e-07 Relative/Absolute residual less than tol Converged in 5 iterations with final norm 0.0003745250027109444 Total cost 782.818; Reg Cost 153.671; Misfit 629.147","title":"9. Compute the MAP point"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields","text":"compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=300) print( \"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300) objs = [dl.Function(Vh, pr_pw_variance), dl.Function(Vh, post_pw_variance)] mytitles = [\"Prior Variance\", \"Posterior Variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 0.000269147; Prior trace 0.00809706; Correction trace 0.00782792","title":"10. Prior and posterior pointwise variance fields"},{"location":"tutorials_v2.3.0/4_AdvectionDiffusionBayesian/4_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions","text":"nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh, name=\"sample_prior\") s_post = dl.Function(Vh, name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"11. Draw samples from the prior and posterior distributions"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/","text":"Spectrum of the preconditioned Hessian misfit operator The linear source inversion problem We consider the following linear source inversion problem. Find the state u \\in H^1_{\\Gamma_D}(\\Omega) and the source ( parameter ) m \\in H^1(\\Omega) that solves \\begin{aligned} {} & \\min_m \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx & {}\\\\ {\\rm s.t.} & {} &{} \\\\ {} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = m & {\\rm in} \\; \\Omega\\\\ {} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\ {} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\ \\end{aligned} Here: u_d is a n_{\\rm obs} finite dimensional vector that denotes noisy observations of the state u in n_{\\rm obs} locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . Specifically, u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where \\eta_i are i.i.d. \\mathcal{N}(0, \\sigma^2) . B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}} is the linear operator that evaluates the state u at the observation locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . \\delta and \\gamma are the parameters of the regularization penalizing the L^2(\\Omega) and H^1(\\Omega) norm of m-m_0 , respectively. k , {\\bf v} , c are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively. \\Gamma_D \\subset \\partial \\Omega , \\Gamma_N \\subset \\partial \\Omega represents the subdomain of \\partial\\Omega where we impose Dirichlet or Neumann boundary conditions, respectively. 1. Load modules from __future__ import absolute_import, division, print_function import dolfin as dl import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) 2. The linear source inversion problem def pde_varf(u,m,p): return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\ + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\ + c*u*p*dl.dx \\ - m*p*dl.dx def u_boundary(x, on_boundary): return on_boundary and x[1] < dl.DOLFIN_EPS def solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True): myRandom = Random() mesh = dl.UnitSquareMesh(nx, ny) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh1, Vh1, Vh1] if verbose: print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) u_bdr = dl.Constant(0.0) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) mtrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector() m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector() pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) if verbose: print( \"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = PointwiseStateObservation(Vh[STATE], targets) reg = LaplacianPrior(Vh[PARAMETER], gamma, delta) #Generate synthetic observations utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX myRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], mtrue), mytitle = \"True source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132) nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133) plt.show() model = Model(pde, reg, misfit) u = model.generate_vector(STATE) m = m0.copy() p = model.generate_vector(ADJOINT) x = [u,m,p] mg = model.generate_vector(PARAMETER) model.solveFwd(u, x) model.solveAdj(p, x) model.evalGradientParameter(x, mg) model.setPointForHessianEvaluations(x, gauss_newton_approx=False) H = ReducedHessian(model, 1e-12) solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( reg.Rsolver ) solver.parameters[\"print_level\"] = -1 solver.parameters[\"rel_tolerance\"] = 1e-9 solver.solve(m, -mg) if solver.converged: if verbose: print( \"CG converged in \", solver.iter, \" iterations.\" ) else: print( \"CG did not converged.\" ) raise model.solveFwd(u, x, 1e-12) total_cost, reg_cost, misfit_cost = model.cost(x) if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], m), mytitle = \"Reconstructed source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132) nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133) plt.show() H.misfit_only = True k_evec = 80 p_evec = 5 if verbose: print( \"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec) ) Omega = MultiVector(x[PARAMETER], k_evec+p_evec) myRandom.normal(1., Omega) lmbda, V = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec) if verbose: plt.figure() nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\") nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15]) plt.show() return lmbda, V, Vh[PARAMETER], solver.iter 3. Solution of the source inversion problem ndim = 2 nx = 32 ny = 32 ntargets = 300 np.random.seed(seed=1) targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) rel_noise = 0.01 gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) lmbda, V, Vm, nit = solve(nx,ny, targets, rel_noise, gamma, delta) Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089 Number of observation points: 300 CG converged in 70 iterations. Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5. 4. Mesh independence of the spectrum of the preconditioned Hessian misfit gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) n = [16,32,64] lmbda1, V1, Vm1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5]) plt.show() Number of Iterations: 69 68 67 5. Dependence on the noise level We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization. gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) rel_noise = [1e-3,1e-2,1e-1] lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5]) plt.show() Number of Iterations: 166 69 23 6. Dependence on the PDE coefficients Assume a constant reaction term c = 1 , and we consider different values for the diffusivity coefficient k . The smaller the value of k the slower the decay in the spectrum. rel_noise = 0.01 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(1.0) lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.1) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.01) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"k=1. Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"k=0.1 Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"k=0.01 Eigen\", which=[0,1,5]) plt.show() Number of Iterations: 80 148 256 Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Spectrum of the preconditioned Hessian misfit operator"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#spectrum-of-the-preconditioned-hessian-misfit-operator","text":"","title":"Spectrum of the preconditioned Hessian misfit operator"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#the-linear-source-inversion-problem","text":"We consider the following linear source inversion problem. Find the state u \\in H^1_{\\Gamma_D}(\\Omega) and the source ( parameter ) m \\in H^1(\\Omega) that solves \\begin{aligned} {} & \\min_m \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx & {}\\\\ {\\rm s.t.} & {} &{} \\\\ {} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = m & {\\rm in} \\; \\Omega\\\\ {} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\ {} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\ \\end{aligned} Here: u_d is a n_{\\rm obs} finite dimensional vector that denotes noisy observations of the state u in n_{\\rm obs} locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . Specifically, u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where \\eta_i are i.i.d. \\mathcal{N}(0, \\sigma^2) . B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}} is the linear operator that evaluates the state u at the observation locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . \\delta and \\gamma are the parameters of the regularization penalizing the L^2(\\Omega) and H^1(\\Omega) norm of m-m_0 , respectively. k , {\\bf v} , c are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively. \\Gamma_D \\subset \\partial \\Omega , \\Gamma_N \\subset \\partial \\Omega represents the subdomain of \\partial\\Omega where we impose Dirichlet or Neumann boundary conditions, respectively.","title":"The linear source inversion problem"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#1-load-modules","text":"from __future__ import absolute_import, division, print_function import dolfin as dl import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#2-the-linear-source-inversion-problem","text":"def pde_varf(u,m,p): return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\ + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\ + c*u*p*dl.dx \\ - m*p*dl.dx def u_boundary(x, on_boundary): return on_boundary and x[1] < dl.DOLFIN_EPS def solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True): myRandom = Random() mesh = dl.UnitSquareMesh(nx, ny) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh1, Vh1, Vh1] if verbose: print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) u_bdr = dl.Constant(0.0) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) mtrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector() m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector() pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) if verbose: print( \"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = PointwiseStateObservation(Vh[STATE], targets) reg = LaplacianPrior(Vh[PARAMETER], gamma, delta) #Generate synthetic observations utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x, 1e-9) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX myRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], mtrue), mytitle = \"True source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132) nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133) plt.show() model = Model(pde, reg, misfit) u = model.generate_vector(STATE) m = m0.copy() p = model.generate_vector(ADJOINT) x = [u,m,p] mg = model.generate_vector(PARAMETER) model.solveFwd(u, x) model.solveAdj(p, x) model.evalGradientParameter(x, mg) model.setPointForHessianEvaluations(x, gauss_newton_approx=False) H = ReducedHessian(model, 1e-12) solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( reg.Rsolver ) solver.parameters[\"print_level\"] = -1 solver.parameters[\"rel_tolerance\"] = 1e-9 solver.solve(m, -mg) if solver.converged: if verbose: print( \"CG converged in \", solver.iter, \" iterations.\" ) else: print( \"CG did not converged.\" ) raise model.solveFwd(u, x, 1e-12) total_cost, reg_cost, misfit_cost = model.cost(x) if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], m), mytitle = \"Reconstructed source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132) nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133) plt.show() H.misfit_only = True k_evec = 80 p_evec = 5 if verbose: print( \"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec) ) Omega = MultiVector(x[PARAMETER], k_evec+p_evec) myRandom.normal(1., Omega) lmbda, V = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec) if verbose: plt.figure() nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\") nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15]) plt.show() return lmbda, V, Vh[PARAMETER], solver.iter","title":"2. The linear source inversion problem"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#3-solution-of-the-source-inversion-problem","text":"ndim = 2 nx = 32 ny = 32 ntargets = 300 np.random.seed(seed=1) targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) rel_noise = 0.01 gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) lmbda, V, Vm, nit = solve(nx,ny, targets, rel_noise, gamma, delta) Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089 Number of observation points: 300 CG converged in 70 iterations. Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.","title":"3. Solution of the source inversion problem"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian-misfit","text":"gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) n = [16,32,64] lmbda1, V1, Vm1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5]) plt.show() Number of Iterations: 69 68 67","title":"4. Mesh independence of the spectrum of the preconditioned Hessian misfit"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#5-dependence-on-the-noise-level","text":"We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization. gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) rel_noise = [1e-3,1e-2,1e-1] lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5]) plt.show() Number of Iterations: 166 69 23","title":"5. Dependence on the noise level"},{"location":"tutorials_v2.3.0/5_HessianSpectrum/5_HessianSpectrum/#6-dependence-on-the-pde-coefficients","text":"Assume a constant reaction term c = 1 , and we consider different values for the diffusivity coefficient k . The smaller the value of k the slower the decay in the spectrum. rel_noise = 0.01 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(1.0) lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.1) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.01) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"k=1. Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"k=0.1 Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"k=0.01 Eigen\", which=[0,1,5]) plt.show() Number of Iterations: 80 148 256 Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"6. Dependence on the PDE coefficients"},{"location":"tutorials_v3.0.0/1_FEniCS101/","text":"FEniCS101 Tutorial In this tutorial we consider the boundary value problem (BVP) \\begin{eqnarray*} - \\nabla \\cdot (k \\nabla u) = f & \\text{ in } \\Omega,\\\\ u = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\cup \\Gamma_{\\rm right},\\\\ k \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\cup \\Gamma_{\\rm bottom}, \\end{eqnarray*} where \\Omega = (0,1) \\times (0,1) , \\Gamma_D and \\Gamma_N are the union of the left and right, and top and bottom boundaries of \\Omega , respectively. Here \\begin{eqnarray*} k(x,y) = 1 & \\text{ on } \\Omega\\\\ f(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\ u_0(x,y) = 0 & \\text{ on } \\Gamma_D, \\\\ \\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right. & \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array} \\end{eqnarray*} The exact solution is u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). Weak formulation Let us define the Hilbert spaces V_{u_0}, V_0 \\in \\Omega as V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\}, V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}. To obtain the weak formulation, we multiply the PDE by an arbitrary function v \\in V_0 and integrate over the domain \\Omega leading to -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. Then, integration by parts the non-conforming term gives \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. Finally by recalling that v = 0 on \\Gamma_D and that k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma on \\Gamma_N , we find the weak formulation: Find * u \\in V_{u_0} such that* \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. 1. Load modules To start we load the following modules: dolfin and ufl: the python/C++ interface to FEniCS math : the python module for mathematical functions numpy : a python package for linear algebra matplotlib : a python package used for plotting the results import dolfin as dl import ufl import math import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) 2. Define the mesh and the finite element space We construct a triangulation (mesh) \\mathcal{T}_h of the computational domain \\Omega := [0, 1]^2 with n elements in each direction. On the mesh \\mathcal{T}_h , we then define the finite element space V_h \\subset H^1(\\Omega) consisting of globally continuous piecewise polynomials. The degree variable defines the polynomial degree. n = 16 degree = 1 mesh = dl.UnitSquareMesh(n, n) nb.plot(mesh) Vh = dl.FunctionSpace(mesh, 'Lagrange', degree) print( \"dim(Vh) = \", Vh.dim() ) dim(Vh) = 289 3. Define boundary labels To partition the boundary of \\Omega in the subdomains \\Gamma_{\\rm top} , \\Gamma_{\\rm bottom} , \\Gamma_{\\rm left} , \\Gamma_{\\rm right} we assign a unique label boundary_parts to each of part of \\partial \\Omega . class TopBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1] - 1) < dl.DOLFIN_EPS class BottomBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1]) < dl.DOLFIN_EPS class LeftBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0]) < dl.DOLFIN_EPS class RightBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0] - 1) < dl.DOLFIN_EPS boundary_parts = dl.MeshFunction(\"size_t\", mesh, 1) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) 4. Define the coefficients of the PDE and the boundary conditions We first define the coefficients of the PDE using the dl.Constant and dl.Expression classes. dl.Constant is used to define coefficients that do not depend on the space coordinates, dl.Expression is used to define coefficients that are a known function of the space coordinates x[0] (x-axis direction) and x[1] (y-axis direction). In the finite element method community, Dirichlet boundary conditions are also known as essential boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class dl.DirichletBC to indicate this type of condition. On the other hand, Newman boundary conditions are also known as natural boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure ds[i] to integrate over the portion of the boundary marked with label i . u_L = dl.Constant(0.) u_R = dl.Constant(0.) sigma_bottom = dl.Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5) sigma_top = dl.Constant(0.) f = dl.Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5) bcs = [dl.DirichletBC(Vh, u_L, boundary_parts, 3), dl.DirichletBC(Vh, u_R, boundary_parts, 4)] ds = dl.Measure(\"ds\", subdomain_data=boundary_parts) 5. Define and solve the variational problem We also define two special types of functions: the dl.TrialFunction u and the dl.TestFunction v . These special types of function are used by FEniCS to generate the finite element vectors and matrices which stem from the weak formulation of the PDE. More specifically, by denoting by \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)} the finite element basis for the space V_h , a function u_h \\in V_h can be written as u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), where {\\rm u}_i represents the coefficients in the finite element expansion of u_h . We then define the bilinear form a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h \\, dx ; the linear form L(v_h) = \\int_\\Omega f v_h \\, dx + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h \\, ds + \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h \\,ds . We can then solve the variational problem Find u_h \\in V_h such that a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h using directly the built-in dl.solve method in FEniCS. NOTE: As an alternative one can also assemble the finite element matrix A and the right hand side b that stems from the discretization of a and L , and then solve the linear system A {\\rm u} = {\\rm b}, where {\\rm u} is the vector collecting the coefficients of the finite element expasion of u_h , the entries of the matrix A are such that A_{ij} = a(\\phi_j, \\phi_i) , the entries of the right hand side b are such that b_i = L(\\phi_i) . u = dl.TrialFunction(Vh) v = dl.TestFunction(Vh) a = ufl.inner(ufl.grad(u), ufl.grad(v))*ufl.dx L = f*v*ufl.dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = dl.Function(Vh) #dl.solve(a == L, uh, bcs=bcs) A, b = dl.assemble_system(a,L, bcs=bcs) dl.solve(A, uh.vector(), b, \"cg\") nb.plot(uh) <matplotlib.collections.TriMesh at 0x11b389ac8> 6. Compute the discretization error For this problem, the exact solution is known. We can therefore compute the following norms of the discretization error (i.e. the difference between the finite element solution u_h and the exact solution u_{\\rm ex} ) \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, and \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. u_e = dl.Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5) grad_u_e = dl.Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5) err_L2 = math.sqrt( dl.assemble( (uh-u_e)**2*ufl.dx ) ) err_grad = math.sqrt( dl.assemble( ufl.inner(ufl.grad(uh) - grad_u_e, ufl.grad(uh) - grad_u_e)*ufl.dx ) ) err_H1 = math.sqrt( err_L2**2 + err_grad**2) print (\"|| u_h - u_e ||_L2 = \", err_L2) print (\"|| u_h - u_e ||_H1 = \", err_H1) || u_h - u_e ||_L2 = 0.008805176703139152 || u_h - u_e ||_H1 = 0.3967189507839944 7. Convergence of the finite element method We now verify numerically a well-known convergence result for the finite element method. Let s denote the polynomial degree of the finite element space, and assume that the solution u_{\\rm ex} is at least in H^{s+1}(\\Omega) . Then we have \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. In the code below, the function compute(n, degree) solves the PDE using a mesh with n elements in each direction and finite element spaces of polynomial order degree . The figure below shows the discretization errors in the H^1 and L^2 as a function of the mesh size h ( h = \\frac{1}{n} ) for piecewise linear (P1, s=1 ) and piecewise quadratic (P2, s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular: for piecewise linear finite element P1 we observe first order convergence in the H^1 -norm and second order convergence in the L^2 -norm; for piecewise quadratic finite element P2 we observe second order convergence in the H^1 -norm and third order convergence in the L^2 -norm. def compute(n, degree): mesh = dl.UnitSquareMesh(n, n) Vh = dl.FunctionSpace(mesh, 'Lagrange', degree) boundary_parts = dl.MeshFunction(\"size_t\", mesh,1) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) bcs = [dl.DirichletBC(Vh, u_L, boundary_parts, 3), dl.DirichletBC(Vh, u_R, boundary_parts, 4)] ds = dl.Measure(\"ds\", subdomain_data=boundary_parts) u = dl.TrialFunction(Vh) v = dl.TestFunction(Vh) a = ufl.inner(ufl.grad(u), ufl.grad(v))*ufl.dx L = f*v*ufl.dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = dl.Function(Vh) dl.solve(a == L, uh, bcs=bcs) err_L2 = math.sqrt( dl.assemble( (uh-u_e)**2*ufl.dx ) ) err_grad = math.sqrt( dl.assemble( ufl.inner(ufl.grad(uh) - grad_u_e, ufl.grad(uh) - grad_u_e)*ufl.dx ) ) err_H1 = math.sqrt( err_L2**2 + err_grad**2) return err_L2, err_H1 nref = 5 n = 8*np.power(2,np.arange(0,nref)) h = 1./n err_L2_P1 = np.zeros(nref) err_H1_P1 = np.zeros(nref) err_L2_P2 = np.zeros(nref) err_H1_P2 = np.zeros(nref) for i in range(nref): err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1) err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2) plt.figure(figsize=(15,5)) plt.subplot(121) plt.loglog(h, err_H1_P1, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P1, '-*b', label = \"L2 error\") plt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label = \"First Order\") plt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label = \"Second Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P1 Finite Element\") plt.legend(loc='lower right') plt.subplot(122) plt.loglog(h, err_H1_P2, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P2, '-*b', label = \"L2 error\") plt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label = \"Second Order\") plt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label = \"Third Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P2 Finite Element\") plt.legend(loc='lower right') plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"FEniCS101 Tutorial"},{"location":"tutorials_v3.0.0/1_FEniCS101/#fenics101-tutorial","text":"In this tutorial we consider the boundary value problem (BVP) \\begin{eqnarray*} - \\nabla \\cdot (k \\nabla u) = f & \\text{ in } \\Omega,\\\\ u = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\cup \\Gamma_{\\rm right},\\\\ k \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\cup \\Gamma_{\\rm bottom}, \\end{eqnarray*} where \\Omega = (0,1) \\times (0,1) , \\Gamma_D and \\Gamma_N are the union of the left and right, and top and bottom boundaries of \\Omega , respectively. Here \\begin{eqnarray*} k(x,y) = 1 & \\text{ on } \\Omega\\\\ f(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\ u_0(x,y) = 0 & \\text{ on } \\Gamma_D, \\\\ \\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right. & \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array} \\end{eqnarray*} The exact solution is u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).","title":"FEniCS101 Tutorial"},{"location":"tutorials_v3.0.0/1_FEniCS101/#weak-formulation","text":"Let us define the Hilbert spaces V_{u_0}, V_0 \\in \\Omega as V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\}, V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}. To obtain the weak formulation, we multiply the PDE by an arbitrary function v \\in V_0 and integrate over the domain \\Omega leading to -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. Then, integration by parts the non-conforming term gives \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. Finally by recalling that v = 0 on \\Gamma_D and that k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma on \\Gamma_N , we find the weak formulation: Find * u \\in V_{u_0} such that* \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.","title":"Weak formulation"},{"location":"tutorials_v3.0.0/1_FEniCS101/#1-load-modules","text":"To start we load the following modules: dolfin and ufl: the python/C++ interface to FEniCS math : the python module for mathematical functions numpy : a python package for linear algebra matplotlib : a python package used for plotting the results import dolfin as dl import ufl import math import numpy as np import logging import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import nb logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v3.0.0/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space","text":"We construct a triangulation (mesh) \\mathcal{T}_h of the computational domain \\Omega := [0, 1]^2 with n elements in each direction. On the mesh \\mathcal{T}_h , we then define the finite element space V_h \\subset H^1(\\Omega) consisting of globally continuous piecewise polynomials. The degree variable defines the polynomial degree. n = 16 degree = 1 mesh = dl.UnitSquareMesh(n, n) nb.plot(mesh) Vh = dl.FunctionSpace(mesh, 'Lagrange', degree) print( \"dim(Vh) = \", Vh.dim() ) dim(Vh) = 289","title":"2. Define the mesh and the finite element space"},{"location":"tutorials_v3.0.0/1_FEniCS101/#3-define-boundary-labels","text":"To partition the boundary of \\Omega in the subdomains \\Gamma_{\\rm top} , \\Gamma_{\\rm bottom} , \\Gamma_{\\rm left} , \\Gamma_{\\rm right} we assign a unique label boundary_parts to each of part of \\partial \\Omega . class TopBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1] - 1) < dl.DOLFIN_EPS class BottomBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[1]) < dl.DOLFIN_EPS class LeftBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0]) < dl.DOLFIN_EPS class RightBoundary(dl.SubDomain): def inside(self, x, on_boundary): return on_boundary and abs(x[0] - 1) < dl.DOLFIN_EPS boundary_parts = dl.MeshFunction(\"size_t\", mesh, 1) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4)","title":"3. Define boundary labels"},{"location":"tutorials_v3.0.0/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions","text":"We first define the coefficients of the PDE using the dl.Constant and dl.Expression classes. dl.Constant is used to define coefficients that do not depend on the space coordinates, dl.Expression is used to define coefficients that are a known function of the space coordinates x[0] (x-axis direction) and x[1] (y-axis direction). In the finite element method community, Dirichlet boundary conditions are also known as essential boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class dl.DirichletBC to indicate this type of condition. On the other hand, Newman boundary conditions are also known as natural boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure ds[i] to integrate over the portion of the boundary marked with label i . u_L = dl.Constant(0.) u_R = dl.Constant(0.) sigma_bottom = dl.Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5) sigma_top = dl.Constant(0.) f = dl.Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5) bcs = [dl.DirichletBC(Vh, u_L, boundary_parts, 3), dl.DirichletBC(Vh, u_R, boundary_parts, 4)] ds = dl.Measure(\"ds\", subdomain_data=boundary_parts)","title":"4. Define the coefficients of the PDE and the boundary conditions"},{"location":"tutorials_v3.0.0/1_FEniCS101/#5-define-and-solve-the-variational-problem","text":"We also define two special types of functions: the dl.TrialFunction u and the dl.TestFunction v . These special types of function are used by FEniCS to generate the finite element vectors and matrices which stem from the weak formulation of the PDE. More specifically, by denoting by \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)} the finite element basis for the space V_h , a function u_h \\in V_h can be written as u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), where {\\rm u}_i represents the coefficients in the finite element expansion of u_h . We then define the bilinear form a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h \\, dx ; the linear form L(v_h) = \\int_\\Omega f v_h \\, dx + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h \\, ds + \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h \\,ds . We can then solve the variational problem Find u_h \\in V_h such that a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h using directly the built-in dl.solve method in FEniCS. NOTE: As an alternative one can also assemble the finite element matrix A and the right hand side b that stems from the discretization of a and L , and then solve the linear system A {\\rm u} = {\\rm b}, where {\\rm u} is the vector collecting the coefficients of the finite element expasion of u_h , the entries of the matrix A are such that A_{ij} = a(\\phi_j, \\phi_i) , the entries of the right hand side b are such that b_i = L(\\phi_i) . u = dl.TrialFunction(Vh) v = dl.TestFunction(Vh) a = ufl.inner(ufl.grad(u), ufl.grad(v))*ufl.dx L = f*v*ufl.dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = dl.Function(Vh) #dl.solve(a == L, uh, bcs=bcs) A, b = dl.assemble_system(a,L, bcs=bcs) dl.solve(A, uh.vector(), b, \"cg\") nb.plot(uh) <matplotlib.collections.TriMesh at 0x11b389ac8>","title":"5. Define and solve the variational problem"},{"location":"tutorials_v3.0.0/1_FEniCS101/#6-compute-the-discretization-error","text":"For this problem, the exact solution is known. We can therefore compute the following norms of the discretization error (i.e. the difference between the finite element solution u_h and the exact solution u_{\\rm ex} ) \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, and \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. u_e = dl.Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5) grad_u_e = dl.Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5) err_L2 = math.sqrt( dl.assemble( (uh-u_e)**2*ufl.dx ) ) err_grad = math.sqrt( dl.assemble( ufl.inner(ufl.grad(uh) - grad_u_e, ufl.grad(uh) - grad_u_e)*ufl.dx ) ) err_H1 = math.sqrt( err_L2**2 + err_grad**2) print (\"|| u_h - u_e ||_L2 = \", err_L2) print (\"|| u_h - u_e ||_H1 = \", err_H1) || u_h - u_e ||_L2 = 0.008805176703139152 || u_h - u_e ||_H1 = 0.3967189507839944","title":"6. Compute the discretization error"},{"location":"tutorials_v3.0.0/1_FEniCS101/#7-convergence-of-the-finite-element-method","text":"We now verify numerically a well-known convergence result for the finite element method. Let s denote the polynomial degree of the finite element space, and assume that the solution u_{\\rm ex} is at least in H^{s+1}(\\Omega) . Then we have \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. In the code below, the function compute(n, degree) solves the PDE using a mesh with n elements in each direction and finite element spaces of polynomial order degree . The figure below shows the discretization errors in the H^1 and L^2 as a function of the mesh size h ( h = \\frac{1}{n} ) for piecewise linear (P1, s=1 ) and piecewise quadratic (P2, s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular: for piecewise linear finite element P1 we observe first order convergence in the H^1 -norm and second order convergence in the L^2 -norm; for piecewise quadratic finite element P2 we observe second order convergence in the H^1 -norm and third order convergence in the L^2 -norm. def compute(n, degree): mesh = dl.UnitSquareMesh(n, n) Vh = dl.FunctionSpace(mesh, 'Lagrange', degree) boundary_parts = dl.MeshFunction(\"size_t\", mesh,1) boundary_parts.set_all(0) Gamma_top = TopBoundary() Gamma_top.mark(boundary_parts, 1) Gamma_bottom = BottomBoundary() Gamma_bottom.mark(boundary_parts, 2) Gamma_left = LeftBoundary() Gamma_left.mark(boundary_parts, 3) Gamma_right = RightBoundary() Gamma_right.mark(boundary_parts, 4) bcs = [dl.DirichletBC(Vh, u_L, boundary_parts, 3), dl.DirichletBC(Vh, u_R, boundary_parts, 4)] ds = dl.Measure(\"ds\", subdomain_data=boundary_parts) u = dl.TrialFunction(Vh) v = dl.TestFunction(Vh) a = ufl.inner(ufl.grad(u), ufl.grad(v))*ufl.dx L = f*v*ufl.dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2) uh = dl.Function(Vh) dl.solve(a == L, uh, bcs=bcs) err_L2 = math.sqrt( dl.assemble( (uh-u_e)**2*ufl.dx ) ) err_grad = math.sqrt( dl.assemble( ufl.inner(ufl.grad(uh) - grad_u_e, ufl.grad(uh) - grad_u_e)*ufl.dx ) ) err_H1 = math.sqrt( err_L2**2 + err_grad**2) return err_L2, err_H1 nref = 5 n = 8*np.power(2,np.arange(0,nref)) h = 1./n err_L2_P1 = np.zeros(nref) err_H1_P1 = np.zeros(nref) err_L2_P2 = np.zeros(nref) err_H1_P2 = np.zeros(nref) for i in range(nref): err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1) err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2) plt.figure(figsize=(15,5)) plt.subplot(121) plt.loglog(h, err_H1_P1, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P1, '-*b', label = \"L2 error\") plt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label = \"First Order\") plt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label = \"Second Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P1 Finite Element\") plt.legend(loc='lower right') plt.subplot(122) plt.loglog(h, err_H1_P2, '-or', label = \"H1 error\") plt.loglog(h, err_L2_P2, '-*b', label = \"L2 error\") plt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label = \"Second Order\") plt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label = \"Third Order\") plt.xlabel(\"Mesh size h\") plt.ylabel(\"Error\") plt.title(\"P2 Finite Element\") plt.legend(loc='lower right') plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"7. Convergence of the finite element method"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/","text":"Coefficient field inversion in an elliptic partial differential equation We consider the estimation of a coefficient in an elliptic partial differential equation as a model problem. Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity. It can also be interpreted as finding a membrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n , n\\in\\{1,2,3\\} be an open, bounded domain and consider the following problem: \\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx, where u is the solution of \\begin{split} \\quad -\\nabla\\cdot(\\exp(m)\\nabla u) &= f \\text{ in }\\Omega,\\\\ u &= 0 \\text{ on }\\partial\\Omega. \\end{split} Here m\\in U_{ad}:=\\{m\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\} the unknown coefficient field, u_d denotes (possibly noisy) data, f\\in H^{-1}(\\Omega) a given force, and \\gamma\\ge 0 the regularization parameter. The variational (or weak) form of the state equation: Find u\\in H_0^1(\\Omega) such that (\\exp(m)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), where H_0^1(\\Omega) is the space of functions vanishing on \\partial\\Omega with square integrable derivatives. Here, (\\cdot\\,,\\cdot) denotes the L^2 -inner product, i.e, for scalar functions u,v \\in L^2(\\Omega) we denote (u,v) := \\int_\\Omega u(x) v(x) \\,dx. Gradient evaluation: The Lagrangian functional \\mathscr{L}:H_0^1(\\Omega)\\times H^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} is given by \\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) + \\frac{\\gamma}{2}(\\nabla m, \\nabla m) + (\\exp(m)\\nabla u,\\nabla p) - (f,p). Then the gradient of the cost functional \\mathcal{J}(m) with respect to the parameter m is \\mathcal{G}(m)(\\tilde m) := \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where u \\in H_0^1(\\Omega) is the solution of the forward problem, \\mathscr{L}_p(u,m,p)(\\tilde{p}) := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega), and p \\in H_0^1(\\Omega) is the solution of the adjoint problem, \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega). Hessian action: To evaluate the action \\mathcal{H}(m)(\\hat{m}) of the Hessian in a given direction \\hat{m} , we consider variations of the meta-Lagrangian functional \\begin{aligned} \\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\ {} & \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) & \\text{gradient}\\\\ {} & + (\\exp(m)\\nabla u, \\nabla \\hat{p}) - (f,\\hat{p}) & \\text{forward eq}\\\\ {} & + (\\exp(m)\\nabla p, \\nabla \\hat{u}) + (u-u_d,\\hat{u}) & \\text{adjoint eq}. \\end{aligned} Then the action of the Hessian in a given direction \\hat{m} is \\begin{aligned} (\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\ {} & = (\\tilde{m} \\exp(m) \\nabla \\hat{u}, \\nabla{p}) + \\gamma (\\nabla \\hat{m}, \\nabla \\tilde{m}) + (\\tilde{m} \\hat{m} \\exp(m)\\nabla u, \\nabla p) + (\\tilde{m}\\exp(m) \\nabla u, \\nabla \\hat{p}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), \\end{aligned} where u\\in H^1_0(\\Omega) and p \\in H^1_0(\\Omega) are the solution of the forward and adjoint problem, respectively; \\hat{u} \\in H^1_0(\\Omega) is the solution of the incremental forward problem, \\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{p}) := (\\exp(m) \\nabla \\hat{u}, \\nabla \\tilde{p}) + (\\hat{m} \\exp(m) \\nabla u, \\nabla \\tilde p) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega); and \\hat{p} \\in H^1_0(\\Omega) is the solution of the incremental adjoint problem, \\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{u}) := (\\hat{u}, \\tilde{u}) + (\\hat{m} \\exp(m)\\nabla p, \\nabla \\tilde{u}) + (\\exp(m) \\nabla \\tilde u, \\nabla \\hat{p}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega). Inexact Newton-CG: Written in abstract form, the Newton Method computes an update direction \\hat{m}_k by solving the linear system (\\tilde{m}, \\mathcal{H}(m_k)(\\hat{m}_k) ) = -\\mathcal{G}(m_k)(\\tilde m) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where the evaluation of the gradient \\mathcal{G}(m_k) involve the solution u_k and p_k of the forward and adjoint problem (respectively) for m = m_k . Similarly, the Hessian action \\mathcal{H}(m_k)(\\hat{m}_k) requires to additional solve the incremental forward and adjoint problems. Discrete Newton system: \\def\\tu{\\tilde u} \\def\\tm{\\tilde m} \\def\\tp{\\tilde p} \\def\\hu{\\hat u} \\def\\hp{\\hat p} \\def\\hm{\\hat m} \\def\\bu{{\\bf u}} \\def\\bm{{\\bf m}} \\def\\bp{{\\bf p}} \\def\\btu{{\\bf \\tilde u}} \\def\\btm{{\\bf \\tilde m}} \\def\\btp{{\\bf \\tilde p}} \\def\\bhu{{\\bf \\hat u}} \\def\\bhm{{\\bf \\hat m}} \\def\\bhp{{\\bf \\hat p}} \\def\\bg{{\\bf g}} \\def\\bA{{\\bf A}} \\def\\bC{{\\bf C}} \\def\\bH{{\\bf H}} \\def\\bR{{\\bf R}} \\def\\bW{{\\bf W}} Let us denote the vectors corresponding to the discretization of the functions u_k, m_k, p_k by \\bu_k, \\bm_k, \\bp_k and of the functions \\hu_k, \\hm_k, \\hp_k by \\bhu_k, \\bhm_k,\\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system: \\bH_k \\, \\bhm_k = -\\bg_k. The gradient \\bg_k is computed using the following three steps Given \\bm_k we solve the forward problem \\bA_k \\bu_k = {\\bf f}, where \\bA_k \\bu_k stems from the discretization (\\exp(m_k)\\nabla u_k, \\nabla \\tilde{p}) , and {\\bf f} stands for the discretization of the right hand side f . Given \\bm_k and \\bu_k solve the adjoint problem \\bA_k^T \\bp_k = - \\bW_{\\scriptsize\\mbox{uu}}\\,(\\bu_k-\\bu_d) where \\bA_k^T \\bp_k stems from the discretization of (\\exp(m_k)\\nabla \\tilde{u}, \\nabla p_k) , \\bW_{\\scriptsize\\mbox{uu}} is the mass matrix corresponding to the L^2 inner product in the state space, and \\bu_d stems from the data. Define the gradient \\bg_k = \\bR \\bm_k + \\bC_k^T \\bp_k, where \\bR is the matrix stemming from discretization of the regularization operator \\gamma ( \\nabla \\hat{m}, \\nabla \\tilde{m}) , and \\bC_k stems from discretization of the term (\\tilde{m}\\exp(m_k)\\nabla u_k, \\nabla p_k) . Similarly the action of the Hessian \\bH_k \\, \\bhm_k in a direction \\bhm_k (by using the CG algorithm we only need the action of \\bH_k to solve the Newton step) is given by Solve the incremental forward problem \\bA_k \\bhu_k = -\\bC_k \\bhm_k, where \\bC_k \\bm_k stems from discretization of (\\hat{m} \\exp(m_k) \\nabla u_k, \\nabla \\tilde p) . Solve the incremental adjoint problem \\bA_k^T \\bhp_k = -(\\bW_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k), where \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k stems for the discretization of (\\hat{m}_k \\exp(m_k)\\nabla p_k, \\nabla \\tilde{u}) . Define the Hessian action \\bH_k \\, \\bhm = \\underbrace{(\\bR + \\bW_{\\scriptsize\\mbox{mm}})}_{\\text{Hessian of the regularization}} \\bhm + \\underbrace{(\\bC_k^{T}\\bA_k^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bA_k^{-1} \\bC_k - \\bW_{\\scriptsize\\mbox{um}}) - \\bW_{\\scriptsize\\mbox{mu}} \\bA_k^{-1} \\bC_k)}_{\\text{Hessian of the data misfit}}\\;\\bhm. Goals: By the end of this notebook, you should be able to: solve the forward and adjoint Poisson equations understand the inverse method framework visualise and understand the results modify the problem and code Mathematical tools used: Finite element method Derivation of gradiant and Hessian via the adjoint method Inexact Newton-CG Armijo line search List of software used: FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , a python package used for plotting the results Set up Import dependencies import dolfin as dl import ufl import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging import math import matplotlib.pyplot as plt %matplotlib inline logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) Model set up: As in the introduction, the first thing we need to do is set up the numerical model. In this cell, we set the mesh, the finite element functions u, p, g corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization. # create mesh and define function spaces nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vm = dl.FunctionSpace(mesh, 'Lagrange', 1) Vu = dl.FunctionSpace(mesh, 'Lagrange', 2) # The true and inverted parameter mtrue_expression = dl.Expression( 'std::log(2 + 7*(std::pow(std::pow(x[0] - 0.5,2) + std::pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5) mtrue = dl.interpolate(mtrue_expression,Vm) m = dl.interpolate(dl.Expression(\"std::log(2.0)\", degree=1),Vm) # define function for state and adjoint u = dl.Function(Vu) p = dl.Function(Vu) # define Trial and Test Functions u_trial, p_trial, m_trial = dl.TrialFunction(Vu), dl.TrialFunction(Vu), dl.TrialFunction(Vm) u_test, p_test, m_test = dl.TestFunction(Vu), dl.TestFunction(Vu), dl.TestFunction(Vm) # initialize input functions f = dl.Constant(1.0) u0 = dl.Constant(0.0) # plot plt.figure(figsize=(15,5)) nb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on') nb.plot(mtrue,subplot_loc=122, mytitle=\"True parameter field\") plt.show() # set up dirichlet boundary conditions def boundary(x,on_boundary): return on_boundary bc_state = dl.DirichletBC(Vu, u0, boundary) bc_adj = dl.DirichletBC(Vu, dl.Constant(0.), boundary) Set up synthetic observations: Propose a coefficient field m_{\\rm true} shown above The weak form of the pde: Find u\\in H_0^1(\\Omega) such that \\underbrace{(\\exp(m_{\\rm true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) . Perturb the solution: u = u + \\eta , where \\eta \\sim \\mathcal{N}(0, \\sigma) # noise level noise_level = 0.05 # weak form for setting up the synthetic observations a_goal = ufl.inner(ufl.exp(mtrue) * ufl.grad(u_trial), ufl.grad(u_test)) * ufl.dx L_goal = f * u_test * ufl.dx # solve the forward/state problem to generate synthetic observations goal_A, goal_b = dl.assemble_system(a_goal, L_goal, bc_state) utrue = dl.Function(Vu) dl.solve(goal_A, utrue.vector(), goal_b) ud = dl.Function(Vu) ud.assign(utrue) # perturb state solution and create synthetic measurements ud # ud = u + ||u||/SNR * random.normal MAX = ud.vector().norm(\"linf\") noise = dl.Vector() goal_A.init_vector(noise,1) parRandom.normal(noise_level * MAX, noise) bc_adj.apply(noise) ud.vector().axpy(1., noise) # plot nb.multi1_plot([utrue, ud], [\"State solution with mtrue\", \"Synthetic observations\"]) plt.show() The cost function evaluation: J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text{misfit} } + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text{reg}} In the code below, \\bW and \\bR are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively. # regularization parameter gamma = 1e-8 # weak for for setting up the misfit and regularization compoment of the cost W_equ = ufl.inner(u_trial, u_test) * ufl.dx R_equ = gamma * ufl.inner(ufl.grad(m_trial), ufl.grad(m_test)) * ufl.dx W = dl.assemble(W_equ) R = dl.assemble(R_equ) # refine cost function def cost(u, ud, m, W, R): diff = u.vector() - ud.vector() reg = 0.5 * m.vector().inner(R*m.vector() ) misfit = 0.5 * diff.inner(W * diff) return [reg + misfit, misfit, reg] Setting up the state equations, right hand side for the adjoint and the necessary matrices: # weak form for setting up the state equation a_state = ufl.inner(ufl.exp(m) * ufl.grad(u_trial), ufl.grad(u_test)) * ufl.dx L_state = f * u_test * ufl.dx # weak form for setting up the adjoint equation a_adj = ufl.inner(ufl.exp(m) * ufl.grad(p_trial), ufl.grad(p_test)) * ufl.dx L_adj = -ufl.inner(u - ud, p_test) * ufl.dx # weak form for setting up matrices Wum_equ = ufl.inner(ufl.exp(m) * m_trial * ufl.grad(p_test), ufl.grad(p)) * ufl.dx C_equ = ufl.inner(ufl.exp(m) * m_trial * ufl.grad(u), ufl.grad(u_test)) * ufl.dx Wmm_equ = ufl.inner(ufl.exp(m) * m_trial * m_test * ufl.grad(u), ufl.grad(p)) * ufl.dx M_equ = ufl.inner(m_trial, m_test) * ufl.dx # assemble matrix M M = dl.assemble(M_equ) Initial guess We solve the state equation and compute the cost functional for the initial guess of the parameter m_ini # solve state equation state_A, state_b = dl.assemble_system (a_state, L_state, bc_state) dl.solve (state_A, u.vector(), state_b) # evaluate cost [cost_old, misfit_old, reg_old] = cost(u, ud, m, W, R) # plot plt.figure(figsize=(15,5)) nb.plot(m,subplot_loc=121, mytitle=\"m_ini\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max()) nb.plot(u,subplot_loc=122, mytitle=\"u(m_ini)\") plt.show() The reduced Hessian apply to a vector \\bhm : Here we describe how to apply the reduced Hessian operator to a vector \\bhm . For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined. For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm. The Hessian apply reads: \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bhu + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm) & \\text{adjoint}\\\\ \\bH \\bhm &= (\\bR + \\bW_{\\scriptsize\\mbox{mm}})\\bhm + \\bC^T \\bhp + \\bW_{\\scriptsize\\mbox{mu}} \\bhu. \\end{align} The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm , \\bW_{\\scriptsize\\mbox{mm}}\\bf \\bhm , and \\bW_{\\scriptsize\\mbox{mu}} \\bhu : \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bf \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} \\bW_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\ \\bH_{\\rm GN} \\bhm &= \\bR \\bhm + \\bC^T \\bhp. \\end{align} # Class HessianOperator to perform Hessian apply to a vector class HessianOperator(): cgiter = 0 def __init__(self, R, Wmm, C, A, adj_A, W, Wum, gauss_newton_approx=False): self.R = R self.Wmm = Wmm self.C = C self.A = A self.adj_A = adj_A self.W = W self.Wum = Wum self.gauss_newton_approx = gauss_newton_approx # incremental state self.du = dl.Vector() self.A.init_vector(self.du,0) # incremental adjoint self.dp = dl.Vector() self.adj_A.init_vector(self.dp,0) # auxiliary vectors self.CT_dp = dl.Vector() self.C.init_vector(self.CT_dp, 1) self.Wum_du = dl.Vector() self.Wum.init_vector(self.Wum_du, 1) def init_vector(self, v, dim): self.R.init_vector(v,dim) # Hessian performed on v, output as generic vector y def mult(self, v, y): self.cgiter += 1 y.zero() if self.gauss_newton_approx: self.mult_GaussNewton(v,y) else: self.mult_Newton(v,y) # define (Gauss-Newton) Hessian apply H * v def mult_GaussNewton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) dl.solve (self.A, self.du, rhs) # incremental adjoint rhs = - (self.W * self.du) bc_adj.apply(rhs) dl.solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1, self.CT_dp) # define (Newton) Hessian apply H * v def mult_Newton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) dl.solve (self.A, self.du, rhs) # incremental adjoint rhs = -(self.W * self.du) - self.Wum * v bc_adj.apply(rhs) dl.solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) y.axpy(1., self.Wmm*v) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1., self.CT_dp) self.Wum.transpmult(self.du, self.Wum_du) y.axpy(1., self.Wum_du) The inexact Newton-CG optimization with Armijo line search: We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search. The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ). First, we compute the gradient by solving the state and adjoint equation for the current parameter m , and then substituing the current state u , parameter m and adjoint p variables in the weak form expression of the gradient: (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}\\nabla u, \\nabla p). Then, we compute the Newton direction \\hat m by iteratively solving \\mathcal{H} {\\hat m} = -g . The Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug (to avoid negative curvature) criteria. Finally, the Armijo line search uses backtracking to find \\alpha such that a sufficient reduction in the cost functional is achieved. More specifically, we use backtracking to find \\alpha such that: J( m + \\alpha \\hat m ) \\leq J(m) + \\alpha c_{\\rm armijo} (\\hat m,g). # define parameters for the optimization tol = 1e-8 c = 1e-4 maxiter = 12 plot_on = False # initialize iter counters iter = 1 total_cg_iter = 0 converged = False # initializations g, m_delta = dl.Vector(), dl.Vector() R.init_vector(m_delta,0) R.init_vector(g,0) m_prev = dl.Function(Vm) print (\"Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg\") while iter < maxiter and not converged: # assemble matrix C C = dl.assemble(C_equ) # solve the adoint problem adjoint_A, adjoint_RHS = dl.assemble_system(a_adj, L_adj, bc_adj) dl.solve(adjoint_A, p.vector(), adjoint_RHS) # assemble W_ua and R Wum = dl.assemble (Wum_equ) Wmm = dl.assemble (Wmm_equ) # evaluate the gradient CT_p = dl.Vector() C.init_vector(CT_p,1) C.transpmult(p.vector(), CT_p) MG = CT_p + R * m.vector() dl.solve(M, g, MG) # calculate the norm of the gradient grad2 = g.inner(MG) gradnorm = math.sqrt(grad2) # set the CG tolerance (use Eisenstat\u2013Walker termination criterion) if iter == 1: gradnorm_ini = gradnorm tolcg = min(0.5, math.sqrt(gradnorm/gradnorm_ini)) # define the Hessian apply operator (with preconditioner) Hess_Apply = HessianOperator(R, Wmm, C, state_A, adjoint_A, W, Wum, gauss_newton_approx=(iter<6) ) P = R + gamma * M Psolver = dl.PETScKrylovSolver(\"cg\", amg_method()) Psolver.set_operator(P) solver = CGSolverSteihaug() solver.set_operator(Hess_Apply) solver.set_preconditioner(Psolver) solver.parameters[\"rel_tolerance\"] = tolcg solver.parameters[\"zero_initial_guess\"] = True solver.parameters[\"print_level\"] = -1 # solve the Newton system H a_delta = - MG solver.solve(m_delta, -MG) total_cg_iter += Hess_Apply.cgiter # linesearch alpha = 1 descent = 0 no_backtrack = 0 m_prev.assign(m) while descent == 0 and no_backtrack < 10: m.vector().axpy(alpha, m_delta ) # solve the state/forward problem state_A, state_b = dl.assemble_system(a_state, L_state, bc_state) dl.solve(state_A, u.vector(), state_b) # evaluate cost [cost_new, misfit_new, reg_new] = cost(u, ud, m, W, R) # check if Armijo conditions are satisfied if cost_new < cost_old + alpha * c * MG.inner(m_delta): cost_old = cost_new descent = 1 else: no_backtrack += 1 alpha *= 0.5 m.assign(m_prev) # reset a # calculate sqrt(-G * D) graddir = math.sqrt(- MG.inner(m_delta) ) sp = \"\" print( \"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\ (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\ graddir, sp, gradnorm, sp, alpha, sp, tolcg) ) if plot_on: nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False) plt.show() # check for convergence if gradnorm < tol and iter > 1: converged = True print( \"Newton's method converged in \",iter,\" iterations\") print( \"Total number of CG iterations: \", total_cg_iter) iter += 1 if not converged: print( \"Newton's method did not converge in \", maxiter, \" iterations\") Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg 1 1 1.12916e-05 1.12916e-05 1.34150e-11 1.56616e-02 3.79614e-04 1.00 5.000e-01 2 1 7.83206e-07 7.83170e-07 3.68430e-11 4.68686e-03 5.35269e-05 1.00 3.755e-01 3 1 3.12292e-07 3.12243e-07 4.92462e-11 9.73515e-04 7.14570e-06 1.00 1.372e-01 4 6 1.91985e-07 1.61584e-07 3.04009e-08 4.54547e-04 1.00594e-06 1.00 5.148e-02 5 1 1.86421e-07 1.56004e-07 3.04171e-08 1.05501e-04 6.25400e-07 1.00 4.059e-02 6 13 1.80334e-07 1.36992e-07 4.33419e-08 1.12181e-04 2.14958e-07 1.00 2.380e-02 7 5 1.80267e-07 1.38198e-07 4.20693e-08 1.15139e-05 3.92325e-08 1.00 1.017e-02 8 15 1.80266e-07 1.38243e-07 4.20235e-08 1.48892e-06 3.20470e-09 1.00 2.906e-03 Newton's method converged in 8 iterations Total number of CG iterations: 43 nb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"]) nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Coefficient field inversion in an elliptic partial differential equation"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#coefficient-field-inversion-in-an-elliptic-partial-differential-equation","text":"We consider the estimation of a coefficient in an elliptic partial differential equation as a model problem. Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity. It can also be interpreted as finding a membrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n , n\\in\\{1,2,3\\} be an open, bounded domain and consider the following problem: \\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx, where u is the solution of \\begin{split} \\quad -\\nabla\\cdot(\\exp(m)\\nabla u) &= f \\text{ in }\\Omega,\\\\ u &= 0 \\text{ on }\\partial\\Omega. \\end{split} Here m\\in U_{ad}:=\\{m\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\} the unknown coefficient field, u_d denotes (possibly noisy) data, f\\in H^{-1}(\\Omega) a given force, and \\gamma\\ge 0 the regularization parameter.","title":"Coefficient field inversion in an elliptic partial differential equation"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#the-variational-or-weak-form-of-the-state-equation","text":"Find u\\in H_0^1(\\Omega) such that (\\exp(m)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), where H_0^1(\\Omega) is the space of functions vanishing on \\partial\\Omega with square integrable derivatives. Here, (\\cdot\\,,\\cdot) denotes the L^2 -inner product, i.e, for scalar functions u,v \\in L^2(\\Omega) we denote (u,v) := \\int_\\Omega u(x) v(x) \\,dx.","title":"The variational (or weak) form of the state equation:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#gradient-evaluation","text":"The Lagrangian functional \\mathscr{L}:H_0^1(\\Omega)\\times H^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} is given by \\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) + \\frac{\\gamma}{2}(\\nabla m, \\nabla m) + (\\exp(m)\\nabla u,\\nabla p) - (f,p). Then the gradient of the cost functional \\mathcal{J}(m) with respect to the parameter m is \\mathcal{G}(m)(\\tilde m) := \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where u \\in H_0^1(\\Omega) is the solution of the forward problem, \\mathscr{L}_p(u,m,p)(\\tilde{p}) := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega), and p \\in H_0^1(\\Omega) is the solution of the adjoint problem, \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).","title":"Gradient evaluation:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#hessian-action","text":"To evaluate the action \\mathcal{H}(m)(\\hat{m}) of the Hessian in a given direction \\hat{m} , we consider variations of the meta-Lagrangian functional \\begin{aligned} \\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\ {} & \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) & \\text{gradient}\\\\ {} & + (\\exp(m)\\nabla u, \\nabla \\hat{p}) - (f,\\hat{p}) & \\text{forward eq}\\\\ {} & + (\\exp(m)\\nabla p, \\nabla \\hat{u}) + (u-u_d,\\hat{u}) & \\text{adjoint eq}. \\end{aligned} Then the action of the Hessian in a given direction \\hat{m} is \\begin{aligned} (\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\ {} & = (\\tilde{m} \\exp(m) \\nabla \\hat{u}, \\nabla{p}) + \\gamma (\\nabla \\hat{m}, \\nabla \\tilde{m}) + (\\tilde{m} \\hat{m} \\exp(m)\\nabla u, \\nabla p) + (\\tilde{m}\\exp(m) \\nabla u, \\nabla \\hat{p}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), \\end{aligned} where u\\in H^1_0(\\Omega) and p \\in H^1_0(\\Omega) are the solution of the forward and adjoint problem, respectively; \\hat{u} \\in H^1_0(\\Omega) is the solution of the incremental forward problem, \\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{p}) := (\\exp(m) \\nabla \\hat{u}, \\nabla \\tilde{p}) + (\\hat{m} \\exp(m) \\nabla u, \\nabla \\tilde p) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega); and \\hat{p} \\in H^1_0(\\Omega) is the solution of the incremental adjoint problem, \\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{u}) := (\\hat{u}, \\tilde{u}) + (\\hat{m} \\exp(m)\\nabla p, \\nabla \\tilde{u}) + (\\exp(m) \\nabla \\tilde u, \\nabla \\hat{p}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).","title":"Hessian action:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#inexact-newton-cg","text":"Written in abstract form, the Newton Method computes an update direction \\hat{m}_k by solving the linear system (\\tilde{m}, \\mathcal{H}(m_k)(\\hat{m}_k) ) = -\\mathcal{G}(m_k)(\\tilde m) \\quad \\forall \\tilde{m} \\in H^1(\\Omega), where the evaluation of the gradient \\mathcal{G}(m_k) involve the solution u_k and p_k of the forward and adjoint problem (respectively) for m = m_k . Similarly, the Hessian action \\mathcal{H}(m_k)(\\hat{m}_k) requires to additional solve the incremental forward and adjoint problems.","title":"Inexact Newton-CG:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#discrete-newton-system","text":"\\def\\tu{\\tilde u} \\def\\tm{\\tilde m} \\def\\tp{\\tilde p} \\def\\hu{\\hat u} \\def\\hp{\\hat p} \\def\\hm{\\hat m} \\def\\bu{{\\bf u}} \\def\\bm{{\\bf m}} \\def\\bp{{\\bf p}} \\def\\btu{{\\bf \\tilde u}} \\def\\btm{{\\bf \\tilde m}} \\def\\btp{{\\bf \\tilde p}} \\def\\bhu{{\\bf \\hat u}} \\def\\bhm{{\\bf \\hat m}} \\def\\bhp{{\\bf \\hat p}} \\def\\bg{{\\bf g}} \\def\\bA{{\\bf A}} \\def\\bC{{\\bf C}} \\def\\bH{{\\bf H}} \\def\\bR{{\\bf R}} \\def\\bW{{\\bf W}} Let us denote the vectors corresponding to the discretization of the functions u_k, m_k, p_k by \\bu_k, \\bm_k, \\bp_k and of the functions \\hu_k, \\hm_k, \\hp_k by \\bhu_k, \\bhm_k,\\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system: \\bH_k \\, \\bhm_k = -\\bg_k. The gradient \\bg_k is computed using the following three steps Given \\bm_k we solve the forward problem \\bA_k \\bu_k = {\\bf f}, where \\bA_k \\bu_k stems from the discretization (\\exp(m_k)\\nabla u_k, \\nabla \\tilde{p}) , and {\\bf f} stands for the discretization of the right hand side f . Given \\bm_k and \\bu_k solve the adjoint problem \\bA_k^T \\bp_k = - \\bW_{\\scriptsize\\mbox{uu}}\\,(\\bu_k-\\bu_d) where \\bA_k^T \\bp_k stems from the discretization of (\\exp(m_k)\\nabla \\tilde{u}, \\nabla p_k) , \\bW_{\\scriptsize\\mbox{uu}} is the mass matrix corresponding to the L^2 inner product in the state space, and \\bu_d stems from the data. Define the gradient \\bg_k = \\bR \\bm_k + \\bC_k^T \\bp_k, where \\bR is the matrix stemming from discretization of the regularization operator \\gamma ( \\nabla \\hat{m}, \\nabla \\tilde{m}) , and \\bC_k stems from discretization of the term (\\tilde{m}\\exp(m_k)\\nabla u_k, \\nabla p_k) . Similarly the action of the Hessian \\bH_k \\, \\bhm_k in a direction \\bhm_k (by using the CG algorithm we only need the action of \\bH_k to solve the Newton step) is given by Solve the incremental forward problem \\bA_k \\bhu_k = -\\bC_k \\bhm_k, where \\bC_k \\bm_k stems from discretization of (\\hat{m} \\exp(m_k) \\nabla u_k, \\nabla \\tilde p) . Solve the incremental adjoint problem \\bA_k^T \\bhp_k = -(\\bW_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k), where \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k stems for the discretization of (\\hat{m}_k \\exp(m_k)\\nabla p_k, \\nabla \\tilde{u}) . Define the Hessian action \\bH_k \\, \\bhm = \\underbrace{(\\bR + \\bW_{\\scriptsize\\mbox{mm}})}_{\\text{Hessian of the regularization}} \\bhm + \\underbrace{(\\bC_k^{T}\\bA_k^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bA_k^{-1} \\bC_k - \\bW_{\\scriptsize\\mbox{um}}) - \\bW_{\\scriptsize\\mbox{mu}} \\bA_k^{-1} \\bC_k)}_{\\text{Hessian of the data misfit}}\\;\\bhm.","title":"Discrete Newton system:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#goals","text":"By the end of this notebook, you should be able to: solve the forward and adjoint Poisson equations understand the inverse method framework visualise and understand the results modify the problem and code","title":"Goals:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#mathematical-tools-used","text":"Finite element method Derivation of gradiant and Hessian via the adjoint method Inexact Newton-CG Armijo line search","title":"Mathematical tools used:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#list-of-software-used","text":"FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , a python package used for plotting the results","title":"List of software used:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#set-up","text":"","title":"Set up"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#import-dependencies","text":"import dolfin as dl import ufl import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging import math import matplotlib.pyplot as plt %matplotlib inline logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"Import dependencies"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#model-set-up","text":"As in the introduction, the first thing we need to do is set up the numerical model. In this cell, we set the mesh, the finite element functions u, p, g corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization. # create mesh and define function spaces nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vm = dl.FunctionSpace(mesh, 'Lagrange', 1) Vu = dl.FunctionSpace(mesh, 'Lagrange', 2) # The true and inverted parameter mtrue_expression = dl.Expression( 'std::log(2 + 7*(std::pow(std::pow(x[0] - 0.5,2) + std::pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5) mtrue = dl.interpolate(mtrue_expression,Vm) m = dl.interpolate(dl.Expression(\"std::log(2.0)\", degree=1),Vm) # define function for state and adjoint u = dl.Function(Vu) p = dl.Function(Vu) # define Trial and Test Functions u_trial, p_trial, m_trial = dl.TrialFunction(Vu), dl.TrialFunction(Vu), dl.TrialFunction(Vm) u_test, p_test, m_test = dl.TestFunction(Vu), dl.TestFunction(Vu), dl.TestFunction(Vm) # initialize input functions f = dl.Constant(1.0) u0 = dl.Constant(0.0) # plot plt.figure(figsize=(15,5)) nb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on') nb.plot(mtrue,subplot_loc=122, mytitle=\"True parameter field\") plt.show() # set up dirichlet boundary conditions def boundary(x,on_boundary): return on_boundary bc_state = dl.DirichletBC(Vu, u0, boundary) bc_adj = dl.DirichletBC(Vu, dl.Constant(0.), boundary)","title":"Model set up:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#set-up-synthetic-observations","text":"Propose a coefficient field m_{\\rm true} shown above The weak form of the pde: Find u\\in H_0^1(\\Omega) such that \\underbrace{(\\exp(m_{\\rm true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) . Perturb the solution: u = u + \\eta , where \\eta \\sim \\mathcal{N}(0, \\sigma) # noise level noise_level = 0.05 # weak form for setting up the synthetic observations a_goal = ufl.inner(ufl.exp(mtrue) * ufl.grad(u_trial), ufl.grad(u_test)) * ufl.dx L_goal = f * u_test * ufl.dx # solve the forward/state problem to generate synthetic observations goal_A, goal_b = dl.assemble_system(a_goal, L_goal, bc_state) utrue = dl.Function(Vu) dl.solve(goal_A, utrue.vector(), goal_b) ud = dl.Function(Vu) ud.assign(utrue) # perturb state solution and create synthetic measurements ud # ud = u + ||u||/SNR * random.normal MAX = ud.vector().norm(\"linf\") noise = dl.Vector() goal_A.init_vector(noise,1) parRandom.normal(noise_level * MAX, noise) bc_adj.apply(noise) ud.vector().axpy(1., noise) # plot nb.multi1_plot([utrue, ud], [\"State solution with mtrue\", \"Synthetic observations\"]) plt.show()","title":"Set up synthetic observations:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#the-cost-function-evaluation","text":"J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text{misfit} } + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text{reg}} In the code below, \\bW and \\bR are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively. # regularization parameter gamma = 1e-8 # weak for for setting up the misfit and regularization compoment of the cost W_equ = ufl.inner(u_trial, u_test) * ufl.dx R_equ = gamma * ufl.inner(ufl.grad(m_trial), ufl.grad(m_test)) * ufl.dx W = dl.assemble(W_equ) R = dl.assemble(R_equ) # refine cost function def cost(u, ud, m, W, R): diff = u.vector() - ud.vector() reg = 0.5 * m.vector().inner(R*m.vector() ) misfit = 0.5 * diff.inner(W * diff) return [reg + misfit, misfit, reg]","title":"The cost function evaluation:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#setting-up-the-state-equations-right-hand-side-for-the-adjoint-and-the-necessary-matrices","text":"# weak form for setting up the state equation a_state = ufl.inner(ufl.exp(m) * ufl.grad(u_trial), ufl.grad(u_test)) * ufl.dx L_state = f * u_test * ufl.dx # weak form for setting up the adjoint equation a_adj = ufl.inner(ufl.exp(m) * ufl.grad(p_trial), ufl.grad(p_test)) * ufl.dx L_adj = -ufl.inner(u - ud, p_test) * ufl.dx # weak form for setting up matrices Wum_equ = ufl.inner(ufl.exp(m) * m_trial * ufl.grad(p_test), ufl.grad(p)) * ufl.dx C_equ = ufl.inner(ufl.exp(m) * m_trial * ufl.grad(u), ufl.grad(u_test)) * ufl.dx Wmm_equ = ufl.inner(ufl.exp(m) * m_trial * m_test * ufl.grad(u), ufl.grad(p)) * ufl.dx M_equ = ufl.inner(m_trial, m_test) * ufl.dx # assemble matrix M M = dl.assemble(M_equ)","title":"Setting up the state equations, right hand side for the adjoint and the necessary matrices:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#initial-guess","text":"We solve the state equation and compute the cost functional for the initial guess of the parameter m_ini # solve state equation state_A, state_b = dl.assemble_system (a_state, L_state, bc_state) dl.solve (state_A, u.vector(), state_b) # evaluate cost [cost_old, misfit_old, reg_old] = cost(u, ud, m, W, R) # plot plt.figure(figsize=(15,5)) nb.plot(m,subplot_loc=121, mytitle=\"m_ini\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max()) nb.plot(u,subplot_loc=122, mytitle=\"u(m_ini)\") plt.show()","title":"Initial guess"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#the-reduced-hessian-apply-to-a-vector-bhm","text":"Here we describe how to apply the reduced Hessian operator to a vector \\bhm . For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined. For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm. The Hessian apply reads: \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bhu + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm) & \\text{adjoint}\\\\ \\bH \\bhm &= (\\bR + \\bW_{\\scriptsize\\mbox{mm}})\\bhm + \\bC^T \\bhp + \\bW_{\\scriptsize\\mbox{mu}} \\bhu. \\end{align} The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm , \\bW_{\\scriptsize\\mbox{mm}}\\bf \\bhm , and \\bW_{\\scriptsize\\mbox{mu}} \\bhu : \\begin{align} \\bhu &= -\\bA^{-1} \\bC \\bf \\bhm\\, & \\text{linearized forward}\\\\ \\bhp &= -\\bA^{-T} \\bW_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\ \\bH_{\\rm GN} \\bhm &= \\bR \\bhm + \\bC^T \\bhp. \\end{align} # Class HessianOperator to perform Hessian apply to a vector class HessianOperator(): cgiter = 0 def __init__(self, R, Wmm, C, A, adj_A, W, Wum, gauss_newton_approx=False): self.R = R self.Wmm = Wmm self.C = C self.A = A self.adj_A = adj_A self.W = W self.Wum = Wum self.gauss_newton_approx = gauss_newton_approx # incremental state self.du = dl.Vector() self.A.init_vector(self.du,0) # incremental adjoint self.dp = dl.Vector() self.adj_A.init_vector(self.dp,0) # auxiliary vectors self.CT_dp = dl.Vector() self.C.init_vector(self.CT_dp, 1) self.Wum_du = dl.Vector() self.Wum.init_vector(self.Wum_du, 1) def init_vector(self, v, dim): self.R.init_vector(v,dim) # Hessian performed on v, output as generic vector y def mult(self, v, y): self.cgiter += 1 y.zero() if self.gauss_newton_approx: self.mult_GaussNewton(v,y) else: self.mult_Newton(v,y) # define (Gauss-Newton) Hessian apply H * v def mult_GaussNewton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) dl.solve (self.A, self.du, rhs) # incremental adjoint rhs = - (self.W * self.du) bc_adj.apply(rhs) dl.solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1, self.CT_dp) # define (Newton) Hessian apply H * v def mult_Newton(self, v, y): # incremental forward rhs = -(self.C * v) bc_adj.apply(rhs) dl.solve (self.A, self.du, rhs) # incremental adjoint rhs = -(self.W * self.du) - self.Wum * v bc_adj.apply(rhs) dl.solve (self.adj_A, self.dp, rhs) # Reg/Prior term self.R.mult(v,y) y.axpy(1., self.Wmm*v) # Misfit term self.C.transpmult(self.dp, self.CT_dp) y.axpy(1., self.CT_dp) self.Wum.transpmult(self.du, self.Wum_du) y.axpy(1., self.Wum_du)","title":"The reduced Hessian apply to a vector \\bhm:"},{"location":"tutorials_v3.0.0/2_PoissonDeterministic/#the-inexact-newton-cg-optimization-with-armijo-line-search","text":"We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search. The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ). First, we compute the gradient by solving the state and adjoint equation for the current parameter m , and then substituing the current state u , parameter m and adjoint p variables in the weak form expression of the gradient: (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}\\nabla u, \\nabla p). Then, we compute the Newton direction \\hat m by iteratively solving \\mathcal{H} {\\hat m} = -g . The Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug (to avoid negative curvature) criteria. Finally, the Armijo line search uses backtracking to find \\alpha such that a sufficient reduction in the cost functional is achieved. More specifically, we use backtracking to find \\alpha such that: J( m + \\alpha \\hat m ) \\leq J(m) + \\alpha c_{\\rm armijo} (\\hat m,g). # define parameters for the optimization tol = 1e-8 c = 1e-4 maxiter = 12 plot_on = False # initialize iter counters iter = 1 total_cg_iter = 0 converged = False # initializations g, m_delta = dl.Vector(), dl.Vector() R.init_vector(m_delta,0) R.init_vector(g,0) m_prev = dl.Function(Vm) print (\"Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg\") while iter < maxiter and not converged: # assemble matrix C C = dl.assemble(C_equ) # solve the adoint problem adjoint_A, adjoint_RHS = dl.assemble_system(a_adj, L_adj, bc_adj) dl.solve(adjoint_A, p.vector(), adjoint_RHS) # assemble W_ua and R Wum = dl.assemble (Wum_equ) Wmm = dl.assemble (Wmm_equ) # evaluate the gradient CT_p = dl.Vector() C.init_vector(CT_p,1) C.transpmult(p.vector(), CT_p) MG = CT_p + R * m.vector() dl.solve(M, g, MG) # calculate the norm of the gradient grad2 = g.inner(MG) gradnorm = math.sqrt(grad2) # set the CG tolerance (use Eisenstat\u2013Walker termination criterion) if iter == 1: gradnorm_ini = gradnorm tolcg = min(0.5, math.sqrt(gradnorm/gradnorm_ini)) # define the Hessian apply operator (with preconditioner) Hess_Apply = HessianOperator(R, Wmm, C, state_A, adjoint_A, W, Wum, gauss_newton_approx=(iter<6) ) P = R + gamma * M Psolver = dl.PETScKrylovSolver(\"cg\", amg_method()) Psolver.set_operator(P) solver = CGSolverSteihaug() solver.set_operator(Hess_Apply) solver.set_preconditioner(Psolver) solver.parameters[\"rel_tolerance\"] = tolcg solver.parameters[\"zero_initial_guess\"] = True solver.parameters[\"print_level\"] = -1 # solve the Newton system H a_delta = - MG solver.solve(m_delta, -MG) total_cg_iter += Hess_Apply.cgiter # linesearch alpha = 1 descent = 0 no_backtrack = 0 m_prev.assign(m) while descent == 0 and no_backtrack < 10: m.vector().axpy(alpha, m_delta ) # solve the state/forward problem state_A, state_b = dl.assemble_system(a_state, L_state, bc_state) dl.solve(state_A, u.vector(), state_b) # evaluate cost [cost_new, misfit_new, reg_new] = cost(u, ud, m, W, R) # check if Armijo conditions are satisfied if cost_new < cost_old + alpha * c * MG.inner(m_delta): cost_old = cost_new descent = 1 else: no_backtrack += 1 alpha *= 0.5 m.assign(m_prev) # reset a # calculate sqrt(-G * D) graddir = math.sqrt(- MG.inner(m_delta) ) sp = \"\" print( \"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\ (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\ graddir, sp, gradnorm, sp, alpha, sp, tolcg) ) if plot_on: nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False) plt.show() # check for convergence if gradnorm < tol and iter > 1: converged = True print( \"Newton's method converged in \",iter,\" iterations\") print( \"Total number of CG iterations: \", total_cg_iter) iter += 1 if not converged: print( \"Newton's method did not converge in \", maxiter, \" iterations\") Nit CGit cost misfit reg sqrt(-G*D) ||grad|| alpha tolcg 1 1 1.12916e-05 1.12916e-05 1.34150e-11 1.56616e-02 3.79614e-04 1.00 5.000e-01 2 1 7.83206e-07 7.83170e-07 3.68430e-11 4.68686e-03 5.35269e-05 1.00 3.755e-01 3 1 3.12292e-07 3.12243e-07 4.92462e-11 9.73515e-04 7.14570e-06 1.00 1.372e-01 4 6 1.91985e-07 1.61584e-07 3.04009e-08 4.54547e-04 1.00594e-06 1.00 5.148e-02 5 1 1.86421e-07 1.56004e-07 3.04171e-08 1.05501e-04 6.25400e-07 1.00 4.059e-02 6 13 1.80334e-07 1.36992e-07 4.33419e-08 1.12181e-04 2.14958e-07 1.00 2.380e-02 7 5 1.80267e-07 1.38198e-07 4.20693e-08 1.15139e-05 3.92325e-08 1.00 1.017e-02 8 15 1.80266e-07 1.38243e-07 4.20235e-08 1.48892e-06 3.20470e-09 1.00 2.906e-03 Newton's method converged in 8 iterations Total number of CG iterations: 43 nb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"]) nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"The inexact Newton-CG optimization with Armijo line search:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/","text":"\\def\\data{ {\\bf d}_\\rm{obs}} \\def\\vec{\\bf} \\def\\m{ {\\bf m}} \\def\\map{{\\bf m}_{\\text{MAP}}} \\def\\postcov{{\\bf \\Gamma}_{\\text{post}}} \\def\\prcov{{\\bf \\Gamma}_{\\text{prior}}} \\def\\matrix{\\bf} \\def\\Hmisfit{{\\bf H}_{\\text{misfit}}} \\def\\HT{{\\tilde{\\bf H}}_{\\text{misfit}}} \\def\\diag{\\operatorname{diag}} \\def\\Vr{{\\matrix V}_r} \\def\\Wr{{\\matrix W}_r} \\def\\Ir{{\\matrix I}_r} \\def\\Dr{{\\matrix D}_r} \\def\\H{{\\matrix H} } Bayesian quantification of parameter uncertainty: Estimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by an elliptic PDE via the Bayesian inference framework. Hence, we state the inverse problem as a problem of statistical inference over the space of uncertain parameters, which are to be inferred from data and a physical model. The resulting solution to the statistical inverse problem is a posterior distribution that assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true'' parameter field that gave rise to the observed data. For simplicity, in what follows we give finite-dimensional expressions (i.e., after discretization of the parameter space) for the Bayesian formulation of the inverse problem. Bayes' Theorem: The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m) over the parameter space, which encodes any knowledge or assumptions about the parameter space that we may wish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\data \\; | \\; \\m) , which explicitly represents the probability that a given set of parameters \\m might give rise to the observed data \\data \\in \\mathbb{R}^m , namely: \\begin{align} \\pi_{\\text{post}}(\\m | \\data) \\propto \\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m). \\end{align} Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions. Gaussian prior and noise: The prior: We consider a Gaussian prior with mean {\\vec m}_{\\text{prior}} and covariance \\prcov . The covariance is given by the discretization of the inverse of differential operator \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where \\gamma , \\delta > 0 control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem. The likelihood: \\data = {\\bf f}(\\m) + {\\bf e }, \\;\\;\\; {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} ) \\pi_{\\text{like}}(\\data \\; | \\; \\m) = \\exp \\left( - \\tfrac{1}{2} ({\\bf f}(\\m) - \\data)^T {\\bf \\Gamma}_{\\text{noise}}^{-1} ({\\bf f}(\\m) - \\data)\\right) Here {\\bf f} is the parameter-to-observable map that takes a parameter vector \\m and maps it to the space observation vector \\data . The posterior: \\pi_{\\text{post}}(\\m \\; | \\; \\data) \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(\\m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\prcov^{-1}} \\right) The Laplace approximation to posterior: \\mathcal{N}({\\vec \\map},\\bf \\postcov) The mean of this posterior distribution, {\\vec \\map} , is the parameter vector maximizing the posterior, and is known as the maximum a posteriori (MAP) point. It can be found by minimizing the negative log of the posterior, which amounts to solving a deterministic inverse problem with appropriately weighted norms, \\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\; \\Big( \\frac{1}{2} \\| {\\bf f}(\\m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} +\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\prcov^{-1}} \\Big). The posterior covariance matrix is then given by the inverse of the Hessian matrix of \\mathcal{J} at \\map , namely \\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1} The generalized eigenvalue problem: \\Hmisfit {\\matrix V} = \\prcov^{-1} {\\matrix V} {\\matrix \\Lambda}, where {\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} contains the generalized eigenvalues and the columns of {\\matrix V}\\in \\mathbb R^{n\\times n} the generalized eigenvectors such that {\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I} . Randomized eigensolvers to construct the approximate spectral decomposition: When the generalized eigenvalues \\{\\lambda_i\\} decay rapidly, we can extract a low-rank approximation of \\Hmisfit by retaining only the r largest eigenvalues and corresponding eigenvectors, \\Hmisfit = \\prcov^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\prcov^{-1}, Here, \\Vr \\in \\mathbb{R}^{n\\times r} contains only the r generalized eigenvectors of \\Hmisfit that correspond to the r largest eigenvalues, which are assembled into the diagonal matrix {\\matrix{\\Lambda}}_r = \\diag (\\lambda_i) \\in \\mathbb{R}^{r \\times r} . The approximate posterior covariance: Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \\begin{align} \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1} = \\prcov-\\Vr {\\matrix{D}}_r \\Vr^T + \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i + 1}\\right), \\end{align} where {\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in \\mathbb{R}^{r\\times r} . The last term in this expression captures the error due to truncation in terms of the discarded eigenvalues; this provides a criterion for truncating the spectrum, namely that r is chosen such that \\lambda_r is small relative to 1. Therefore we can approximate the posterior covariance as \\postcov \\approx \\prcov - \\Vr {\\matrix{D}}_r \\Vr^T Drawing samples from a Gaussian distribution with covariance \\H^{-1} Let {\\bf x} be a sample for the prior distribution, i.e. {\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1}) as {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r + \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1} + {\\bf I} \\big\\} {\\bf x} This tutorial shows: Description of the inverse problem (the forward problem, the prior, and the misfit functional) Convergence of the inexact Newton-CG algorithm Low-rank-based approximation of the posterior covariance (built on a low-rank approximation of the Hessian of the data misfit) How to construct the low-rank approximation of the Hessian of the data misfit How to apply the inverse and square-root inverse Hessian to a vector efficiently Samples from the Gaussian approximation of the posterior Goals: By the end of this notebook, you should be able to: Understand the Bayesian inverse framework Visualise and understand the results Modify the problem and code Mathematical tools used: Finite element method Derivation of gradient and Hessian via the adjoint method inexact Newton-CG Armijo line search Bayes' formula randomized eigensolvers List of software used: FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , A great python package that I used for plotting many of the results Numpy , A python package for linear algebra. While extensive, this is mostly used to compute means and sums in this notebook. 1. Load modules import dolfin as dl import ufl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(seed=1) 2. Generate the true parameter This function generates a random field with a prescribed anysotropic covariance function. def true_model(prior): noise = dl.Vector() prior.init_vector(noise,\"noise\") parRandom.normal(1., noise) mtrue = dl.Vector() prior.init_vector(mtrue, 0) prior.sample(noise,mtrue) return mtrue 3. Set up the mesh and finite element spaces We compute a two dimensional mesh of a unit square with nx by ny elements. We define a P2 finite element space for the state and adjoint variable and P1 for the parameter . ndim = 2 nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh2, Vh1, Vh2] print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641 4. Set up the forward problem To set up the forward problem we use the PDEVariationalProblem class, which requires the following inputs - the finite element spaces for the state, parameter, and adjoint variables Vh - the pde in weak form pde_varf - the boundary conditions bc for the forward problem and bc0 for the adjoint and incremental problems. The PDEVariationalProblem class offer the following functionality: - solving the forward/adjoint and incremental problems - evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables. def u_boundary(x, on_boundary): return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS) u_bdr = dl.Expression(\"x[1]\", degree=1) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) f = dl.Constant(0.0) def pde_varf(u,m,p): return ufl.exp(m)*ufl.inner(ufl.grad(u), ufl.grad(p))*ufl.dx - f*p*ufl.dx pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) 4. Set up the prior To obtain the synthetic true paramter m_{\\rm true} we generate a realization from the prior distribution. Here we assume a Gaussian prior with zero average and covariance matrix \\mathcal{C} = \\mathcal{A}^{-2} . The action of \\mathcal{A} on a field m is given by \\mathcal{A}m = \\left\\{ \\begin{array}{rl} \\gamma \\nabla \\cdot \\left( \\Theta\\nabla m\\right)+ \\delta m & \\text{in } \\Omega\\\\ \\left( \\Theta\\, \\nabla m\\right) \\cdot \\boldsymbol{n} + \\beta m & \\text{on } \\partial\\Omega, \\end{array} \\right. where \\beta \\propto \\sqrt{\\gamma\\delta} is chosen to minimize boundary artifacts. Here \\Theta is an s.p.d. anisotropic tensor of the form \\Theta = \\begin{bmatrix} \\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\ (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2 \\end{bmatrix}. gamma = .1 delta = .5 theta0 = 2. theta1 = .5 alpha = math.pi/4 anis_diff = dl.CompiledExpression(ExpressionModule.AnisTensor2D(), degree = 1) anis_diff.set(theta0, theta1, alpha) prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True) mtrue = true_model(prior) print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2)) objs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)] mytitles = [\"True Parameter\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2 5. Set up the misfit functional and generate synthetic observations To setup the observation operator \\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t} , we generate n_t ( ntargets in the code below) random locations where to evaluate the value of the state. Under the assumption of Gaussian additive noise, the likelihood function \\pi_{\\rm like} has the form \\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right), where u(m) denotes the solution of the forward model at a given parameter m . The class PointwiseStateObservation implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state u and parameter m . To generate the synthetic observation, we first solve the forward problem using the true parameter m_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise. rel_noise is the signal to noise ratio. ntargets = 50 rel_noise = 0.01 #Targets only on the bottom targets_x = np.random.uniform(0.1,0.9, [ntargets] ) targets_y = np.random.uniform(0.1,0.5, [ntargets] ) targets = np.zeros([ntargets, ndim]) targets[:,0] = targets_x targets[:,1] = targets_y #targets everywhere #targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) print( \"Number of observation points: {0}\".format(ntargets) ) misfit = PointwiseStateObservation(Vh[STATE], targets) utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev vmax = max( utrue.max(), misfit.d.max() ) vmin = min( utrue.min(), misfit.d.min() ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax) nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax) plt.show() Number of observation points: 50 6. Set up the model and test gradient and Hessian The model is defined by three component: - the PDEVariationalProblem pde which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems. - the Prior prior which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator) - the Misfit misfit which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables. To test gradient and the Hessian of the model we use forward finite differences. model = Model(pde, prior, misfit) m0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER]) _ = modelVerify(model, m0.vector()) (yy, H xx) - (xx, H yy) = -2.7824662881775175e-14 7. Compute the MAP point We used the globalized Newtown-CG method to compute the MAP point. m = prior.mean.copy() solver = ReducedSpaceNewtonCG(model) solver.parameters[\"rel_tolerance\"] = 1e-6 solver.parameters[\"abs_tolerance\"] = 1e-12 solver.parameters[\"max_iter\"] = 25 solver.parameters[\"GN_iter\"] = 5 solver.parameters[\"globalization\"] = \"LS\" solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4 x = solver.solve([None, m, None]) if solver.converged: print( \"\\nConverged in \", solver.it, \" iterations.\") else: print( \"\\nNot Converged\") print( \"Termination reason: \", solver.termination_reasons[solver.reason] ) print( \"Final gradient norm: \", solver.final_grad_norm ) print( \"Final cost: \", solver.final_cost ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\") nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\") plt.show() It cg_it cost misfit reg (g,dm) ||g||L2 alpha tolcg 1 2 5.676752e+02 5.665943e+02 1.080947e+00 -1.487693e+04 4.008143e+04 1.000000e+00 5.000000e-01 2 2 1.315109e+02 1.292859e+02 2.225043e+00 -8.730801e+02 7.040800e+03 1.000000e+00 4.191210e-01 3 4 5.827697e+01 5.504162e+01 3.235343e+00 -1.565905e+02 2.115036e+03 1.000000e+00 2.297139e-01 4 1 5.257433e+01 4.934371e+01 3.230622e+00 -1.142190e+01 1.827983e+03 1.000000e+00 2.135573e-01 5 6 3.958912e+01 3.507186e+01 4.517259e+00 -3.227629e+01 9.179935e+02 1.000000e+00 1.513381e-01 6 2 3.681315e+01 3.218885e+01 4.624305e+00 -5.500988e+00 7.541262e+02 1.000000e+00 1.371673e-01 7 11 3.276535e+01 2.530155e+01 7.463799e+00 -9.109597e+00 4.788040e+02 1.000000e+00 1.092968e-01 8 1 3.225221e+01 2.479005e+01 7.462158e+00 -1.027637e+00 4.891427e+02 1.000000e+00 1.104705e-01 9 10 3.216230e+01 2.445205e+01 7.710247e+00 -1.804897e-01 7.393553e+01 1.000000e+00 4.294919e-02 10 10 3.216015e+01 2.439780e+01 7.762350e+00 -4.285017e-03 1.308609e+01 1.000000e+00 1.806897e-02 11 18 3.215993e+01 2.438399e+01 7.775934e+00 -4.424839e-04 3.526254e+00 1.000000e+00 9.379620e-03 12 22 3.215993e+01 2.438428e+01 7.775651e+00 -1.835996e-07 7.735457e-02 1.000000e+00 1.389221e-03 Converged in 12 iterations. Termination reason: Norm of the gradient less than tolerance Final gradient norm: 0.00013206555177717097 Final cost: 32.15992905024544 8. Compute the low rank-based Laplace approximation to posterior We used the double pass algorithm to compute a low-rank decomposition of the Hessian Misfit. In particular, we solve \\Hmisfit {\\bf v}_i = \\lambda_i \\prcov^{-1} {\\bf v}_i. The figure shows the largest k generalized eigenvectors of the Hessian misfit. The effective rank of the Hessian misfit is the number of eigenvalues above the red line ( y=1 ). The effective rank is independent of the mesh size. model.setPointForHessianEvaluations(x, gauss_newton_approx=False) Hmisfit = ReducedHessian(model, misfit_only=True) k = 50 p = 20 print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior(prior, lmbda, V) posterior.mean = x[PARAMETER] plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15]) Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20. 9. Prior and Laplace approximation to posterior pointwise variance fields compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print( \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200) objs = [dl.Function(Vh[PARAMETER], pr_pw_variance), dl.Function(Vh[PARAMETER], post_pw_variance)] mytitles = [\"Prior variance\", \"Posterior variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 1.014727e+00; Prior trace 1.797376e+00; Correction trace 7.826487e-01 10. Generate samples from Prior and Laplace approximation to posterior nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\") s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"3 SubsurfaceBayesian"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#bayesian-quantification-of-parameter-uncertainty","text":"","title":"Bayesian quantification of parameter uncertainty:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#estimating-the-gaussian-approximation-of-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde","text":"In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by an elliptic PDE via the Bayesian inference framework. Hence, we state the inverse problem as a problem of statistical inference over the space of uncertain parameters, which are to be inferred from data and a physical model. The resulting solution to the statistical inverse problem is a posterior distribution that assigns to any candidate set of parameter fields our belief (expressed as a probability) that a member of this candidate set is the ``true'' parameter field that gave rise to the observed data. For simplicity, in what follows we give finite-dimensional expressions (i.e., after discretization of the parameter space) for the Bayesian formulation of the inverse problem.","title":"Estimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#bayes-theorem","text":"The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m) over the parameter space, which encodes any knowledge or assumptions about the parameter space that we may wish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\data \\; | \\; \\m) , which explicitly represents the probability that a given set of parameters \\m might give rise to the observed data \\data \\in \\mathbb{R}^m , namely: \\begin{align} \\pi_{\\text{post}}(\\m | \\data) \\propto \\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m). \\end{align} Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.","title":"Bayes' Theorem:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#gaussian-prior-and-noise","text":"","title":"Gaussian prior and noise:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#the-prior","text":"We consider a Gaussian prior with mean {\\vec m}_{\\text{prior}} and covariance \\prcov . The covariance is given by the discretization of the inverse of differential operator \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where \\gamma , \\delta > 0 control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem.","title":"The prior:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#the-likelihood","text":"\\data = {\\bf f}(\\m) + {\\bf e }, \\;\\;\\; {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} ) \\pi_{\\text{like}}(\\data \\; | \\; \\m) = \\exp \\left( - \\tfrac{1}{2} ({\\bf f}(\\m) - \\data)^T {\\bf \\Gamma}_{\\text{noise}}^{-1} ({\\bf f}(\\m) - \\data)\\right) Here {\\bf f} is the parameter-to-observable map that takes a parameter vector \\m and maps it to the space observation vector \\data .","title":"The likelihood:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#the-posterior","text":"\\pi_{\\text{post}}(\\m \\; | \\; \\data) \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(\\m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\prcov^{-1}} \\right)","title":"The posterior:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#the-laplace-approximation-to-posterior-mathcalnvec-mapbf-postcov","text":"The mean of this posterior distribution, {\\vec \\map} , is the parameter vector maximizing the posterior, and is known as the maximum a posteriori (MAP) point. It can be found by minimizing the negative log of the posterior, which amounts to solving a deterministic inverse problem with appropriately weighted norms, \\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\; \\Big( \\frac{1}{2} \\| {\\bf f}(\\m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} +\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\prcov^{-1}} \\Big). The posterior covariance matrix is then given by the inverse of the Hessian matrix of \\mathcal{J} at \\map , namely \\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1}","title":"The Laplace approximation to posterior: \\mathcal{N}({\\vec \\map},\\bf \\postcov)"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#the-generalized-eigenvalue-problem","text":"\\Hmisfit {\\matrix V} = \\prcov^{-1} {\\matrix V} {\\matrix \\Lambda}, where {\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} contains the generalized eigenvalues and the columns of {\\matrix V}\\in \\mathbb R^{n\\times n} the generalized eigenvectors such that {\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I} .","title":"The generalized eigenvalue problem:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#randomized-eigensolvers-to-construct-the-approximate-spectral-decomposition","text":"When the generalized eigenvalues \\{\\lambda_i\\} decay rapidly, we can extract a low-rank approximation of \\Hmisfit by retaining only the r largest eigenvalues and corresponding eigenvectors, \\Hmisfit = \\prcov^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\prcov^{-1}, Here, \\Vr \\in \\mathbb{R}^{n\\times r} contains only the r generalized eigenvectors of \\Hmisfit that correspond to the r largest eigenvalues, which are assembled into the diagonal matrix {\\matrix{\\Lambda}}_r = \\diag (\\lambda_i) \\in \\mathbb{R}^{r \\times r} .","title":"Randomized eigensolvers to construct the approximate spectral decomposition:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#the-approximate-posterior-covariance","text":"Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \\begin{align} \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1} = \\prcov-\\Vr {\\matrix{D}}_r \\Vr^T + \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i + 1}\\right), \\end{align} where {\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in \\mathbb{R}^{r\\times r} . The last term in this expression captures the error due to truncation in terms of the discarded eigenvalues; this provides a criterion for truncating the spectrum, namely that r is chosen such that \\lambda_r is small relative to 1. Therefore we can approximate the posterior covariance as \\postcov \\approx \\prcov - \\Vr {\\matrix{D}}_r \\Vr^T","title":"The approximate posterior covariance:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#drawing-samples-from-a-gaussian-distribution-with-covariance-h-1","text":"Let {\\bf x} be a sample for the prior distribution, i.e. {\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1}) as {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r + \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1} + {\\bf I} \\big\\} {\\bf x}","title":"Drawing samples from a Gaussian distribution with covariance \\H^{-1}"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#this-tutorial-shows","text":"Description of the inverse problem (the forward problem, the prior, and the misfit functional) Convergence of the inexact Newton-CG algorithm Low-rank-based approximation of the posterior covariance (built on a low-rank approximation of the Hessian of the data misfit) How to construct the low-rank approximation of the Hessian of the data misfit How to apply the inverse and square-root inverse Hessian to a vector efficiently Samples from the Gaussian approximation of the posterior","title":"This tutorial shows:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#goals","text":"By the end of this notebook, you should be able to: Understand the Bayesian inverse framework Visualise and understand the results Modify the problem and code","title":"Goals:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#mathematical-tools-used","text":"Finite element method Derivation of gradient and Hessian via the adjoint method inexact Newton-CG Armijo line search Bayes' formula randomized eigensolvers","title":"Mathematical tools used:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#list-of-software-used","text":"FEniCS , a parallel finite element element library for the discretization of partial differential equations PETSc , for scalable and efficient linear algebra operations and solvers Matplotlib , A great python package that I used for plotting many of the results Numpy , A python package for linear algebra. While extensive, this is mostly used to compute means and sums in this notebook.","title":"List of software used:"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#1-load-modules","text":"import dolfin as dl import ufl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) np.random.seed(seed=1)","title":"1. Load modules"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#2-generate-the-true-parameter","text":"This function generates a random field with a prescribed anysotropic covariance function. def true_model(prior): noise = dl.Vector() prior.init_vector(noise,\"noise\") parRandom.normal(1., noise) mtrue = dl.Vector() prior.init_vector(mtrue, 0) prior.sample(noise,mtrue) return mtrue","title":"2. Generate the true parameter"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces","text":"We compute a two dimensional mesh of a unit square with nx by ny elements. We define a P2 finite element space for the state and adjoint variable and P1 for the parameter . ndim = 2 nx = 64 ny = 64 mesh = dl.UnitSquareMesh(nx, ny) Vh2 = dl.FunctionSpace(mesh, 'Lagrange', 2) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh2, Vh1, Vh2] print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format( Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641","title":"3. Set up the mesh and finite element spaces"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#4-set-up-the-forward-problem","text":"To set up the forward problem we use the PDEVariationalProblem class, which requires the following inputs - the finite element spaces for the state, parameter, and adjoint variables Vh - the pde in weak form pde_varf - the boundary conditions bc for the forward problem and bc0 for the adjoint and incremental problems. The PDEVariationalProblem class offer the following functionality: - solving the forward/adjoint and incremental problems - evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables. def u_boundary(x, on_boundary): return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS) u_bdr = dl.Expression(\"x[1]\", degree=1) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) f = dl.Constant(0.0) def pde_varf(u,m,p): return ufl.exp(m)*ufl.inner(ufl.grad(u), ufl.grad(p))*ufl.dx - f*p*ufl.dx pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)","title":"4. Set up the forward problem"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#4-set-up-the-prior","text":"To obtain the synthetic true paramter m_{\\rm true} we generate a realization from the prior distribution. Here we assume a Gaussian prior with zero average and covariance matrix \\mathcal{C} = \\mathcal{A}^{-2} . The action of \\mathcal{A} on a field m is given by \\mathcal{A}m = \\left\\{ \\begin{array}{rl} \\gamma \\nabla \\cdot \\left( \\Theta\\nabla m\\right)+ \\delta m & \\text{in } \\Omega\\\\ \\left( \\Theta\\, \\nabla m\\right) \\cdot \\boldsymbol{n} + \\beta m & \\text{on } \\partial\\Omega, \\end{array} \\right. where \\beta \\propto \\sqrt{\\gamma\\delta} is chosen to minimize boundary artifacts. Here \\Theta is an s.p.d. anisotropic tensor of the form \\Theta = \\begin{bmatrix} \\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\ (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2 \\end{bmatrix}. gamma = .1 delta = .5 theta0 = 2. theta1 = .5 alpha = math.pi/4 anis_diff = dl.CompiledExpression(ExpressionModule.AnisTensor2D(), degree = 1) anis_diff.set(theta0, theta1, alpha) prior = BiLaplacianPrior(Vh[PARAMETER], gamma, delta, anis_diff, robin_bc=True) mtrue = true_model(prior) print(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2)) objs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)] mytitles = [\"True Parameter\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2","title":"4. Set up the prior"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations","text":"To setup the observation operator \\mathcal{B}: \\mathcal{V} \\mapsto \\mathbb{R}^{n_t} , we generate n_t ( ntargets in the code below) random locations where to evaluate the value of the state. Under the assumption of Gaussian additive noise, the likelihood function \\pi_{\\rm like} has the form \\pi_{\\rm like}( \\data \\,| \\, m ) \\propto \\exp\\left( -\\frac{1}{2}\\|\\mathcal{B}\\,u(m) - \\data \\|^2_{\\Gamma_{\\rm noise}^{-1}}\\right), where u(m) denotes the solution of the forward model at a given parameter m . The class PointwiseStateObservation implements the evaluation of the log-likelihood function and of its partial derivatives w.r.t. the state u and parameter m . To generate the synthetic observation, we first solve the forward problem using the true parameter m_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise. rel_noise is the signal to noise ratio. ntargets = 50 rel_noise = 0.01 #Targets only on the bottom targets_x = np.random.uniform(0.1,0.9, [ntargets] ) targets_y = np.random.uniform(0.1,0.5, [ntargets] ) targets = np.zeros([ntargets, ndim]) targets[:,0] = targets_x targets[:,1] = targets_y #targets everywhere #targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) print( \"Number of observation points: {0}\".format(ntargets) ) misfit = PointwiseStateObservation(Vh[STATE], targets) utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev vmax = max( utrue.max(), misfit.d.max() ) vmin = min( utrue.min(), misfit.d.min() ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax) nb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax) plt.show() Number of observation points: 50","title":"5. Set up the misfit functional and generate synthetic observations"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian","text":"The model is defined by three component: - the PDEVariationalProblem pde which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems. - the Prior prior which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator) - the Misfit misfit which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables. To test gradient and the Hessian of the model we use forward finite differences. model = Model(pde, prior, misfit) m0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER]) _ = modelVerify(model, m0.vector()) (yy, H xx) - (xx, H yy) = -2.7824662881775175e-14","title":"6. Set up the model and test gradient and Hessian"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#7-compute-the-map-point","text":"We used the globalized Newtown-CG method to compute the MAP point. m = prior.mean.copy() solver = ReducedSpaceNewtonCG(model) solver.parameters[\"rel_tolerance\"] = 1e-6 solver.parameters[\"abs_tolerance\"] = 1e-12 solver.parameters[\"max_iter\"] = 25 solver.parameters[\"GN_iter\"] = 5 solver.parameters[\"globalization\"] = \"LS\" solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4 x = solver.solve([None, m, None]) if solver.converged: print( \"\\nConverged in \", solver.it, \" iterations.\") else: print( \"\\nNot Converged\") print( \"Termination reason: \", solver.termination_reasons[solver.reason] ) print( \"Final gradient norm: \", solver.final_grad_norm ) print( \"Final cost: \", solver.final_cost ) plt.figure(figsize=(15,5)) nb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\") nb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\") plt.show() It cg_it cost misfit reg (g,dm) ||g||L2 alpha tolcg 1 2 5.676752e+02 5.665943e+02 1.080947e+00 -1.487693e+04 4.008143e+04 1.000000e+00 5.000000e-01 2 2 1.315109e+02 1.292859e+02 2.225043e+00 -8.730801e+02 7.040800e+03 1.000000e+00 4.191210e-01 3 4 5.827697e+01 5.504162e+01 3.235343e+00 -1.565905e+02 2.115036e+03 1.000000e+00 2.297139e-01 4 1 5.257433e+01 4.934371e+01 3.230622e+00 -1.142190e+01 1.827983e+03 1.000000e+00 2.135573e-01 5 6 3.958912e+01 3.507186e+01 4.517259e+00 -3.227629e+01 9.179935e+02 1.000000e+00 1.513381e-01 6 2 3.681315e+01 3.218885e+01 4.624305e+00 -5.500988e+00 7.541262e+02 1.000000e+00 1.371673e-01 7 11 3.276535e+01 2.530155e+01 7.463799e+00 -9.109597e+00 4.788040e+02 1.000000e+00 1.092968e-01 8 1 3.225221e+01 2.479005e+01 7.462158e+00 -1.027637e+00 4.891427e+02 1.000000e+00 1.104705e-01 9 10 3.216230e+01 2.445205e+01 7.710247e+00 -1.804897e-01 7.393553e+01 1.000000e+00 4.294919e-02 10 10 3.216015e+01 2.439780e+01 7.762350e+00 -4.285017e-03 1.308609e+01 1.000000e+00 1.806897e-02 11 18 3.215993e+01 2.438399e+01 7.775934e+00 -4.424839e-04 3.526254e+00 1.000000e+00 9.379620e-03 12 22 3.215993e+01 2.438428e+01 7.775651e+00 -1.835996e-07 7.735457e-02 1.000000e+00 1.389221e-03 Converged in 12 iterations. Termination reason: Norm of the gradient less than tolerance Final gradient norm: 0.00013206555177717097 Final cost: 32.15992905024544","title":"7. Compute the MAP point"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#8-compute-the-low-rank-based-laplace-approximation-to-posterior","text":"We used the double pass algorithm to compute a low-rank decomposition of the Hessian Misfit. In particular, we solve \\Hmisfit {\\bf v}_i = \\lambda_i \\prcov^{-1} {\\bf v}_i. The figure shows the largest k generalized eigenvectors of the Hessian misfit. The effective rank of the Hessian misfit is the number of eigenvalues above the red line ( y=1 ). The effective rank is independent of the mesh size. model.setPointForHessianEvaluations(x, gauss_newton_approx=False) Hmisfit = ReducedHessian(model, misfit_only=True) k = 50 p = 20 print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior(prior, lmbda, V) posterior.mean = x[PARAMETER] plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15]) Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.","title":"8. Compute the low rank-based Laplace approximation to posterior"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#9-prior-and-laplace-approximation-to-posterior-pointwise-variance-fields","text":"compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200) print( \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200) objs = [dl.Function(Vh[PARAMETER], pr_pw_variance), dl.Function(Vh[PARAMETER], post_pw_variance)] mytitles = [\"Prior variance\", \"Posterior variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 1.014727e+00; Prior trace 1.797376e+00; Correction trace 7.826487e-01","title":"9. Prior and Laplace approximation to posterior pointwise variance fields"},{"location":"tutorials_v3.0.0/3_SubsurfaceBayesian/#10-generate-samples-from-prior-and-laplace-approximation-to-posterior","text":"nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\") s_post = dl.Function(Vh[PARAMETER], name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"10. Generate samples from Prior and Laplace approximation to posterior"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/","text":"\\def\\D{\\Omega} \\def\\ipar{m} \\def\\R{\\mathbb{R}} \\def\\del{\\partial} \\def\\vec{\\bf} \\def\\priorm{\\mu_0} \\def\\C{\\mathcal{C}} \\def\\Acal{\\mathcal{A}} \\def\\postm{\\mu_{\\rm{post}}} \\def\\iparpost{\\ipar_\\text{post}} \\def\\obs{ {\\vec d}} \\def\\yobs{\\obs^{\\text{obs}}} \\def\\obsop{\\mathcal{B}} \\def\\dd{\\vec{\\bar{d}}} \\def\\iFF{\\mathcal{F}} \\def\\iFFadj{\\mathcal{F}^*} \\def\\ncov{\\Gamma_{\\mathrm{noise}}} Bayesian initial condition inversion in an advection-diffusion problem In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements. The Bayesian inverse problem: Following the Bayesian framework, we utilize a Gaussian prior measure \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) , with \\C_0=\\Acal^{-2} where \\Acal is an elliptic differential operator as described in the PoissonBayesian example, and use an additive Gaussian noise model. Therefore, the solution of the Bayesian inverse problem is the posterior measure, \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post}) with \\iparpost and \\C_\\text{post} . The posterior mean \\iparpost is characterized as the minimizer of \\begin{aligned} & \\mathcal{J}(\\ipar) := \\frac{1}{2} \\left\\| \\obsop u(\\ipar) -\\obs \\right\\|^2_{\\ncov^{-1}} + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)}, \\end{aligned} which can also be interpreted as the regularized functional to be minimized in deterministic inversion. The observation operator \\mathcal{B} extracts the values of the forward solution u on a set of locations \\{{\\vec{x}}_1, \\ldots, {\\vec{x}}_n\\} \\subset \\D at times \\{t_1, \\ldots, t_N\\} \\subset [0, T] . The posterior covariance \\C_{\\text{post}} is the inverse of the Hessian of \\mathcal{J}(\\ipar) , i.e., \\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}. The forward problem: The parameter-to-observable map \\iFF \\,\\ipar := \\obsop\\, u(\\ipar) maps an initial condition \\ipar \\in L^2(\\D) to pointwise spatiotemporal observations of the concentration field u({\\vec x},t) through solution of the advection-diffusion equation given by \\begin{split} u_t - \\kappa\\Delta u + {\\vec v} \\cdot \\nabla u &= 0 & \\quad \\text{in } \\D\\times(0,T),\\\\ u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\ \\kappa \\nabla u\\cdot {\\vec{n}} &= 0 & \\quad \\text{on } \\partial\\D \\times (0,T). \\end{split} Here, \\D \\subset \\R^d ( d \\in \\{2, 3\\} ) is a bounded domain, \\kappa > 0 is the diffusion coefficient and T > 0 is the final time. The velocity field \\vec{v} is computed by solving the following steady-state Navier-Stokes equation with the side walls driving the flow: \\begin{aligned} - \\frac{1}{\\operatorname{Re}} \\Delta {\\vec v} + \\nabla q + {\\vec v} \\cdot \\nabla {\\vec v} &= 0 &\\quad&\\text{ in }\\D,\\\\ \\nabla \\cdot {\\vec v} &= 0 &&\\text{ in }\\D,\\\\ {\\vec v} &= {\\vec g} &&\\text{ on } \\partial\\D. \\end{aligned} Here, q is pressure, \\text{Re} is the Reynolds number. The Dirichlet boundary data {\\vec g} \\in \\R^d is given by {\\vec g} = {\\vec e}_2 on the left wall of the domain, {\\vec g}=-{\\vec e}_2 on the right wall, and {\\vec g} = {\\vec 0} everywhere else. The adjoint problem: The adjoint problem is a final value problem, since p is specified at t = T rather than at t = 0 . Thus, it is solved backwards in time, which amounts to the solution of the advection-diffusion equation \\begin{aligned} -p_t - \\nabla \\cdot (p {\\vec v}) - \\kappa \\Delta p &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\ p(\\cdot, T) &= 0 & &\\text{ in } \\D,\\\\ ({ \\vec{v} }p+\\kappa\\nabla p)\\cdot {\\vec{n}} &= 0 & &\\text{ on } \\partial\\D\\times (0,T). \\end{aligned} Then, the adjoint of the parameter to observable map \\iFF^* is defined by setting \\iFF^*\\obs = p({\\vec x}, 0). 1. Load modules import dolfin as dl import ufl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" ) from model_ad_diff import TimeDependentAD, SpaceTimePointwiseStateObservation import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) 2. Construct the velocity field def v_boundary(x,on_boundary): return on_boundary def q_boundary(x,on_boundary): return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS def computeVelocityField(mesh): Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2) Wh = dl.FunctionSpace(mesh, 'Lagrange', 1) mixed_element = ufl.MixedElement([Xh.ufl_element(), Wh.ufl_element()]) XW = dl.FunctionSpace(mesh, mixed_element) Re = dl.Constant(1e2) g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1) bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary) bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise') bcs = [bc1, bc2] vq = dl.Function(XW) (v,q) = ufl.split(vq) (v_test, q_test) = dl.TestFunctions (XW) def strain(v): return ufl.sym(ufl.grad(v)) F = ( (2./Re)*ufl.inner(strain(v),strain(v_test))+ ufl.inner (ufl.nabla_grad(v)*v, v_test) - (q * ufl.div(v_test)) + ( ufl.div(v) * q_test) ) * ufl.dx dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\": {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}}) plt.figure(figsize=(15,5)) vh = dl.project(v,Xh) qh = dl.project(q,Wh) nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\") nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\") plt.show() return v 3. Set up the mesh and finite element spaces mesh = dl.refine( dl.Mesh(\"ad_20.xml\") ) wind_velocity = computeVelocityField(mesh) Vh = dl.FunctionSpace(mesh, \"Lagrange\", 1) print( \"Number of dofs: {0}\".format( Vh.dim() ) ) Number of dofs: 2023 4. Set up model (prior, true/proposed initial condition) ic_expr = dl.Expression( 'std::min(0.5,std::exp(-100*(std::pow(x[0]-0.35,2) + std::pow(x[1]-0.7,2))))', element=Vh.ufl_element()) true_initial_condition = dl.interpolate(ic_expr, Vh).vector() gamma = 1. delta = 8. prior = BiLaplacianPrior(Vh, gamma, delta, robin_bc=True) prior.mean = dl.interpolate(dl.Constant(0.25), Vh).vector() t_init = 0. t_final = 4. t_1 = 1. dt = .1 observation_dt = .2 simulation_times = np.arange(t_init, t_final+.5*dt, dt) observation_times = np.arange(t_1, t_final+.5*dt, observation_dt) targets = np.loadtxt('targets.txt') print (\"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = SpaceTimePointwiseStateObservation(Vh, observation_times, targets) problem = TimeDependentAD(mesh, [Vh,Vh,Vh], prior, misfit, simulation_times, wind_velocity, True) objs = [dl.Function(Vh,true_initial_condition), dl.Function(Vh,prior.mean)] mytitles = [\"True Initial Condition\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() Number of observation points: 80 5. Generate the synthetic observations rel_noise = 0.01 utrue = problem.generate_vector(STATE) x = [utrue, true_initial_condition, None] problem.solveFwd(x[STATE], x) misfit.observe(x, misfit.d) MAX = misfit.d.norm(\"linf\", \"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev,misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev nb.show_solution(Vh, true_initial_condition, utrue, \"Solution\") 6. Test the gradient and the Hessian of the cost (negative log posterior) m0 = true_initial_condition.copy() _ = modelVerify(problem, m0, is_quadratic=True) (yy, H xx) - (xx, H yy) = -1.1681934136389683e-13 7. Evaluate the gradient [u,m,p] = problem.generate_vector() problem.solveFwd(u, [u,m,p]) problem.solveAdj(p, [u,m,p]) mg = problem.generate_vector(PARAMETER) grad_norm = problem.evalGradientParameter([u,m,p], mg) print( \"(g,g) = \", grad_norm) (g,g) = 2354159843455358.0 8. The Gaussian posterior H = ReducedHessian(problem, misfit_only=True) k = 80 p = 20 print( \"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = singlePassG(H, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior( prior, lmbda, V ) plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh, V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60]) Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20. 9. Compute the MAP point H.misfit_only = False solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( posterior.Hlr ) solver.parameters[\"print_level\"] = 1 solver.parameters[\"rel_tolerance\"] = 1e-6 solver.solve(m, -mg) problem.solveFwd(u, [u,m,p]) total_cost, reg_cost, misfit_cost = problem.cost([u,m,p]) print( \"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost) ) posterior.mean = m plt.figure(figsize=(7.5,5)) nb.plot(dl.Function(Vh, m), mytitle=\"Initial Condition\") plt.show() nb.show_solution(Vh, m, u, \"Solution\") Iterartion : 0 (B r, r) = 1231866.6310868163 Iteration : 1 (B r, r) = 66.31734328582567 Iteration : 2 (B r, r) = 0.3150082598782035 Iteration : 3 (B r, r) = 0.003975346025817018 Iteration : 4 (B r, r) = 1.7003714104952925e-05 Iteration : 5 (B r, r) = 8.577697903362507e-08 Relative/Absolute residual less than tol Converged in 5 iterations with final norm 0.00029287707153962235 Total cost 782.968; Reg Cost 153.412; Misfit 629.556 10. Prior and posterior pointwise variance fields compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=300) print( \"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300) objs = [dl.Function(Vh, pr_pw_variance), dl.Function(Vh, post_pw_variance)] mytitles = [\"Prior Variance\", \"Posterior Variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 0.000268048; Prior trace 0.00809706; Correction trace 0.00782902 11. Draw samples from the prior and posterior distributions nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh, name=\"sample_prior\") s_post = dl.Function(Vh, name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"4 AdvectionDiffusionBayesian"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#bayesian-initial-condition-inversion-in-an-advection-diffusion-problem","text":"In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.","title":"Bayesian initial condition inversion in an advection-diffusion problem"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#the-bayesian-inverse-problem","text":"Following the Bayesian framework, we utilize a Gaussian prior measure \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) , with \\C_0=\\Acal^{-2} where \\Acal is an elliptic differential operator as described in the PoissonBayesian example, and use an additive Gaussian noise model. Therefore, the solution of the Bayesian inverse problem is the posterior measure, \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post}) with \\iparpost and \\C_\\text{post} . The posterior mean \\iparpost is characterized as the minimizer of \\begin{aligned} & \\mathcal{J}(\\ipar) := \\frac{1}{2} \\left\\| \\obsop u(\\ipar) -\\obs \\right\\|^2_{\\ncov^{-1}} + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)}, \\end{aligned} which can also be interpreted as the regularized functional to be minimized in deterministic inversion. The observation operator \\mathcal{B} extracts the values of the forward solution u on a set of locations \\{{\\vec{x}}_1, \\ldots, {\\vec{x}}_n\\} \\subset \\D at times \\{t_1, \\ldots, t_N\\} \\subset [0, T] . The posterior covariance \\C_{\\text{post}} is the inverse of the Hessian of \\mathcal{J}(\\ipar) , i.e., \\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.","title":"The Bayesian inverse problem:"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#the-forward-problem","text":"The parameter-to-observable map \\iFF \\,\\ipar := \\obsop\\, u(\\ipar) maps an initial condition \\ipar \\in L^2(\\D) to pointwise spatiotemporal observations of the concentration field u({\\vec x},t) through solution of the advection-diffusion equation given by \\begin{split} u_t - \\kappa\\Delta u + {\\vec v} \\cdot \\nabla u &= 0 & \\quad \\text{in } \\D\\times(0,T),\\\\ u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\ \\kappa \\nabla u\\cdot {\\vec{n}} &= 0 & \\quad \\text{on } \\partial\\D \\times (0,T). \\end{split} Here, \\D \\subset \\R^d ( d \\in \\{2, 3\\} ) is a bounded domain, \\kappa > 0 is the diffusion coefficient and T > 0 is the final time. The velocity field \\vec{v} is computed by solving the following steady-state Navier-Stokes equation with the side walls driving the flow: \\begin{aligned} - \\frac{1}{\\operatorname{Re}} \\Delta {\\vec v} + \\nabla q + {\\vec v} \\cdot \\nabla {\\vec v} &= 0 &\\quad&\\text{ in }\\D,\\\\ \\nabla \\cdot {\\vec v} &= 0 &&\\text{ in }\\D,\\\\ {\\vec v} &= {\\vec g} &&\\text{ on } \\partial\\D. \\end{aligned} Here, q is pressure, \\text{Re} is the Reynolds number. The Dirichlet boundary data {\\vec g} \\in \\R^d is given by {\\vec g} = {\\vec e}_2 on the left wall of the domain, {\\vec g}=-{\\vec e}_2 on the right wall, and {\\vec g} = {\\vec 0} everywhere else.","title":"The forward problem:"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#the-adjoint-problem","text":"The adjoint problem is a final value problem, since p is specified at t = T rather than at t = 0 . Thus, it is solved backwards in time, which amounts to the solution of the advection-diffusion equation \\begin{aligned} -p_t - \\nabla \\cdot (p {\\vec v}) - \\kappa \\Delta p &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\ p(\\cdot, T) &= 0 & &\\text{ in } \\D,\\\\ ({ \\vec{v} }p+\\kappa\\nabla p)\\cdot {\\vec{n}} &= 0 & &\\text{ on } \\partial\\D\\times (0,T). \\end{aligned} Then, the adjoint of the parameter to observable map \\iFF^* is defined by setting \\iFF^*\\obs = p({\\vec x}, 0).","title":"The adjoint problem:"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#1-load-modules","text":"import dolfin as dl import ufl import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" ) from model_ad_diff import TimeDependentAD, SpaceTimePointwiseStateObservation import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#2-construct-the-velocity-field","text":"def v_boundary(x,on_boundary): return on_boundary def q_boundary(x,on_boundary): return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS def computeVelocityField(mesh): Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2) Wh = dl.FunctionSpace(mesh, 'Lagrange', 1) mixed_element = ufl.MixedElement([Xh.ufl_element(), Wh.ufl_element()]) XW = dl.FunctionSpace(mesh, mixed_element) Re = dl.Constant(1e2) g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1) bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary) bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise') bcs = [bc1, bc2] vq = dl.Function(XW) (v,q) = ufl.split(vq) (v_test, q_test) = dl.TestFunctions (XW) def strain(v): return ufl.sym(ufl.grad(v)) F = ( (2./Re)*ufl.inner(strain(v),strain(v_test))+ ufl.inner (ufl.nabla_grad(v)*v, v_test) - (q * ufl.div(v_test)) + ( ufl.div(v) * q_test) ) * ufl.dx dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\": {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}}) plt.figure(figsize=(15,5)) vh = dl.project(v,Xh) qh = dl.project(q,Wh) nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\") nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\") plt.show() return v","title":"2. Construct the velocity field"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces","text":"mesh = dl.refine( dl.Mesh(\"ad_20.xml\") ) wind_velocity = computeVelocityField(mesh) Vh = dl.FunctionSpace(mesh, \"Lagrange\", 1) print( \"Number of dofs: {0}\".format( Vh.dim() ) ) Number of dofs: 2023","title":"3. Set up the mesh and finite element spaces"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition","text":"ic_expr = dl.Expression( 'std::min(0.5,std::exp(-100*(std::pow(x[0]-0.35,2) + std::pow(x[1]-0.7,2))))', element=Vh.ufl_element()) true_initial_condition = dl.interpolate(ic_expr, Vh).vector() gamma = 1. delta = 8. prior = BiLaplacianPrior(Vh, gamma, delta, robin_bc=True) prior.mean = dl.interpolate(dl.Constant(0.25), Vh).vector() t_init = 0. t_final = 4. t_1 = 1. dt = .1 observation_dt = .2 simulation_times = np.arange(t_init, t_final+.5*dt, dt) observation_times = np.arange(t_1, t_final+.5*dt, observation_dt) targets = np.loadtxt('targets.txt') print (\"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = SpaceTimePointwiseStateObservation(Vh, observation_times, targets) problem = TimeDependentAD(mesh, [Vh,Vh,Vh], prior, misfit, simulation_times, wind_velocity, True) objs = [dl.Function(Vh,true_initial_condition), dl.Function(Vh,prior.mean)] mytitles = [\"True Initial Condition\", \"Prior mean\"] nb.multi1_plot(objs, mytitles) plt.show() Number of observation points: 80","title":"4. Set up model (prior, true/proposed initial condition)"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations","text":"rel_noise = 0.01 utrue = problem.generate_vector(STATE) x = [utrue, true_initial_condition, None] problem.solveFwd(x[STATE], x) misfit.observe(x, misfit.d) MAX = misfit.d.norm(\"linf\", \"linf\") noise_std_dev = rel_noise * MAX parRandom.normal_perturb(noise_std_dev,misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev nb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")","title":"5. Generate the synthetic observations"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior","text":"m0 = true_initial_condition.copy() _ = modelVerify(problem, m0, is_quadratic=True) (yy, H xx) - (xx, H yy) = -1.1681934136389683e-13","title":"6. Test the gradient and the Hessian of the cost (negative log posterior)"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#7-evaluate-the-gradient","text":"[u,m,p] = problem.generate_vector() problem.solveFwd(u, [u,m,p]) problem.solveAdj(p, [u,m,p]) mg = problem.generate_vector(PARAMETER) grad_norm = problem.evalGradientParameter([u,m,p], mg) print( \"(g,g) = \", grad_norm) (g,g) = 2354159843455358.0","title":"7. Evaluate the gradient"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#8-the-gaussian-posterior","text":"H = ReducedHessian(problem, misfit_only=True) k = 80 p = 20 print( \"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) ) Omega = MultiVector(x[PARAMETER], k+p) parRandom.normal(1., Omega) lmbda, V = singlePassG(H, prior.R, prior.Rsolver, Omega, k) posterior = GaussianLRPosterior( prior, lmbda, V ) plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r') plt.yscale('log') plt.xlabel('number') plt.ylabel('eigenvalue') nb.plot_eigenvectors(Vh, V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60]) Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.","title":"8. The Gaussian posterior"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#9-compute-the-map-point","text":"H.misfit_only = False solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( posterior.Hlr ) solver.parameters[\"print_level\"] = 1 solver.parameters[\"rel_tolerance\"] = 1e-6 solver.solve(m, -mg) problem.solveFwd(u, [u,m,p]) total_cost, reg_cost, misfit_cost = problem.cost([u,m,p]) print( \"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost) ) posterior.mean = m plt.figure(figsize=(7.5,5)) nb.plot(dl.Function(Vh, m), mytitle=\"Initial Condition\") plt.show() nb.show_solution(Vh, m, u, \"Solution\") Iterartion : 0 (B r, r) = 1231866.6310868163 Iteration : 1 (B r, r) = 66.31734328582567 Iteration : 2 (B r, r) = 0.3150082598782035 Iteration : 3 (B r, r) = 0.003975346025817018 Iteration : 4 (B r, r) = 1.7003714104952925e-05 Iteration : 5 (B r, r) = 8.577697903362507e-08 Relative/Absolute residual less than tol Converged in 5 iterations with final norm 0.00029287707153962235 Total cost 782.968; Reg Cost 153.412; Misfit 629.556","title":"9. Compute the MAP point"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields","text":"compute_trace = True if compute_trace: post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=300) print( \"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr) ) post_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300) objs = [dl.Function(Vh, pr_pw_variance), dl.Function(Vh, post_pw_variance)] mytitles = [\"Prior Variance\", \"Posterior Variance\"] nb.multi1_plot(objs, mytitles, logscale=False) plt.show() Posterior trace 0.000268048; Prior trace 0.00809706; Correction trace 0.00782902","title":"10. Prior and posterior pointwise variance fields"},{"location":"tutorials_v3.0.0/4_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions","text":"nsamples = 5 noise = dl.Vector() posterior.init_vector(noise,\"noise\") s_prior = dl.Function(Vh, name=\"sample_prior\") s_post = dl.Function(Vh, name=\"sample_post\") pr_max = 2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max() pr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min() ps_max = 2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max() ps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min() for i in range(nsamples): parRandom.normal(1., noise) posterior.sample(noise, s_prior.vector(), s_post.vector()) plt.figure(figsize=(15,5)) nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max) nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max) plt.show() Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"11. Draw samples from the prior and posterior distributions"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/","text":"Spectrum of the preconditioned Hessian misfit operator The linear source inversion problem We consider the following linear source inversion problem. Find the state u \\in H^1_{\\Gamma_D}(\\Omega) and the source ( parameter ) m \\in H^1(\\Omega) that solves \\begin{aligned} {} & \\min_m \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx & {}\\\\ {\\rm s.t.} & {} &{} \\\\ {} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = m & {\\rm in} \\; \\Omega\\\\ {} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\ {} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\ \\end{aligned} Here: u_d is a n_{\\rm obs} finite dimensional vector that denotes noisy observations of the state u in n_{\\rm obs} locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . Specifically, u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where \\eta_i are i.i.d. \\mathcal{N}(0, \\sigma^2) . B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}} is the linear operator that evaluates the state u at the observation locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . \\delta and \\gamma are the parameters of the regularization penalizing the L^2(\\Omega) and H^1(\\Omega) norm of m-m_0 , respectively. k , {\\bf v} , c are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively. \\Gamma_D \\subset \\partial \\Omega , \\Gamma_N \\subset \\partial \\Omega represents the subdomain of \\partial\\Omega where we impose Dirichlet or Neumann boundary conditions, respectively. 1. Load modules import dolfin as dl import ufl import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False) 2. The linear source inversion problem def pde_varf(u,m,p): return k*ufl.inner(ufl.grad(u), ufl.grad(p))*ufl.dx \\ + ufl.inner(ufl.grad(u), v*p)*ufl.dx \\ + c*u*p*ufl.dx \\ - m*p*ufl.dx def u_boundary(x, on_boundary): return on_boundary and x[1] < dl.DOLFIN_EPS def solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True): myRandom = Random() mesh = dl.UnitSquareMesh(nx, ny) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh1, Vh1, Vh1] if verbose: print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) u_bdr = dl.Constant(0.0) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) mtrue_expr = dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))',degree=5) mtrue = dl.interpolate(mtrue_expr , Vh[PARAMETER]).vector() m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector() pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) if verbose: print( \"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = PointwiseStateObservation(Vh[STATE], targets) reg = LaplacianPrior(Vh[PARAMETER], gamma, delta) #Generate synthetic observations utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX myRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], mtrue), mytitle = \"True source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132) nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133) plt.show() model = Model(pde, reg, misfit) u = model.generate_vector(STATE) m = m0.copy() p = model.generate_vector(ADJOINT) x = [u,m,p] mg = model.generate_vector(PARAMETER) model.solveFwd(u, x) model.solveAdj(p, x) model.evalGradientParameter(x, mg) model.setPointForHessianEvaluations(x, gauss_newton_approx=False) H = ReducedHessian(model) solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( reg.Rsolver ) solver.parameters[\"print_level\"] = -1 solver.parameters[\"rel_tolerance\"] = 1e-9 solver.solve(m, -mg) if solver.converged: if verbose: print( \"CG converged in \", solver.iter, \" iterations.\" ) else: print( \"CG did not converged.\" ) raise model.solveFwd(u, x) total_cost, reg_cost, misfit_cost = model.cost(x) if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], m), mytitle = \"Reconstructed source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132) nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133) plt.show() H.misfit_only = True k_evec = 80 p_evec = 5 if verbose: print( \"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec) ) Omega = MultiVector(x[PARAMETER], k_evec+p_evec) myRandom.normal(1., Omega) lmbda, V = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec) if verbose: plt.figure() nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\") nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15]) plt.show() return lmbda, V, Vh[PARAMETER], solver.iter 3. Solution of the source inversion problem ndim = 2 nx = 32 ny = 32 ntargets = 300 np.random.seed(seed=1) targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) rel_noise = 0.01 gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) lmbda, V, Vm, nit = solve(nx,ny, targets, rel_noise, gamma, delta) Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089 Number of observation points: 300 CG converged in 69 iterations. Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5. 4. Mesh independence of the spectrum of the preconditioned Hessian misfit gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) n = [16,32,64] lmbda1, V1, Vm1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5]) plt.show() Number of Iterations: 67 69 68 5. Dependence on the noise level We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization. gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) rel_noise = [1e-3,1e-2,1e-1] lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5]) plt.show() Number of Iterations: 185 67 23 6. Dependence on the PDE coefficients Assume a constant reaction term c = 1 , and we consider different values for the diffusivity coefficient k . The smaller the value of k the slower the decay in the spectrum. rel_noise = 0.01 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(1.0) lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.1) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.01) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"k=1. Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"k=0.1 Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"k=0.01 Eigen\", which=[0,1,5]) plt.show() Number of Iterations: 79 143 256 Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"Spectrum of the preconditioned Hessian misfit operator"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#spectrum-of-the-preconditioned-hessian-misfit-operator","text":"","title":"Spectrum of the preconditioned Hessian misfit operator"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#the-linear-source-inversion-problem","text":"We consider the following linear source inversion problem. Find the state u \\in H^1_{\\Gamma_D}(\\Omega) and the source ( parameter ) m \\in H^1(\\Omega) that solves \\begin{aligned} {} & \\min_m \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx & {}\\\\ {\\rm s.t.} & {} &{} \\\\ {} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = m & {\\rm in} \\; \\Omega\\\\ {} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\ {} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\ \\end{aligned} Here: u_d is a n_{\\rm obs} finite dimensional vector that denotes noisy observations of the state u in n_{\\rm obs} locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . Specifically, u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where \\eta_i are i.i.d. \\mathcal{N}(0, \\sigma^2) . B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}} is the linear operator that evaluates the state u at the observation locations \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . \\delta and \\gamma are the parameters of the regularization penalizing the L^2(\\Omega) and H^1(\\Omega) norm of m-m_0 , respectively. k , {\\bf v} , c are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively. \\Gamma_D \\subset \\partial \\Omega , \\Gamma_N \\subset \\partial \\Omega represents the subdomain of \\partial\\Omega where we impose Dirichlet or Neumann boundary conditions, respectively.","title":"The linear source inversion problem"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#1-load-modules","text":"import dolfin as dl import ufl import numpy as np import matplotlib.pyplot as plt %matplotlib inline import sys import os sys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") ) from hippylib import * import logging logging.getLogger('FFC').setLevel(logging.WARNING) logging.getLogger('UFL').setLevel(logging.WARNING) dl.set_log_active(False)","title":"1. Load modules"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#2-the-linear-source-inversion-problem","text":"def pde_varf(u,m,p): return k*ufl.inner(ufl.grad(u), ufl.grad(p))*ufl.dx \\ + ufl.inner(ufl.grad(u), v*p)*ufl.dx \\ + c*u*p*ufl.dx \\ - m*p*ufl.dx def u_boundary(x, on_boundary): return on_boundary and x[1] < dl.DOLFIN_EPS def solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True): myRandom = Random() mesh = dl.UnitSquareMesh(nx, ny) Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1) Vh = [Vh1, Vh1, Vh1] if verbose: print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) ) u_bdr = dl.Constant(0.0) u_bdr0 = dl.Constant(0.0) bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary) bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary) mtrue_expr = dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) + pow(x[1]-0.7,2))))',degree=5) mtrue = dl.interpolate(mtrue_expr , Vh[PARAMETER]).vector() m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector() pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True) if verbose: print( \"Number of observation points: {0}\".format(targets.shape[0]) ) misfit = PointwiseStateObservation(Vh[STATE], targets) reg = LaplacianPrior(Vh[PARAMETER], gamma, delta) #Generate synthetic observations utrue = pde.generate_state() x = [utrue, mtrue, None] pde.solveFwd(x[STATE], x) misfit.B.mult(x[STATE], misfit.d) MAX = misfit.d.norm(\"linf\") noise_std_dev = rel_noise * MAX myRandom.normal_perturb(noise_std_dev, misfit.d) misfit.noise_variance = noise_std_dev*noise_std_dev if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], mtrue), mytitle = \"True source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132) nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133) plt.show() model = Model(pde, reg, misfit) u = model.generate_vector(STATE) m = m0.copy() p = model.generate_vector(ADJOINT) x = [u,m,p] mg = model.generate_vector(PARAMETER) model.solveFwd(u, x) model.solveAdj(p, x) model.evalGradientParameter(x, mg) model.setPointForHessianEvaluations(x, gauss_newton_approx=False) H = ReducedHessian(model) solver = CGSolverSteihaug() solver.set_operator(H) solver.set_preconditioner( reg.Rsolver ) solver.parameters[\"print_level\"] = -1 solver.parameters[\"rel_tolerance\"] = 1e-9 solver.solve(m, -mg) if solver.converged: if verbose: print( \"CG converged in \", solver.iter, \" iterations.\" ) else: print( \"CG did not converged.\" ) raise model.solveFwd(u, x) total_cost, reg_cost, misfit_cost = model.cost(x) if verbose: plt.figure(figsize=(18,4)) nb.plot(dl.Function(Vh[PARAMETER], m), mytitle = \"Reconstructed source\", subplot_loc=131) nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132) nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133) plt.show() H.misfit_only = True k_evec = 80 p_evec = 5 if verbose: print( \"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec) ) Omega = MultiVector(x[PARAMETER], k_evec+p_evec) myRandom.normal(1., Omega) lmbda, V = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec) if verbose: plt.figure() nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\") nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15]) plt.show() return lmbda, V, Vh[PARAMETER], solver.iter","title":"2. The linear source inversion problem"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#3-solution-of-the-source-inversion-problem","text":"ndim = 2 nx = 32 ny = 32 ntargets = 300 np.random.seed(seed=1) targets = np.random.uniform(0.1,0.9, [ntargets, ndim] ) rel_noise = 0.01 gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) lmbda, V, Vm, nit = solve(nx,ny, targets, rel_noise, gamma, delta) Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089 Number of observation points: 300 CG converged in 69 iterations. Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.","title":"3. Solution of the source inversion problem"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian-misfit","text":"gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) n = [16,32,64] lmbda1, V1, Vm1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5]) plt.show() Number of Iterations: 67 69 68","title":"4. Mesh independence of the spectrum of the preconditioned Hessian misfit"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#5-dependence-on-the-noise-level","text":"We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization. gamma = 70. delta = 1e-1 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(0.) rel_noise = [1e-3,1e-2,1e-1] lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5]) plt.show() Number of Iterations: 185 67 23","title":"5. Dependence on the noise level"},{"location":"tutorials_v3.0.0/5_HessianSpectrum/#6-dependence-on-the-pde-coefficients","text":"Assume a constant reaction term c = 1 , and we consider different values for the diffusivity coefficient k . The smaller the value of k the slower the decay in the spectrum. rel_noise = 0.01 k = dl.Constant(1.0) v = dl.Constant((0.0, 0.0)) c = dl.Constant(1.0) lmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.1) lmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) k = dl.Constant(0.01) lmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False) print( \"Number of Iterations: \", niter1, niter2, niter3 ) plt.figure(figsize=(18,4)) nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131) nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132) nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133) nb.plot_eigenvectors(Vm1, V1, mytitle=\"k=1. Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm2, V2, mytitle=\"k=0.1 Eigen\", which=[0,1,5]) nb.plot_eigenvectors(Vm3, V3, mytitle=\"k=0.01 Eigen\", which=[0,1,5]) plt.show() Number of Iterations: 79 143 256 Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. Copyright (c) 2019-2020, The University of Texas at Austin, University of California--Merced, Washington University in St. Louis. All Rights reserved. See file COPYRIGHT for details. This file is part of the hIPPYlib library. For more information and source code availability see https://hippylib.github.io. hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.","title":"6. Dependence on the PDE coefficients"}]}